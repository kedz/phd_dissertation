While \autoref{dlsal} and \autoref{mlsal} focused on content selection
(through the lens of \salienceestimation), that is, what to say, in
a \texttotext~generation system, they relied on trivial text generation
algorithms, that is, copying or extracting text units from the input to
construct the output. In this chapter, given an idealized representation
of content, we now focus on the text generation process in detail. 


%In particular, we focus on ensuring the language generation component is more
%than just a good language model producing fluent text but also produces 
%semantically correct or consistent output given its conditioning inputs.
%We 


\Deeplearning~architectures have become the standard modeling 
method for a host
of language generation tasks. When data is plentiful, 
%end-to-end training of 
%such models is straightforward, and 
the \deeplearning~\sequencetosequence~framework
has proven to be incredibly adaptable to a variety of problem domains. 
Recent evaluations of end-to-end
trained \deeplearning~models for dialogue generation have shown that they 
are capable of learning very natural text realizations of formal
meaning representations (MRs),
i.e. dialogue acts (DAs) with slot-filler type attributes
(see \autoref{figure:introexample} for an
example).
In many cases,
they  beat rule and 
template based
systems on human and automatic measures of quality \cite{duvsek2019evaluating}.
  
%\input{example.tex}

However, this powerful generation capability comes with a cost; 
\deeplearning-based language
models are notoriously difficult to control, often producing quite 
fluent but  semantically misleading outputs. 
In order 
    for such models to truly be useful, they must be capable
    of correctly generating utterances for novel MRs at test time. 
In practice,
   even with delexicalization \cite{duvsek2016sequence,juraskaslug2slug}, 
copy and coverage mechanisms \cite{elder2018e2e}, 
    and overgeneration plus reranking \cite{duvsek2016sequence,juraskaslug2slug}, \deeplearning-based language generation models still produce 
errors \cite{duvsek2019evaluating}.


We call a \naturallanguagegeneration~model that generates utterances 
that are semantically correct with respect to the input 
\meaningrepresentation~a \faithfulgeneration~model. While a
\faithfulgeneration~model produces semantically correct output, in general
it is free to let the surface realization order of output utterances
be determined by its language model. We further define a 
\controllablegeneration~model as a \naturallanguagegeneration~model that can
follow a externally provided discourse ordering plan. 
\Controllablegeneration~models are a subset of \faithfulgeneration~models.

In this chapter we develop training strategies to produce both \faithful~and
\controllablegeneration~models. In \autoref{sec:fg}, we propose a 
data augmentation strategy for using an 
un\faithful~\naturallanguagegeneration~model and 
a \naturallanguageunderstanding~model to 
generate novel \meaningrepresentation/utterance pairs that are not well
represented in the original training data. Crucially, we use a noise-injection
sampling method that allows us to generate semantically diverse 
yet syntactically well formed outputs. Using this procedure we can generate
a large collection of synthetic data points. Training a new 
\sequencetosequence~model on the union of the original training and novel
synthetic data yields a more \faithfulgeneration~model, with substantially
reduced semantic errors relative to very competitive baselines. 


In \autoref{sec:cg}, we show how different \linearizationstrategies,
that is, mappings from \meaningrepresentation~to 
\naturallanguagegeneration~model input sequences, can effect the 
\faithfulness~of the model. In particular, we show that during training that
linearizing the \meaningrepresentation~in a way that aligns with the surface
realization order of the corresponding reference utterance yields very 
controllable models at test time. Such a model can now follow an arbitrary
order at test time. Additionally, we propose a phrase-based data augmentation
technique to improve the models ability to follow arbitrary orders. We find
this controllable generation ability holds for a variety of popular architectures and a large, pretrained conditional language generation model.


In the next sections, we present related works, before formally defining the 
\meaningrepresentation~to~text generation task and models, before developing
the data augmentation techniques for \faithfulgeneration~\autoref{somesec} 
and \linearizationstrategies~for~\controllablegeneration~\autoref{somesec}.


 


