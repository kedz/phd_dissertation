\section{Task Definition}
\label{sec:strmsumProbDef}

We now describe the query focused, sentence extractive, streaming news
summarization problem in detail.  We start with the query, $\query$, which is a
brief string describing an event of interest to be summarized. See
\autoref{tab:queries} for some example queries. All relevance judgements about
a sentence are made with respect to the query string.  Additionally, the query
is used to construct the news stream which we now turn to.

\input{ch4/tables/queryevents.tex}

The news stream is an ordered sequence of text approximately relevant to the
query (i.e. the results of an information retrieval system). It is useful to be
able to talk about this stream from two perspectives, as either a stream of
documents, $\docstream$, or a flat stream of sentences, $\sentstream$.  From
the document perspective, the stream is an ordered sequence of $\ssize$
documents  
\[ 
  \docstreamq = \left[ 
              \sdoc_1,\sdoc_2, \ldots, \sdoc_\ssize
  \right] 
\] 
where each document $\sdoc_i$ is itself an ordered sequence of $\dssize_i$
sentences, 
\[ 
    \sdoc_i = \left[ 
        \ssent_{i,1}, \ssent_{i,2}, \ldots, \ssent_{i,\dssize_i}  
    \right].  
\] 
Each document $\sdoc_i$ also has a timestamp $\docts_i$ which is also shared by
all of its sentences $\ssent_{i,j}  \in \sdoc_i$.  The stream is ordered by
timestamp, so we have $\docts_i < \docts_{i+1}$ for all $i \in \{1,\ldots,
\ssize -1\}$. From the sentence perspective, the news stream is an ordered
sequence of $\sssize$ sentences, 
\[
    \sentstream = \left[ \ssent_1, \ssent_2, \ldots, \ssent_\sssize \right], 
\] 
where each sentence $\ssent_i$ has a timestamp $\sentts_i$ and $\sentts_i \le
\sentts_{i+1}$ for all $i \in \{1,\ldots,\sssize - 1\}$.  The two points of
view are equivalent in the sense that the concatenation of $\docstream$ equals
$\sentstream$, 
\[  \sdoc_1 \oplus \sdoc_2 \oplus \cdots \oplus \sdoc_\ssize = \sentstream 
\] 
and $\docts_i = \sentts_j$ for all $\ssent_j \in \sdoc_i$.

A query focused, sentence extractive stream summarization model must process
the stream sequentially in time and determine for each sentence $\ssent_j \in
\sentstream$ whether to extract it or to skip it. That is, the system must
decide to add the sentence to a summary of the stream, or to ignore the
sentence.  While there is no explicit length constraint in the number of
updates for a summary, the ideal update covers novel and salient information.
A summarizer that extracts many sentences that are not informative or are
redundant given prior updates will receive lower scores under the evaluation
measures which we describe later in this section.

Crucial to the stream summarization problem is the notion of system time, which
indicates what information from the stream has been read and can be used to
make extraction predictions. When the system time is $\systs_t$, the
summarization model can in theory use any information collected from all
documents $\sdoc_i \in \docstream$ such that $\docts_i \le \systs_t$, although
for more significant query-streams, it may not be practical for a summarization
model to store all previous documents. For any sentences not yet extracted, it
can similarly decide to extract any sentence $\ssent_j \in \sentstream$  such
that $\sentts_j \le \systs_t$. Previous extraction decisions, however, cannot
be undone. The system time can be incremented by arbitrary positive amounts to
$\systs_{t+1}$ (i.e. $\systs_t < \systs_{t+1}$) to allow the summarization
model to observe and extract more sentences.  Given two timestamps
$\timestamp_1,\timestamp_2$ with $\timestamp_1 < \timestamp_2$, we denote the
sub-sequence of documents in the stream that fall between them as
$\docstream_{\ts_1:\ts_2}$. Similarly, $\sentstream_{\ts_1:\ts_2}$ indicates
the subsequence of sentences with timestamps that fall between $\ts_1$ and
$\ts_2$.

We refer to a sentence that has been extracted as an update. Let $\supdate_k$
be the $k$-th update extracted by the system.  $\supdate_k$ has a corresponding
timestamp, $\uts_k$ that is equal to the system time that it was extracted by
the summarizer.  That is, if $\supdate_k$ was extracted at $\systs_t$, then
$\uts_k = \systs_t$. We refer to a collection of $K$ timestamped updates as an
update summary, $\updateSummary = \left[ \left(\supdate_1, \uts_1\right),
\ldots,\left(\supdate_K, \uts_K\right) \right]$.

For evaluation purposes we compare the update summary to a human reference
abstract summary of the query event. The reference abstract summary, $\snuggets
= \left[ \left(\snugget_1, \nts_1\right), \ldots, \left(\snugget_\nsize,
\nts_\nsize\right) \right]$, contains a series of $\nsize$ sentences describing
important facts about  the event. We refer to these facts as \textit{nuggets}.
Each nugget $\snugget_i$ has a timestamp $\nts_i$, indicating the reference
time that information was published.

We say an update $\supdate$ covers a nugget $\snugget$, which we write
$\denotes{\snugget} \in \denotes{\supdate}$, if the information in the nugget
$\snugget$ is described in the update $\supdate$. Note that it is possible for
an update to cover multiple nuggets.  For example if we have,
\begin{align*}
    \snugget_1 & =  \textit{Hurricane Sandy was a category 5 hurricane.}\\
\snugget_2 & = \textit{Hurricane Sandy made landfall on Saturday.}\\
\supdate & = \textit{Hurricane Sandy, which made landfall on Saturday,
                was upgraded to a category 5 storm.}
\end{align*}
then $\denotes{\snugget_1},\denotes{\snugget_2} \in \denotes{\supdate}$. Given
an update summary $\updateSummary$ and a nugget $\snugget$, we define the
earliest cover, 
\[
    \earlymatch(\snugget,\updateSummary) = 
   \begin{cases} 
\argmin_{\utuple{k} \in \updateSummary : \denotes{\snugget} \in \denotes{\supdate_k}} 
  \uts_k &
\textrm{if $\exists \utuple{k} \in \updateSummary: 
            \denotes{\snugget} \in \denotes{\supdate}$} \\ 
            \emptyset & \textrm{otherwise} 
    \end{cases}
\] 
as the earliest update that covers the nugget $\snugget$, or the empty set if
no update in the update summary covers $\snugget$. We also define the inverse
mapping,
\[
    \invearlymatch(\supdate, \updateSummary, \snuggets) =
    \left\{\ntuple{l} \in \snuggets \Big\vert \exists\ts : \left(\supdate,\ts\right) = \earlymatch(\snugget_l,
\updateSummary) \right\},
\] 
which is the set of nuggets that have $\supdate$ as its earliest cover.

The objective of the summarization model is to produce an update summary
$\updateSummary$ such that the set of updates covers the information expressed
by the nuggets without containing much repeated information. Additionally, the
summarization system should try to minimize the latency between the time
information in a nugget is available in the stream and the system time that
information is extracted. 

We now formalize these evaluation criteria using the official Temporal
Summarization evaluation measures \citep{aslam2013}.  Given an update and
reference summary, $\updateSummary$ and $\snuggets$ respectively, we define the
gain of an update as 
\[  
    \gain(\supdate_k,\uts_k, \updateSummary, \snuggets) =  
    \sum_{\ntuple{j} \in  
    \invearlymatch(\supdate_k, \updateSummary, \snuggets)} 1 
\] 
which is essentially the number of nuggets covered by an update $\supdate$.  We
also define a latency penalized version of this function, \[
\gain_\latency\left(\supdate_i, \updateSummary,\snuggets \right) =
\sum_{\ntuple{j} \in  \invearlymatch(\supdate_k,  \updateSummary, \snuggets)}
\latency\left(\nts_j, \uts_i\right)  \] where $\latency(\timestamp^*,
\timestamp) = 1 - \frac{2}{\pi}\arctan\left( \frac{\timestamp -
\timestamp^*}{3600 * 6} \right)$, $\latency(\timestamp^*,\timestamp) = 1$ when
$\timestamp=\timestamp^*$, and $\latency(\timestamp^*,\timestamp)$ gradually
approaches 0 as $\timestamp - \timestamp^*$ increases. In real time, the
latency weighting $\latency\left(\nts_j,\uts_i\right)$ aproaches 2 if
$\supdate_i$ covers $\snugget_j$ over 50 hours before $\nts_j$;
$\latency\left(\nts_j,\uts_i\right)$ approaches 0 if $\supdate_i$ covers
$\snugget_j$ over 50 hours after $\nts_j$. Under the latency weighting, systems
receive a gain bonus for covering a nugget $\snugget_j$ before its reference
time $\nts_j$ and a penalty for covering it after this time.

The first metric we use to evaluate a summary is the normalized expected gain,
\[
\nEG\left(\updateSummary\right) = \frac{1}{Z\setsize{\updateSummary}} 
\sum_{\utuple{i} \in \updateSummary} 
    \gain(\supdate_i, \updateSummary, \snuggets)\]
where $Z$ is maximum achievable expected gain (computed per query).  This
metric can be thought of as roughly analogous to precision.  We also report a
recall focused metric, called comprehensiveness, which is defined as 
\[
\comp\left(\updateSummary\right) = \frac{1}{\setsize{\snuggets}} 
\sum_{\utuple{i} \in \updateSummary} 
    \gain(\supdate_i, \updateSummary, \snuggets)
\]
which measures the percentage of nuggets covered by the update summary. We also
report the harmonic mean of normalized expected gain and  comprehensiveness,
 \[ 
 \harm(\updateSummary) = 
2 \frac{\nEG\left(\updateSummary\right) \times \comp(\updateSummary)}
{  \nEG\left(\updateSummary\right) + \comp(\updateSummary)   }.
\]
A latency-penalized version of normalized expected gain, comprehensiveness, and
their harmonic mean can be obtained by replacing $\gain$ with $\gain_\latency$
in the above calculations.
