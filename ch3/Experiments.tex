\section{Experiments} \label{sec:exps}

For our main experiments, we train every possible pairing of sentence encoder
and extractor architecture ($3\times4=12$) on each of dataset $\mathcal{D} =
\left\{\left(\doc^{(1)}, \nnsals^{(1)}\right),\ldots,
\left(\doc^{(\corpusSize)}, \bsals^{(\corpusSize)}\right) \right\}$.  We use
the trained models to produce extract summaries for the test set, and we then
evaluate summary quality with respect to the reference abstract summaries using
\rouge-2 recall.\footnote{\rouge-1 recall and \rouge-LCS trend similarity in
our experiments so we omit them for space.} We use extract summary word budgets
of $\wordbudget=100$ words for news, and $\wordbudget=75$, $\wordbudget=290$,
and $\wordbudget=200$ for Reddit, AMI, and PubMed respectively.  We also
evaluate using \meteor~\citep{banerjee2005meteor}, which measures precision and
recall of reference words while allowing for more matchings on synonymy or
morphology.  We use the default settings for \meteor. We compute \rouge~with
stopwords removed and without stemming, keeping defaults for all other
parameters.

For each model configuration, we train five different versions using different
random seeds and report the mean evaluation measure.  We estimate statistical
significance by first averaging each document level \rouge~or \meteor~score
over the five random initializations.  We then test the difference between the
best system on each dataset and all other systems using the approximate
randomization test with the Bonferroni correction for multiple comparisons
\citep{riezler2005pitfalls}, testing for significance at the $0.05$ level. 

\subsection{Training}

We train all models to minimize the weighted negative log-likelihood
\[\mathcal{L(\params)} = -\sum_{(\doc,\bsals) \in \corpus } \sum_{i=1}^\docSize \omega(\bsal_i) \log \model\left(\bsal_i|\bsal_1,\ldots,\bsal_{i-1},\doc;\params\right) \]
over the training data $\corpus$ using stochastic gradient descent with the
\textsc{Adam} optimizer \citep{kingma2014adam}. Since positive salience labels
(i.e. $\bsal_i = 1$) are much rarer than negative salience labels, we reweight
the negative log likelihood above, setting
\[\omega(0)=1 \quad \textrm{and}\quad \omega(1) = \docSize_0/\docSize_1\] 
where $\docSize_0$ and $\docSize_1$ 
are the number of training sentences labeled $0$ and $1$ respectively.  We
trained for a maximum of 50 epochs and the best model was selected with early
stopping on the validation set according to ROUGE-2. Each epoch constitutes a
full pass through the dataset. The average stopping epoch was: CNN-DailyMail,
16.2; NYT, 21.36; DUC, 37.11; Reddit, 36.59; AMI, 19.58; PubMed, 19.84.  All
experiments were repeated with five random initializations.     Unless
specified, word embeddings were initialized using pretrained GloVe embeddings
\citep{pennington2014glove} and we did not update them during training. Unknown
words were mapped to a zero embedding.

We use a learning rate of $.0001$ and a dropout rate of $0.25$ for all dropout
layers. We also employ gradient clipping ($-5 < \nabla_\theta < 5$).  Weight
matrix parameters are initialized using Xavier initialization with the normal
distribution \citep{glorot2010understanding} and bias terms are set to $0$.
Hyperparameter settings were found using manual exploration and observing
consistent improvements in \rouge~on the validation set.  We use a batch size
of 32 for all datasets except AMI and PubMed, which are often longer and
consume more memory, for which we use sizes two and four respectively.

For \clext~based models, we train for half of the maximum epochs with teacher
forcing, i.e. we set $\psal_i = 1$ if $\bsal_i = 1$ in the gold data and $0$
otherwise when computing the decoder input $\psal_i \sentEmb_i$. We revert to
the predicted model probability during the second half training and during
test-time inference.

\subsection{Baselines}

\paragraph{Lead} As a baseline we include the lead summary, i.e. taking the
first $\wordbudget$ words of the document as summary, where $\wordbudget$ is
the summary word budget for each dataset (see the first paragraph of
\autoref{sec:exps}). While incredibly simple, this method is still a
competitive baseline for single document summarization, especially on newswire.

\paragraph{Oracle} To measure the performance ceiling, we show the
\rouge/\meteor~scores using the extractive summary $\extractSummary$~which was
a bi-product of our algorithm for obtaining salience labels $\bsals$ (see
\autoref{sec:labelgen} for details). Essentially, this summary represents an
approximate ceiling on \rouge~performance, as it has clairvoyant knowledge of
the human reference summaries for each document.
