In many cases, estimating \salience~is not the entirety of the summarization
system's task. Accounting for \redundancy~is also an important factor in
many summarization systems (especially multi-document summarization) 
since the same information can often be restated multiple times. Additionally,
in many situations, \salience~is dynamic, changing
over time with the information need of the summary receiver.
In this chapter, we explore ways of encorporating salience predictions into
more holistic algorithms for constructing extract summaries using information
about text unit redundancy and prior extraction decisions.  





%For instance, since \rouge~primarily rewards recall, 
%in extractive multi-document summarization, 
%accounting for redundancy becomes an important factor when selecting which
%input text units to extract. 
%In this chapter we explore ways of encorporating salience predictions into
%more holistic algorithms for constructing extract summaries given 
%salience estimates of the text units.


Since a structured approach to summarization is not totally necessary
for single document summarization, we motivate the models in this chapter 
wth a more difficult summarization challenge: 
query focused, sentence extractive, streaming news summarization.
In this problem, the summarization system must monitor a stream of news articles
and extract sentences, which we call \updates, that are relevant to the user 
query. Collectively, these \updates~constitute an \updatesummary. 
As in the last chapter, we rely a data-driven assignment of \update~\salience,
where an \update~is \salient~if it contains information that was found
in a human authored summary of the query/document stream. 

Additionally, in this setting we have the notion of system time -- the 
summarization system can consider all sentences that have entered the 
stream before the current system time. Advancing the system time introduces more
sentences to the stream. However, the \salience~of given piece of summary information, 
which we call a \nugget, decreases monotonically from the earliest 
system time that the \nugget~information was introduced into the stream. 
Because of this, we must extract sentences in an online fashion while trying 
to minimize the latency between the time important information first enters 
the stream and the time the summarization system extracts that information.



%we must do this under several constraints. First we introduce time as 
%a dimension to the summarization problem. While the relevance of a sentence
%to a given query may be static, the ground truth salience 
%of a sentence diminishes monotonically over time, reflecting the idea
%that information from a week ago is less valuable than information from
%today. This is an assumption of many streaming summarization scenarios, 
%especially in crisis informatics \cite{somebody}. 


Since there is little supervised data for this task, we rely on a  
feature-based regression model to provide our \salience~estimates.   
The time constraint
makes summarization particularly challenging as the typical features for 
summarization make use of the frequency of words and phrases in the input, 
In the streaming case, these features are now constantly evolving with time.
 For instance, at the start of the stream, these estimates may not 
be very reliable. 

A second but important issue is that \salience~estimates do not occur
in isolation. As we add \updates~to the summary, the \salience~of our 
remaining inputs is likely to change based on \redundancy~and other 
factors.
Unfortunately, adding summary/sentence interaction features introduces an element of 
exploration to training the model for now various summary configuration 
and candidate sentence pairs must be considered.



%Learning models of sentence salience in summarization is often difficult
%because summarization datasets are typically small (a few hundred training
%examples in many cases) and learning lexical 
%feature weights directly is likely to run into issues of train-test distribution mismatch and 
%overfitting. 
%To get around this problem, we rely on heavily lexicalized components 
%trainable on unlabeled data to produce scores that serve as unlexicalized 
%features in our salience models. The canonical example of this is to use 
%a language model trained on in-domain but non-summarization data to 
%obtain average token perplexity scores for a sentence; the salience model
%only sees the average perplexity feature. 
%In this section we 
%describe our features in the context of a stream summarization task, but the 
%features are themselves fairly generic and likely applicable in other task
%settings.
%
%






Our two proposed feature-based summarization models deal with these
issues in slightly different ways. 
The first model, Salience-biased Affinity Propagation (SAP) 
\cite{kedzie2015predicting}, combines 
independent sentence level \salience~estimates into a clustering algorithm,
Affinity Propagation (AP) \cite{frey2007clustering}, that also 
considers sentence similarity to jointly select salient and 
non-redundant sentences for inclusion in the summary. In this model, for 
a sentence to be selected it must have a high salience estimate
and also be
representative of other sentences in the input.

Our second model, Learning-to-Summarize (L2S) \cite{kedzie2016real}, allows 
us to freely incorporate summary/sentence interaction
features, as we train the salience model using the learning-to-search regime
\cite{daume2009search,chang2015learning}
where learning takes place using different exploration policies. Using 
this method we can learn a salience model that makes good individual sentence
selection choices (i.e. good local decisions) that also correlate to a good 
final summary (i.e. good global decisions). 

In the next sections, we will formally describe the query focused, 
sentence extractive, streaming news summarization task, and the cover
related work on this task before covering the models/experiments for the 
SAP and L2S models. 
 
%and related work 
%in detail, before moving on to the sentence features, and finally the models
%and their evaluations.


