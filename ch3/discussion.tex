\input{ch3/figures/output.tex}

\section{Discussion}

Learning content selection for summarization in the news domain is severely
inhibited by the lead bias.  The summaries generated by all systems described
here--the prior work and our proposed simplified models--are highly similar to
each other and to the lead baseline. The Cheng \& Lapata and Seq2Seq extractors
(using the averaging encoder) share 87.8\% of output sentences on average on
the CNN/DM data, with similar numbers for the other news domains (see
\autoref{tab:output} for a typical example).  Also on CNN/DM, 58\% of the
Seq2Seq selected sentences also occur in the lead summary, with similar numbers
for DUC, NYT, and Reddit. Shuffling reduces lead overlap to 35.2\% but the
overall system performance drops significantly; the models are not able to
identify important information without position.
    
The relative robustness of the news domain to part of speech ablation also
suggests that models are mostly learning to recognize the stylistic features
unique to the beginning of the article, and not the content.  Additionally, the
drop in performance when learning word embeddings on the news domain suggests
that word embeddings alone do not provide very generalizable content features
compared to recognizing the lead.

The picture is rosier for non-news summarization where part of speech ablation
leads to larger performance differences and shuffling either does not inhibit
content selection significantly or leads to modest gains. Learning better
word-level representations on these domains will likely require much larger
corpora, something which might remain unlikely for personal stories and
meetings.

The lack of distinction among sentence encoders is interesting because it
echoes findings in the generic sentence embedding literature where word
embedding averaging is frustratingly difficult to outperform
\citep{iyyer2015,wieting2015,arora2017,wieting2017}.  The inability to learn
useful sentence representations is also borne out in the SummaRunner model,
where there are explicit similarity computations between document or summary
representations and sentence embeddings; these computations do not seem to add
much to the performance as the Cheng \& Lapata and Seq2Seq models which lack
these features generally perform as well or better.  Furthermore, the Cheng \&
Lapata and SummaRunner extractors both construct a history of previous
selection decisions to inform future choices but this does not seem to
significantly improve performance over the Seq2Seq extractor (which does not).
This suggests that we need to be cautious about the single document
summarization task as a test bed for learning summarization models.  The input
does not appear to be sufficiently rich enough or difficult enough that
modeling dependicies in the output needs to be done explicity. 

A manual examination of the outputs revealed some interesting failure modes,
although in general it was hard to discern clear patterns of behaviour other
than lead bias. On the news domain, the models consistently learned to ignore
quoted material in the lead, as often the quotes provide color to the story but
are unlikely to be included in the summary (e.g. \textit{``It was like somebody
slugging a punching bag.''}).  This behavior was most likely triggered by the
presence of quotes, as the quote attributions, which were often tokenized as
separate sentences, would subsequently be included in the summary despite also
not containing much information (e.g. \textit{Gil Clark of the National
Hurricane Center said Thursday}). 
