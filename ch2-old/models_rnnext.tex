\subsubsection{\rnnext~Extractor}


\input{ch2/figures/models_rnnext.tex}

    Our first proposed model is a very simple 
    \bidirectional~\recurrentneuralnetwork~based tagging
    model \citep{sometagfolks}, which we refer to as the \rnnext~extractor.
See \autoref{fig:rnnext} for a visual depiction of   
the extractor.
As in the \srext~extractor, the first step of the \rnnext~extractor
is to run a \bidirectional~\recurrentneuralnetwork~over the sentence 
embeddings produced by the sentence encoder layer, which produces
left and right partial contextual embeddings $\rnnextRHid_i$ and $\rnnextLHid_i$
respectively,

  \vspace{10pt} \noindent\textit{(Fig.~\ref{fig:rnnext}.a) Left and Right Partial Contexual Sentence Embeddings}
\begin{align}
    \rnnextRHid_0 = \zeroEmb, &\quad \rnnextLHid_{\docSize+1} = \zeroEmb, \\
    \forall i : \;\; i \in \{1,\ldots,\docSize\}& \nonumber \\
 \rnnextRHid_i &= \fgru\left(\sentEmb_i, \rnnextRHid_{i-1}; \rnnextRParams\right), \\
 \rnnextLHid_i &= \fgru\left(\sentEmb_i, \rnnextLHid_{i+1};\rnnextLParams\right), 
\end{align}
where $\rnnextRHid_i,\rnnextLHid_i \in \reals^{\rnnextRNNDim}$,
and $\rnnextRParams$ and $\rnnextLParams$ are the forward and backward
\gru~parameters.

%As in the \recurrentneuralnetwork~sentence encoder we use a \gru~cell
%to implement the forward and backward \recurrentneuralnetwork s.
The left and right partial contextual embeddings of each sentence 
are then passed through a \feedforward~layer to produce contextual
sentence embeddings $\rnnextHid_i$,

\vspace{10pt}\noindent\textit{(Fig.~\ref{fig:rnnext}.b) Contextual Sentence Embeddings}
\begin{align}
    \forall i :\;\; i \in \{1,\ldots,\docSize\}& \nonumber \\
   \rnnextHid_i &= \relu\left(
    \rnnextHidWeight
    \left[ \begin{array}{c} 
        \rnnextRHid_i \\
        \rnnextLHid_i \end{array}\right] + \rnnextHidBias \right),
 %p(\bsal_i=1|\sentEmb_i,\ldots,\sentEmb_\docSize) &= \sigma\left(\rnnextPredWeight\rnnextHid_i + \rnnextPredBias  \right)
\end{align}
where $\rnnextHidWeight \in \reals^{\rnnextHidDim \times 2 \rnnextRNNDim}$
and $\rnnextHidBias \in \reals^{\rnnextHidDim}$ are learned parameters.

Another \feedforward~layer
with a logsitic sigmoid activation computes the actual salience 
estimates $\psal_1,\ldots,\psal_\docSize$ where $\psal_i = \model(\bsal_i=1|\sentEmb_1,\ldots,\sentEmb_\docSize;\xParams)$ and 

\vspace{10pt}\noindent\textit{(Fig.~\ref{fig:rnnext}.c) Salience Estimates}
\begin{align}
    \forall i : \;\; i \in \{1,\ldots,\docSize\}& \nonumber \\
    \psal_i &= \model(\bsal_i=1|\sentEmb_i,\ldots,\sentEmb_\docSize; \xParams) = \sigma\left(\rnnextPredWeight\rnnextHid_i + \rnnextPredBias  \right)
\end{align}
where $\rnnextPredWeight \in \reals^{1 \times \rnnextHidDim}$
and $\rnnextPredBias \in \reals$ are learned parameters.

The complete set of parameters for the extractor is 
\[ \xParams = \left\{\rnnextRParams, \rnnextLParams, \rnnextHidWeight, \rnnextHidBias, \rnnextPredWeight, \rnnextPredBias \right\}.\]
In our experiments, we set $\rnnextRNNDim=300$ and $\rnnextHidDim=100$.
Dropout with drop probability of $0.25$ is applied to $\rnnextRHid_i, \rnnextLHid_i,$ and $\rnnextHid_i$ for $i \in \{1,\ldots,\docSize\}$.

\FloatBarrier


