
@inproceedings{wieting2017revisiting,
  author    = {John Wieting and
               Kevin Gimpel},
  editor    = {Regina Barzilay and
               Min{-}Yen Kan},
  title     = {Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume
               1: Long Papers},
  pages     = {2078--2088},
  publisher = {Association for Computational Linguistics},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/P17-1190},
  doi       = {10.18653/v1/P17-1190},
  timestamp = {Tue, 20 Aug 2019 11:59:00 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/WietingG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{arora2016simple,
  title={A simple but tough-to-beat baseline for sentence embeddings},
  author={Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  booktitle={5th International Conference on Learning Representations, ICLR 2017},
  year={2017}
}

@inproceedings{wieting2015towards,
  author    = {John Wieting and
               Mohit Bansal and
               Kevin Gimpel and
               Karen Livescu},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Towards Universal Paraphrastic Sentence Embeddings},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.08198},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WietingBGL15a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{iyyer2015deep,
    title = "Deep Unordered Composition Rivals Syntactic Methods for Text Classification",
    author = "Iyyer, Mohit  and
      Manjunatha, Varun  and
      Boyd-Graber, Jordan  and
      Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1162",
    doi = "10.3115/v1/P15-1162",
    pages = "1681--1691",
}

@InProceedings{glorot2010understanding, title = {Understanding the difficulty of training deep feedforward neural networks}, author = {Xavier Glorot and Yoshua Bengio}, pages = {249--256}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = {http://proceedings.mlr.press/v9/glorot10a.html}, abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.} }

@inproceedings{kingma2014adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{riezler2005pitfalls,
    title = "On Some Pitfalls in Automatic Evaluation and Significance Testing for {MT}",
    author = "Riezler, Stefan  and
      Maxwell, John T.",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W05-0908",
    pages = "57--64",
}


@inproceedings{banerjee2005meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W05-0909",
    pages = "65--72",
}

@inproceedings{durrett2016learning,
    title = "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
    author = "Durrett, Greg  and
      Berg-Kirkpatrick, Taylor  and
      Klein, Dan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1188",
    doi = "10.18653/v1/P16-1188",
    pages = "1998--2008",
}


@inproceedings{sipos2012large,
    title = "Large-Margin Learning of Submodular Summarization Models",
    author = "Sipos, Ruben  and
      Shivaswamy, Pannaga  and
      Joachims, Thorsten",
    booktitle = "Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2012",
    address = "Avignon, France",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E12-1023",
    pages = "224--233",
}

@inproceedings{lin2004rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@inproceedings{ouyang2017crowd,
    title = "Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora",
    author = "Ouyang, Jessica  and
      Chang, Serina  and
      McKeown, Kathy",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2008",
    pages = "46--51",
    abstract = "We present an iterative annotation process for producing aligned, parallel corpora of abstractive and extractive summaries for narrative. Our approach uses a combination of trained annotators and crowd-sourcing, allowing us to elicit human-generated summaries and alignments quickly and at low cost. We use crowd-sourcing to annotate aligned phrases with the text-to-text generation techniques needed to transform each phrase into the other. We apply this process to a corpus of 476 personal narratives, which we make available on the Web.",
}

@article{over2002introduction,
      title={Introduction to duc: An intrinsic evaluation of generic news text summarization systems},
        author={Over, Paul and Liggett, Walter},
          journal={Proc. DUC. http://wwwnlpir. nist. gov/projects/duc/guidelines/2002. html},
            year={2002}
}


@article{sandhaus2008new,
      title={The new york times annotated corpus},
        author={Sandhaus, Evan},
          journal={Linguistic Data Consortium, Philadelphia},
            volume={6},
              number={12},
                pages={e26752},
                  year={2008}
}

@inproceedings{carletta2005ami,
author = {Carletta, Jean and Ashby, Simone and Bourban, Sebastien and Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec, Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and Kronenthal, Melissa and Lathoud, Guillaume and Lincoln, Mike and Lisowska, Agnes and McCowan, Iain and Post, Wilfried and Reidsma, Dennis and Wellner, Pierre},
title = {The AMI Meeting Corpus: A Pre-Announcement},
year = {2005},
isbn = {3540325492},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11677482_3},
doi = {10.1007/11677482_3},
abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
booktitle = {Proceedings of the Second International Conference on Machine Learning for Multimodal Interaction},
pages = {28–39},
numpages = {12},
location = {Edinburgh, UK},
series = {MLMI'05}
}




@inproceedings{see2017pointer,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail and Liu, Peter J.  and Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}


@article{jones1999automatic,
  title={Automatic summarizing: factors and directions},
  author={Jones, Karen Sp{\"a}rck},
  journal={Advances in automatic text summarization},
  pages={1--12},
  year={1999},
  publisher={Cambridge, MA: MIT Press}
}

@book{nenkova2011automatic,
  title={Automatic summarization},
  author={Nenkova, Ani and McKeown, Kathleen},
  year={2011},
  publisher={Now Publishers Inc}
}

@inproceedings{cheng2016neural,
    title = "Neural Summarization by Extracting Sentences and Words",
    author = "Cheng, Jianpeng  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1046",
    doi = "10.18653/v1/P16-1046",
    pages = "484--494",
}

@inproceedings{nallapati2017summarunner,
author = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
title = {SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents},
year = {2017},
publisher = {AAAI Press},
abstract = {We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3075–3081},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17},
url = {https://dl.acm.org/doi/10.5555/3298483.3298681}
}

@inproceedings{kim2014convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@inproceedings{cho2014gru,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@inproceedings{conroy2001,
author = {Conroy, John M. and O'Leary, Dianne P.},
title = {Text Summarization via Hidden Markov Models},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.384042},
doi = {10.1145/383952.384042},
abstract = {A sentence extract summary of a document is a subset of the document's sentences that contains the main ideas in the document. We present an approach to generating such summaries, a hidden Markov model that judges the likelihood that each sentence should be contained in the summary. We compare the results of this method with summaries generated by humans, showing that we obtain significantly higher agreement than do earlier methods.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {406–407},
numpages = {2},
keywords = {text summarization, hidden Markov models, extract summaries, document summarization, automatic summarization},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}




@inproceedings{pennington2014glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}





































@article{luhn1958automatic,
  title = {The automatic creation of literature abstracts},
  author = {Luhn, Hans Peter},
  journal = {IBM Journal of research and development},
  volume ={2},
  number= {2},
  pages = {159--165},
  year = {1958},
  publisher = {IBM},
}

@article{yngve1955syntax,
  title={Syntax and the problem of multiple meaning},
  author={Yngve, Victor H},
  journal={Machine translation of language},
  year={1955},
  publisher={John Wiley \& Sons}
}

@book{harris1962string,
  title={String Analysis of Sentence Structure},
  author={Harris, Z.S.},
  lccn={64002402},
  series={Papers on formal linguistics},
  url={https://books.google.com/books?id=4dssAAAAMAAJ},
  year={1962},
  publisher={Mouton}
}

@book{hyman2016automaton,
  title={The Automaton in English Renaissance Literature},
  author={Hyman, Wendy Beth},
  year={2016},
  publisher={Routledge}
}


@inproceedings{novikova2017e2e,
  title={The E2E Dataset: New Challenges For End-to-End Generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  booktitle={Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  pages={201--206},
  year={2017}
}

@inproceedings{vinyals2015grammar,
  title={Grammar as a foreign language},
  author={Vinyals, Oriol and Kaiser, {\L}ukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  booktitle={Advances in neural information processing systems},
  pages={2773--2781},
  year={2015}
}

@inproceedings{konstas2017neural,
  title={Neural AMR: Sequence-to-Sequence Models for Parsing and Generation},
  author={Konstas, Ioannis and Iyer, Srinivasan and Yatskar, Mark and Choi, Yejin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={146--157},
  year={2017}
}

@inproceedings{gardent2017webnlg,
  title={The webnlg challenge: Generating text from rdf data},
  author={Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and Perez-Beltrachini, Laura},
  booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={124--133},
  year={2017}
}

@inproceedings{mille-etal-2018-first,
    title = "The First Multilingual Surface Realisation Shared Task ({SR}{'}18): Overview and Evaluation Results",
    author = "Mille, Simon  and
      Belz, Anja  and
      Bohnet, Bernd  and
      Graham, Yvette  and
      Pitler, Emily  and
      Wanner, Leo",
    booktitle = "Proceedings of the First Workshop on Multilingual Surface Realisation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-3601",
    doi = "10.18653/v1/W18-3601",
    pages = "1--12",
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}



@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@InCollection{sep-roger-bacon,
        author       =  {Hackett, Jeremiah},
            title        =  {Roger Bacon},
                booktitle    =  {The Stanford Encyclopedia of Philosophy},
                    editor       =  {Edward N. Zalta},
                        howpublished =  {\url{https://plato.stanford.edu/archives/sum2020/entries/roger-bacon/}},
                            year         =  {2020},
                                edition      =  {Summer 2020},
                                    publisher    =  {Metaphysics Research Lab, Stanford University}
}

@book{rosenthal1958muqaddimah,
      title={The Muqaddimah : an introduction to history ; in three volumes.},
        author={Rosenthal, F. and Khald{\=u}n, A. R. I. and Dawood, N. J.},
          isbn={9780691017549},
            lccn={74186373},
              series={Bollingen Series (General) Series},
                url={https://books.google.com/books?id=FlNZ5wmo5LAC},
                  year={1958},
                    publisher={Pantheon Books}
}

@incollection{link2010variantology,
      author      = "Link, David",
        title       = "Scrambling T-R-U-T-H: Rotating Letters as a Material Form of Thought",
          editor      = "Zielinski, S.",
            booktitle   = "Variantology 4: On deep time relations of arts, sciences and technologies in the Arabic-Islamic world and beyond",
              publisher   = {Walther K{\"o}nig},
                address     = "Oxford",
                    pages       = "215-266",
                    year="2010"
}

@book{reiter2000building,
    title={Building natural language generation systems},
    author={Reiter, Ehud and Dale, Robert},
    year={2000},
    publisher={Cambridge university press}
}

@inproceedings{kupiec1995trainable,
      title={A trainable document summarizer},
        author={Kupiec, Julian and Pedersen, Jan and Chen, Francine},
          booktitle={Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval},
            pages={68--73},
              year={1995}
}

@inproceedings{osborne2002using,
      title={Using maximum entropy for sentence extraction},
        author={Osborne, Miles},
          booktitle={Proceedings of the ACL-02 Workshop on Automatic Summarization},
            pages={1--8},
              year={2002}
}

@inproceedings{hirao2002extracting,
      title={Extracting important sentences with support vector machines},
        author={Hirao, Tsutomu and Isozaki, Hideki and Maeda, Eisaku and Matsumoto, Yuji},
          booktitle={COLING 2002: The 19th International Conference on Computational Linguistics},
            year={2002}
}

@article{Crupi2019VolvellesOK,
      title={Volvelles of knowledge. Origin and development of an instrument of scientific imagination (13th-17th centuries)},
        author={Gianfranco Crupi},
          journal={JLIS.it},
            year={2019},
              volume={10},
                pages={1-27}
}

@article{kahn1980,
     ISSN = {00211753, 15456994},
      URL = {http://www.jstor.org/stable/230316},
       author = {David Kahn},
        journal = {Isis},
         number = {1},
          pages = {122--127},
           publisher = {[The University of Chicago Press, The History of Science Society]},
            title = {On the Origin of Polyalphabetic Substitution},
             volume = {71},
              year = {1980}
}


@InCollection{sepllull,
        author       =  {Priani, Ernesto},
            title        =  {Ramon Llull},
                booktitle    =  {The Stanford Encyclopedia of Philosophy},
                    editor       =  {Edward N. Zalta},
                        howpublished =  {\url{https://plato.stanford.edu/archives/spr2017/entries/llull/}},
                            year         =  {2017},
                                edition      =  {Spring 2017},
                                    publisher    =  {Metaphysics Research Lab, Stanford University}
}

@book{knuth2013art,
  title={Art of Computer Programming, Volume 4, Fascicle 4,The: Generating All Trees--History of Combinatorial Generation},
  author={Knuth, D.E.},
  isbn={9780132702348},
  url={https://books.google.com/books?id=56LNfE2QGtYC},
  year={2013},
  publisher={Pearson Education}
}
@book{bonner2007art,
  title={The Art and Logic of Ramon Llull: A User's Guide},
  author={Bonner, A.},
  isbn={9789047431923},
  lccn={2007045315},
  series={Studien und Texte zur Geistesgeschichte des Mittelalters},
  url={https://books.google.com/books?id=UDawCQAAQBAJ},
  year={2007},
  publisher={Brill}
}

@article{reiter1997building,
      title={Building applied natural language generation systems},
        author={Reiter, Ehud and Dale, Robert},
          journal={Natural Language Engineering},
            volume={3},
              number={1},
                pages={57--87},
                  year={1997},
                    publisher={Cambridge university press}
}


@article{schafer2006literary,
      title={Literary machines made in Germany. German proto-cybertexts from the baroque era to the present},
        author={Sch{\"a}fer, J{\"o}rgen},
          journal={Markku Eskelinen/Raine Koskimaa (Hg.): The Cybertext Yearbook Database. Theme Issue on Ergodic Histories},
            pages={1--69},
              year={2006},
                publisher={Citeseer}
}

@article{hutchins2003machine,
      title={Machine translation},
        author={Hutchins, W John},
          year={2003},
            publisher={John Wiley and Sons Ltd.}
}

@article{ornstein1955mechanical,
      title={Mechanical Translation},
        author={Ornstein, Jacob},
          journal={Science},
            volume={122},
              number={3173},
                pages={745--748},
                  year={1955},
                    publisher={JSTOR}
}

@book{national1966language,
      title={Language and machines: computers in translation and linguistics; a report},
        author={National Research Council (US). Automatic Language Processing Advisory Committee},
          volume={1416},
            year={1966},
              publisher={National Academies}
}

@article{gatt2018survey,
      title={Survey of the state of the art in natural language generation: Core tasks, applications and evaluation},
        author={Gatt, Albert and Krahmer, Emiel},
          journal={Journal of Artificial Intelligence Research},
            volume={61},
              pages={65--170},
                year={2018}
}

@book{yngve1961random,
      title={Random generation of English sentences},
        author={Yngve, Victor H},
          year={1961},
            publisher={Massachusetts Inst. of Technology}
}

@techreport{mann1981text,
      title={Text Generation: The State of the Art and the Literature.},
        author={Mann, William C and Bates, Madeline and Grosz, Barbara J and McDonald, David D and McKeown, Kathleen R},
          year={1981},
            institution={UNIVERSITY OF SOUTHERN CALIFORNIA MARINA DEL REY INFORMATION SCIENCES INST}
}


@article{mcdonald2010natural,
      title={Natural Language Generation.},
        author={McDonald, David D},
          journal={Handbook of Natural Language Processing},
            volume={2},
              pages={121--144},
                year={2010}
}

@book{halliday2013halliday,
      title={Halliday's Introduction to Functional Grammar},
        author={Halliday, M.A.K. and Matthiessen, C.M.I.M.},
          isbn={9781135983413},
            series={LSE International Studies},
              url={https://books.google.com/books?id=odUqAAAAQBAJ},
                year={2013},
                  publisher={Taylor \& Francis}
}
@article{chomsky1965aspects,
      title={Aspects of the theory of syntax Cambridge},
        author={Chomsky, Noam},
          journal={Multilingual Matters: MIT Press},
            year={1965}
}

@book{gazdar1985generalized,
      title={Generalized phrase structure grammar},
        author={Gazdar, Gerald and Klein, Ewan and Pullum, Geoffrey K and Sag, Ivan A},
          year={1985},
            publisher={Harvard University Press}
}

@inproceedings{appelt1982planning,
      title={Planning Natural-Language Utterances.},
        author={Appelt, Douglas E},
          booktitle={AAAI},
            pages={59--62},
              year={1982}
}

@techreport{hovy1993natural,
      title={Natural Language Processing by the Penman Project at USC/ISI},
        author={Hovy, Eduard H},
          year={1993},
            institution={UNIVERSITY OF SOUTHERN CALIFORNIA MARINA DEL REY INFORMATION SCIENCES INST}
}

@article{mckeown1982text,
      title={The TEXT system for natural language generation: An overview},
        author={McKeown, Kathleen},
          year={1982}
}

@inproceedings{McDonald1981MUMBLEAF,
      title={MUMBLE: A Flexible System for Language Production},
        author={David B. McDonald},
          booktitle={IJCAI},
            year={1981}
}


@inproceedings{reiter1994has,
      title={Has a consensus NL generation architecture appeared, and is it psycholinguistically plausible?},
        author={Reiter, Ehud},
          booktitle={Proceedings of the Seventh International Workshop on Natural Language Generation},
            pages={163--170},
              year={1994}
}


@article{goldberg1994using,
      title={Using natural-language processing to produce weather forecasts},
        author={Goldberg, Eli and Driedger, Norbert and Kittredge, Richard I},
          journal={IEEE Expert},
            volume={9},
              number={2},
                pages={45--53},
                  year={1994},
                    publisher={IEEE}
}

@inproceedings{iordanskaja-etal-1992-generation,
        title = "Generation of Extended Bilingual Statistical Reports",
            author = "Iordanskaja, L.  and
                      Kim, M.  and
                            Kittredge, R.  and
                                  Lavoie, B.  and
                                        Polguere, A.",
                booktitle = "{COLING} 1992 Volume 3: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics",
                    year = "1992",
                        url = "https://www.aclweb.org/anthology/C92-3158",
}

@article{swartout1983xplain,
      title={XPLAIN: A system for creating and explaining expert consulting programs},
        author={Swartout, William R},
          journal={Artificial intelligence},
            volume={21},
              number={3},
                pages={285--325},
                  year={1983},
                    publisher={Elsevier}
}

@book{todd1992introduction,
      title={An introduction to expert systems},
        author={Todd, Bryan S},
          year={1992}
}

@inproceedings{auli2013joint,
  title={Joint Language and Translation Modeling with Recurrent Neural Networks},
  author={Auli, Michael and Galley, Michel and Quirk, Chris and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1044--1054},
  year={2013}
}


@inproceedings{cho2014learning,
  title={Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1724--1734},
  year={2014}
}



@article{dusek2020,
title = "Evaluating the state-of-the-art of {End}-to-{End} {N}atural {L}anguage {G}eneration: The {E2E} {NLG} challenge",
journal = "Computer Speech \& Language",
volume = "59",
pages = "123 - 156",
year = "2020",
issn = "0885-2308",
doi = "https://doi.org/10.1016/j.csl.2019.06.009",
url = "http://www.sciencedirect.com/science/article/pii/S0885230819300919",
author = "Ondřej Dušek and Jekaterina Novikova and Verena Rieser",
abstract = "This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness – with the winning Slug system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper."
}


@inproceedings{dusek2016,
    title = "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings",
    author = "Du{\v{s}}ek, Ond{\v{r}}ej  and
      Jur{\v{c}}{\'\i}{\v{c}}ek, Filip",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2008",
    doi = "10.18653/v1/P16-2008",
    pages = "45--51",
}


@inproceedings{wen2015,
    title = "Semantically Conditioned {LSTM}-based Natural Language Generation for Spoken Dialogue Systems",
    author = "Wen, Tsung-Hsien  and
      Ga{\v{s}}i{\'c}, Milica  and
      Mrk{\v{s}}i{\'c}, Nikola  and
      Su, Pei-Hao  and
      Vandyke, David  and
      Young, Steve",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1199",
    doi = "10.18653/v1/D15-1199",
    pages = "1711--1721",
}

@inproceedings{dusek2018,
    title = "Findings of the {E}2{E} {NLG} Challenge",
    author = "Du{\v{s}}ek, Ond{\v{r}}ej  and
      Novikova, Jekaterina  and
      Rieser, Verena",
    booktitle = "Proceedings of the 11th International Conference on Natural Language Generation",
    month = nov,
    year = "2018",
    address = "Tilburg University, The Netherlands",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6539",
    doi = "10.18653/v1/W18-6539",
    pages = "322--328",
    abstract = "This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems. Recent end-to-end generation systems are promising since they reduce the need for data annotation. However, they are currently limited to small, delexicalised datasets. The E2E NLG shared task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical richness, syntactic complexity and diverse discourse phenomena. We compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures {--} with the majority implementing sequence-to-sequence models (seq2seq) {--} as well as systems based on grammatical rules and templates.",
}


@inproceedings{elder2018,
    title = "{E}2{E} {NLG} Challenge Submission: Towards Controllable Generation of Diverse Natural Language",
    author = "Elder, Henry  and
      Gehrmann, Sebastian  and
      O{'}Connor, Alexander  and
      Liu, Qun",
    booktitle = "Proceedings of the 11th International Conference on Natural Language Generation",
    month = nov,
    year = "2018",
    address = "Tilburg University, The Netherlands",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6556",
    doi = "10.18653/v1/W18-6556",
    pages = "457--462",
    abstract = "In natural language generation (NLG), the task is to generate utterances from a more abstract input, such as structured data. An added challenge is to generate utterances that contain an accurate representation of the input, while reflecting the fluency and variety of human-generated text. In this paper, we report experiments with NLG models that can be used in task oriented dialogue systems. We explore the use of additional input to the model to encourage diversity and control of outputs. While our submission does not rank highly using automated metrics, qualitative investigation of generated utterances suggests the use of additional information in neural network NLG systems to be a promising research direction.",
}

@inproceedings{juraska2018,
    title = "A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation",
    author = "Juraska, Juraj  and
      Karagiannis, Panagiotis  and
      Bowden, Kevin  and
      Walker, Marilyn",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1014",
    doi = "10.18653/v1/N18-1014",
    pages = "152--162",
    abstract = "Natural language generation lies at the core of generative dialogue systems and conversational agents. We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model. We test the model on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best model. Using a range of automatic metrics, as well as human evaluators, we show that our approach achieves better results than state-of-the-art models on the same datasets.",
}

@inproceedings{mairesse2010,
    title = "Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning",
    author = "Mairesse, Fran{\c{c}}ois  and
      Ga{\v{s}}i{\'c}, Milica  and
      Jur{\v{c}}{\'\i}{\v{c}}ek, Filip  and
      Keizer, Simon  and
      Thomson, Blaise  and
      Yu, Kai  and
      Young, Steve",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P10-1157",
    pages = "1552--1561",
}

@inproceedings{wiseman2017,
    title = "Challenges in Data-to-Document Generation",
    author = "Wiseman, Sam  and
      Shieber, Stuart  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1239",
    doi = "10.18653/v1/D17-1239",
    pages = "2253--2263",
    abstract = "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",
}

@inproceedings{wang2019,
    title = "Revisiting Challenges in Data-to-Text Generation with Fact Grounding",
    author = "Wang, Hongmin",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-8639",
    doi = "10.18653/v1/W19-8639",
    pages = "311--322",
    abstract = "Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60{\%} of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50{\%} more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.",
}


@InProceedings{lake18, title = {Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks}, author = {Lake, Brenden and Baroni, Marco}, pages = {2873--2882}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/lake18a/lake18a.pdf}, url = {http://proceedings.mlr.press/v80/lake18a.html}, abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.} }

@inproceedings{juraska2019,
    title = "{V}i{GGO}: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation",
    author = "Juraska, Juraj  and
      Bowden, Kevin  and
      Walker, Marilyn",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-8623",
    doi = "10.18653/v1/W19-8623",
    pages = "164--172",
    abstract = "The uptake of deep learning in natural language generation (NLG) led to the release of both small and relatively large parallel corpora for training neural models. The existing data-to-text datasets are, however, aimed at task-oriented dialogue systems, and often thus limited in diversity and versatility. They are typically crowdsourced, with much of the noise left in them. Moreover, current neural NLG models do not take full advantage of large training data, and due to their strong generalizing properties produce sentences that look template-like regardless. We therefore present a new corpus of 7K samples, which (1) is clean despite being crowdsourced, (2) has utterances of 9 generalizable and conversational dialogue act types, making it more suitable for open-domain dialogue systems, and (3) explores the domain of video games, which is new to dialogue systems despite having excellent potential for supporting rich conversations.",
}

@inproceedings{novikova2017,
    title = "The {E}2{E} Dataset: New Challenges For End-to-End Generation",
    author = "Novikova, Jekaterina  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Rieser, Verena",
    booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = aug,
    year = "2017",
    address = {Saarbr{\"u}cken, Germany},
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-5525",
    doi = "10.18653/v1/W17-5525",
    pages = "201--206",
    abstract = "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.",
}

@inproceedings{gasic2014,
author={Ga{\v{s}}i{\'c}, M. and Kim, Dongho and Tsiakoulis, Pirros and Breslin, Catherine and Henderson, Matthew and Szummer, Martin and Thomson, Blaise and Young, Steve},
  editor    = {Haizhou Li and
               Helen M. Meng and
               Bin Ma and
               Engsiong Chng and
               Lei Xie},
  title     = {Incremental on-line adaptation of POMDP-based dialogue managers to
               extended domains},
  booktitle = {{INTERSPEECH} 2014, 15th Annual Conference of the International Speech
               Communication Association, Singapore, September 14-18, 2014},
  pages     = {140--144},
  publisher = {{ISCA}},
  year      = {2014},
  url       = {https://www.isca-speech.org/archive/interspeech_2014/i14_0140.html},
  timestamp = {Wed, 12 Sep 2018 16:38:51 +0200},
  biburl    = {https://dblp.org/rec/conf/interspeech/GasicKTBHSTY14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reddy1977,
  title={Speech understanding systems: A summary of results of the five-year research effort},
  author={Reddy, D Raj and others},
  journal={Department of Computer Science. Camegie-Mell University, Pittsburgh, PA},
  volume={17},
  year={1977}
}

@inproceedings{bahdanau2015,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural {M}achine {T}ranslation by {J}ointly {L}earning to {A}lign and {T}ranslate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{luong2015,
    title = "Effective {A}pproaches to {A}ttention-based {N}eural {M}achine {T}ranslation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}


@article{ba2016,
  author    = {Lei Jimmy Ba and
               Jamie Ryan Kiros and
               Geoffrey E. Hinton},
  title     = {Layer {N}ormalization},
  journal   = {CoRR},
  volume    = {abs/1607.06450},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.06450},
  archivePrefix = {arXiv},
  timestamp = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{church1990,
    title = "Word Association Norms, Mutual Information, and Lexicography",
    author = "Church, Kenneth Ward  and
      Hanks, Patrick",
    journal = "Computational Linguistics",
    volume = "16",
    number = "1",
    year = "1990",
    url = "https://www.aclweb.org/anthology/J90-1003",
    pages = "22--29",
}

@incollection{sutskever2014,
title = {Sequence to Sequence Learning with Neural Networks},
author = {Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
booktitle = {Advances in Neural Information Processing Systems 27},
editor = {Z. Ghahramani and M. Welling and C. Cortes and N. D. Lawrence and K. Q. Weinberger},
pages = {3104--3112},
year = {2014},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/5346-sequence-to-sequence-learning-with-neural-networks.pdf}
}

@InProceedings{xu2015,
  title =        {Show, Attend and Tell: Neural Image Caption Generation with Visual Attention},
  author =       {Kelvin Xu and Jimmy Ba and Ryan Kiros and Kyunghyun Cho and Aaron Courville and Ruslan Salakhudinov and Rich Zemel and Yoshua Bengio},
  pages =        {2048--2057},
  year =         {2015},
  editor =       {Francis Bach and David Blei},
  volume =       {37},
  series =       {Proceedings of Machine Learning Research},
  address =      {Lille, France},
  month =        {07--09 Jul},
  publisher =    {PMLR},
  pdf =          {http://proceedings.mlr.press/v37/xuc15.pdf},
  url =          {http://proceedings.mlr.press/v37/xuc15.html},
  abstract =     {Inspired by recent work in machine translation and object detection, we introduce an attention based model that automatically learns to describe the content of images. We describe how we can train this model in a deterministic manner using standard backpropagation techniques and stochastically by maximizing a variational lower bound. We also show through visualization how the model is able to automatically learn to fix its gaze on salient objects while generating the corresponding words in the output sequence. We validate the use of attention with state-of-the-art performance on three benchmark datasets: Flickr8k, Flickr30k and MS COCO.}
}

@incollection{vaswani2017,
title = {Attention is All you Need},
author = {Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, \L ukasz and Polosukhin, Illia},
booktitle = {Advances in Neural Information Processing Systems 30},
editor = {I. Guyon and U. V. Luxburg and S. Bengio and H. Wallach and R. Fergus and S. Vishwanathan and R. Garnett},
pages = {5998--6008},
year = {2017},
publisher = {Curran Associates, Inc.},
url = {http://papers.nips.cc/paper/7181-attention-is-all-you-need.pdf}
}



@inproceedings{fan2018,
    title = "Hierarchical Neural Story Generation",
    author = "Fan, Angela  and
      Lewis, Mike  and
      Dauphin, Yann",
    booktitle = "Proceedings of the 56th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P18-1082",
    doi = "10.18653/v1/P18-1082",
    pages = "889--898",
    abstract = "We explore story generation: creative systems that can build coherent and fluent passages of text about a topic. We collect a large dataset of 300K human-written stories paired with writing prompts from an online forum. Our dataset enables hierarchical story generation, where the model first generates a premise, and then transforms it into a passage of text. We gain further improvements with a novel form of model fusion that improves the relevance of the story to the prompt, and adding a new gated multi-scale self-attention mechanism to model long-range context. Experiments show large improvements over strong baselines on both automated and human evaluations. Human judges prefer stories generated by our approach to those from a strong non-hierarchical model by a factor of two to one.",
}

@article{cho2016,
  author    = {Kyunghyun Cho},
  title     = {Noisy Parallel Approximate Decoding for Conditional Recurrent Language
               Model},
  journal   = {CoRR},
  volume    = {abs/1605.03835},
  year      = {2016},
  url       = {http://arxiv.org/abs/1605.03835},
  archivePrefix = {arXiv},
  eprint    = {1605.03835},
  timestamp = {Mon, 13 Aug 2018 16:48:55 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/Cho16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@book{reiter2000, place={Cambridge}, series={Studies in Natural Language Processing}, title={Building Natural Language Generation Systems}, DOI={10.1017/CBO9780511519857}, publisher={Cambridge University Press}, author={Reiter, Ehud and Dale, Robert}, year={2000}, collection={Studies in Natural Language Processing}}

@inproceedings{walker2001,
    title = "{SP}o{T}: A {T}rainable {S}entence {P}lanner",
    author = "Walker, Marilyn A.  and
      Rambow, Owen  and
      Rogati, Monica",
    booktitle = "Second Meeting of the North {A}merican Chapter of the Association for Computational Linguistics",
    year = "2001",
    url = "https://www.aclweb.org/anthology/N01-1003",
}

@article{stone2003,
author = {Stone, Matthew and Doran, Christine and Webber, Bonnie and Bleam, Ton
ia and Palmer, Martha},
title = {Microplanning with {C}ommunicative {I}ntentions: {T}he {SPUD} {S}ystem},
journal = {Computational Intelligence},
volume = {19},
number = {4},
pages = {311-381},
keywords = {aggregation, lexical choice, lexicalized grammar, microplanning, natural language generation, referring expression generation, syntactic choice},
doi = {10.1046/j.0824-7935.2003.00221.x},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1046/j.0824-7935.2003.00221.x},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1046/j.0824-7935.2003.00221.x},
abstract = {The process of microplanning in natural language generation (NLG) encompasses a range of problems in which a generator must bridge underlying domain-specific representations and general linguistic representations. These problems include constructing linguistic referring expressions to identify domain objects, selecting lexical items to express domain concepts, and using complex linguistic constructions to concisely convey related domain facts. In this paper, we argue that such problems are best solved through a uniform, comprehensive, declarative process. In our approach, the generator directly explores a search space for utterances described by a linguistic grammar. At each stage of search, the generator uses a model of interpretation, which characterizes the potential links between the utterance and the domain and context, to assess its progress in conveying domain-specific representations. We further address the challenges for implementation and knowledge representation in this approach. We show how to implement this approach effectively by using the lexicalized tree-adjoining grammar (LTAG) formalism to connect structure to meaning and using modal logic programming to connect meaning to context. We articulate a detailed methodology for designing grammatical and conceptual resources which the generator can use to achieve desired microplanning behavior in a specified domain. In describing our approach to microplanning, we emphasize that we are in fact realizing a deliberative process of goal-directed activity. As we formulate it, interpretation offers a declarative representation of a generator's communicative intent. It associates the concrete linguistic structure planned by the generator with inferences that show how the meaning of that structure communicates needed information about some application domain in the current discourse context. Thus, interpretations are plans that the microplanner constructs and outputs. At the same time, communicative intent representations provide a rich and uniform resource for the process of NLG. Using representations of communicative intent, a generator can augment the syntax, semantics, and pragmatics of an incomplete sentence simultaneously, and can work incrementally toward solutions for the various problems of microplanning.},
year = {2003}
}



@article{grosz1995,
    title = "{C}entering: {A} {F}ramework for {M}odeling the {L}ocal {C}oherence of {D}iscourse",
    author = "Grosz, Barbara J.  and
      Joshi, Aravind K.  and
      Weinstein, Scott",
    journal = "Computational Linguistics",
    volume = "21",
    number = "2",
    year = "1995",
    url = "https://www.aclweb.org/anthology/J95-2003",
    pages = "203--225",
    publisher={MIT Press},
}

@incollection{ariel2001,
   author = "Ariel, Mira",
   title = "Accessibility theory: {A}n overview",
   booktitle = "Text Representation: Linguistic and psycholinguistic aspects",
   publisher = "John Benjamins",
   year = "2001",
  volume={8},
  pages={29--87},
   url = "https://www.jbe-platform.com/content/books/9789027297679-hcp.8.04ari"
}

@inproceedings{moryossef2019b,
    title = "{S}tep-by-{S}tep: {S}eparating {P}lanning from {R}ealization in {N}eural {D}ata-to-{T}ext {G}eneration",
    author = "Moryossef, Amit  and
      Goldberg, Yoav  and
      Dagan, Ido",
    booktitle = "Proceedings of the 2019 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long and Short Papers)",
    month = jun,
    year = "2019",
    address = "Minneapolis, Minnesota",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N19-1236",
    doi = "10.18653/v1/N19-1236",
    pages = "2267--2277",
    abstract = "Data-to-text generation can be conceptually divided into two parts: ordering and structuring the information (planning), and generating fluent language describing the information (realization). Modern neural generation systems conflate these two steps into a single end-to-end differentiable system. We propose to split the generation process into a symbolic text-planning stage that is faithful to the input, followed by a neural generation stage that focuses only on realization. For training a plan-to-text generator, we present a method for matching reference texts to their corresponding text plans. For inference time, we describe a method for selecting high-quality text plans for new inputs. We implement and evaluate our approach on the WebNLG benchmark. Our results demonstrate that decoupling text planning from neural realization indeed improves the system{'}s reliability and adequacy while maintaining fluent output. We observe improvements both in BLEU scores and in manual evaluations. Another benefit of our approach is the ability to output diverse realizations of the same input, paving the way to explicit control over the generated text structure.",
}

@inproceedings{moryossef2019a,
    title = "Improving {Q}uality and {E}fficiency in {P}lan-based {N}eural {D}ata-to-text {G}eneration",
    author = "Moryossef, Amit  and
      Goldberg, Yoav  and
      Dagan, Ido",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-8645",
    doi = "10.18653/v1/W19-8645",
    pages = "377--382",
    abstract = "We follow the step-by-step approach to neural data-to-text generation proposed by Moryossef et al (2019), in which the generation process is divided into a text planning stage followed by a plan realization stage. We suggest four extensions to that framework: (1) we introduce a trainable neural planning component that can generate effective plans several orders of magnitude faster than the original planner; (2) we incorporate typing hints that improve the model{'}s ability to deal with unseen relations and entities; (3) we introduce a verification-by-reranking stage that substantially improves the faithfulness of the resulting texts; (4) we incorporate a simple but effective referring expression generation module. These extensions result in a generation process that is faster, more fluent, and more accurate.",
}

@inproceedings{castroferreira2019,
    title = "Neural data-to-text generation: A comparison between pipeline and end-to-end architectures",
    author = "Castro Ferreira, Thiago  and
      van der Lee, Chris  and
      van Miltenburg, Emiel  and
      Krahmer, Emiel",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1052",
    doi = "10.18653/v1/D19-1052",
    pages = "552--562",
    abstract = "Traditionally, most data-to-text applications have been designed using a modular pipeline architecture, in which non-linguistic input data is converted into natural language through several intermediate transformations. By contrast, recent neural models for data-to-text generation have been proposed as end-to-end approaches, where the non-linguistic input is rendered in natural language with much less explicit intermediate representations in between. This study introduces a systematic comparison between neural pipeline and end-to-end data-to-text approaches for the generation of text from RDF triples. Both architectures were implemented making use of the encoder-decoder Gated-Recurrent Units (GRU) and Transformer, two state-of-the art deep learning methods. Automatic and human evaluations together with a qualitative analysis suggest that having explicit intermediate steps in the generation process results in better texts than the ones generated by end-to-end approaches. Moreover, the pipeline models generalize better to unseen inputs. Data and code are publicly available.",
}

@article{elman1990,
author = {Elman, Jeffrey L.},
title = {Finding Structure in Time},
journal = {Cognitive Science},
volume = {14},
number = {2},
pages = {179-211},
doi = {10.1207/s15516709cog1402\_1},
url = {https://onlinelibrary.wiley.com/doi/abs/10.1207/s15516709cog1402_1},
eprint = {https://onlinelibrary.wiley.com/doi/pdf/10.1207/s15516709cog1402_1},
abstract = {Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves: the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands: indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent, while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction.},
year = {1990}
}

@INPROCEEDINGS{szegedy2016,
author={C. {Szegedy} and V. {Vanhoucke} and S. {Ioffe} and J. {Shlens} and Z. {Wojna}}, 
booktitle={2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)}, title={Rethinking the Inception Architecture for Computer Vision}, 
year={2016}, 
volume={}, 
number={}, 
pages={2818-2826}, 
doi={10.1109/CVPR.2016.308}}

@article{xie2017neural,
  title={Neural text generation: A practical guide},
  author={Xie, Ziang},
  journal={arXiv preprint arXiv:1711.09534},
  year={2017}
}

@article{welleck2020consistency,
  title={Consistency of a Recurrent Language Model With Respect to Incomplete Decoding},
  author={Welleck, Sean and Kulikov, Ilia and Kim, Jaedeok and Pang, Richard Yuanzhe and Cho, Kyunghyun},
  journal={arXiv preprint arXiv:2002.02492},
  year={2020}
}

@inproceedings{andor2016,
    title = "Globally Normalized Transition-Based Neural Networks",
    author = "Andor, Daniel  and
      Alberti, Chris  and
      Weiss, David  and
      Severyn, Aliaksei  and
      Presta, Alessandro  and
      Ganchev, Kuzman  and
      Petrov, Slav  and
      Collins, Michael",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1231",
    doi = "10.18653/v1/P16-1231",
    pages = "2442--2452",
}

@inproceedings{lafferty2001,
author = {Lafferty, John D. and McCallum, Andrew and Pereira, Fernando C. N.},
title = {Conditional Random Fields: Probabilistic Models for Segmenting and Labeling Sequence Data},
year = {2001},
isbn = {1558607781},
publisher = {Morgan Kaufmann Publishers Inc.},
address = {San Francisco, CA, USA},
booktitle = {Proceedings of the Eighteenth International Conference on Machine Learning},
pages = {282–289},
numpages = {8},
series = {ICML '01}
}

@inproceedings{koehn2017,
    title = "Six Challenges for Neural Machine Translation",
    author = "Koehn, Philipp  and
      Knowles, Rebecca",
    booktitle = "Proceedings of the First Workshop on Neural Machine Translation",
    month = aug,
    year = "2017",
    address = "Vancouver",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-3204",
    doi = "10.18653/v1/W17-3204",
    pages = "28--39",
    abstract = "We explore six challenges for neural machine translation: domain mismatch, amount of training data, rare words, long sentences, word alignment, and beam search. We show both deficiencies and improvements over the quality of phrase-based statistical machine translation.",
}

@inproceedings{stahlberg2019,
    title = "On {NMT} Search Errors and Model Errors: Cat Got Your Tongue?",
    author = "Stahlberg, Felix  and
      Byrne, Bill",
    booktitle = "Proceedings of the 2019 Conference on Empirical Methods in Natural Language Processing and the 9th International Joint Conference on Natural Language Processing (EMNLP-IJCNLP)",
    month = nov,
    year = "2019",
    address = "Hong Kong, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D19-1331",
    doi = "10.18653/v1/D19-1331",
    pages = "3356--3362",
    abstract = "We report on search errors and model errors in neural machine translation (NMT). We present an exact inference procedure for neural sequence models based on a combination of beam search and depth-first search. We use our exact search to find the global best model scores under a Transformer base model for the entire WMT15 English-German test set. Surprisingly, beam search fails to find these global best model scores in most cases, even with a very large beam size of 100. For more than 50{\%} of the sentences, the model in fact assigns its global best score to the empty translation, revealing a massive failure of neural models in properly accounting for adequacy. We show by constraining search with a minimum translation length that at the root of the problem of empty translations lies an inherent bias towards shorter translations. We conclude that vanilla NMT in its current form requires just the right amount of beam search errors, which, from a modelling perspective, is a highly unsatisfactory conclusion indeed, as the model often prefers an empty translation.",
}

@inproceedings{li2016,
    title = "A Diversity-Promoting Objective Function for Neural Conversation Models",
    author = "Li, Jiwei  and
      Galley, Michel  and
      Brockett, Chris  and
      Gao, Jianfeng  and
      Dolan, Bill",
    booktitle = "Proceedings of the 2016 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = jun,
    year = "2016",
    address = "San Diego, California",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N16-1014",
    doi = "10.18653/v1/N16-1014",
    pages = "110--119",
}

@inproceedings{galley2015,
    title = "delta{BLEU}: A Discriminative Metric for Generation Tasks with Intrinsically Diverse Targets",
    author = "Galley, Michel  and
      Brockett, Chris  and
      Sordoni, Alessandro  and
      Ji, Yangfeng  and
      Auli, Michael  and
      Quirk, Chris  and
      Mitchell, Margaret  and
      Gao, Jianfeng  and
      Dolan, Bill",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 2: Short Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-2073",
    doi = "10.3115/v1/P15-2073",
    pages = "445--450",
}

@inproceedings{sordoni2015,
    title = "A Neural Network Approach to Context-Sensitive Generation of Conversational Responses",
    author = "Sordoni, Alessandro  and
      Galley, Michel  and
      Auli, Michael  and
      Brockett, Chris  and
      Ji, Yangfeng  and
      Mitchell, Margaret  and
      Nie, Jian-Yun  and
      Gao, Jianfeng  and
      Dolan, Bill",
    booktitle = "Proceedings of the 2015 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies",
    month = may # "{--}" # jun,
    year = "2015",
    address = "Denver, Colorado",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N15-1020",
    doi = "10.3115/v1/N15-1020",
    pages = "196--205",
}

@inproceedings{serban2016,
author = {Serban, Iulian V. and Sordoni, Alessandro and Bengio, Yoshua and Courville, Aaron and Pineau, Joelle},
title = {Building End-to-End Dialogue Systems Using Generative Hierarchical Neural Network Models},
year = {2016},
publisher = {AAAI Press},
abstract = {We investigate the task of building open domain, conversational dialogue systems based on large dialogue corpora using generative models. Generative models produce system responses that are autonomously generated word-by-word, opening up the possibility for realistic, flexible interactions. In support of this goal, we extend the recently proposed hierarchical recurrent encoder-decoder neural network to the dialogue domain, and demonstrate that this model is competitive with state-of-the-art neural language models and backoff n-gram models. We investigate the limitations of this and similar approaches, and show how its performance can be improved by bootstrapping the learning from a larger question-answer pair corpus and from pretrained word embeddings.},
booktitle = {Proceedings of the Thirtieth AAAI Conference on Artificial Intelligence},
pages = {3776–3783},
numpages = {8},
location = {Phoenix, Arizona},
series = {AAAI'16}
}

@article{vinyals2015,
  title={A neural conversational model},
  author={Vinyals, Oriol and Le, Quoc},
  journal={arXiv preprint arXiv:1506.05869},
  year={2015}
}
