
@inproceedings{wieting2017revisiting,
  author    = {John Wieting and
               Kevin Gimpel},
  editor    = {Regina Barzilay and
               Min{-}Yen Kan},
  title     = {Revisiting Recurrent Networks for Paraphrastic Sentence Embeddings},
  booktitle = {Proceedings of the 55th Annual Meeting of the Association for Computational
               Linguistics, {ACL} 2017, Vancouver, Canada, July 30 - August 4, Volume
               1: Long Papers},
  pages     = {2078--2088},
  publisher = {Association for Computational Linguistics},
  year      = {2017},
  url       = {https://doi.org/10.18653/v1/P17-1190},
  doi       = {10.18653/v1/P17-1190},
  timestamp = {Tue, 20 Aug 2019 11:59:00 +0200},
  biburl    = {https://dblp.org/rec/conf/acl/WietingG17.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{arora2016simple,
  title={A simple but tough-to-beat baseline for sentence embeddings},
  author={Arora, Sanjeev and Liang, Yingyu and Ma, Tengyu},
  booktitle={5th International Conference on Learning Representations, ICLR 2017},
  year={2017}
}

@inproceedings{wieting2015towards,
  author    = {John Wieting and
               Mohit Bansal and
               Kevin Gimpel and
               Karen Livescu},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Towards Universal Paraphrastic Sentence Embeddings},
  booktitle = {4th International Conference on Learning Representations, {ICLR} 2016,
               San Juan, Puerto Rico, May 2-4, 2016, Conference Track Proceedings},
  year      = {2016},
  url       = {http://arxiv.org/abs/1511.08198},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/WietingBGL15a.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{iyyer2015deep,
    title = "Deep Unordered Composition Rivals Syntactic Methods for Text Classification",
    author = "Iyyer, Mohit  and
      Manjunatha, Varun  and
      Boyd-Graber, Jordan  and
      Daum{\'e} III, Hal",
    booktitle = "Proceedings of the 53rd Annual Meeting of the Association for Computational Linguistics and the 7th International Joint Conference on Natural Language Processing (Volume 1: Long Papers)",
    month = jul,
    year = "2015",
    address = "Beijing, China",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P15-1162",
    doi = "10.3115/v1/P15-1162",
    pages = "1681--1691",
}

@InProceedings{glorot2010understanding, title = {Understanding the difficulty of training deep feedforward neural networks}, author = {Xavier Glorot and Yoshua Bengio}, pages = {249--256}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = {http://proceedings.mlr.press/v9/glorot10a.html}, abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.} }

@inproceedings{kingma2014adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{riezler2005pitfalls,
    title = "On Some Pitfalls in Automatic Evaluation and Significance Testing for {MT}",
    author = "Riezler, Stefan  and
      Maxwell, John T.",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W05-0908",
    pages = "57--64",
}


@inproceedings{banerjee2005meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W05-0909",
    pages = "65--72",
}

@inproceedings{durrett2016learning,
    title = "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
    author = "Durrett, Greg  and
      Berg-Kirkpatrick, Taylor  and
      Klein, Dan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1188",
    doi = "10.18653/v1/P16-1188",
    pages = "1998--2008",
}


@inproceedings{sipos2012large,
    title = "Large-Margin Learning of Submodular Summarization Models",
    author = "Sipos, Ruben  and
      Shivaswamy, Pannaga  and
      Joachims, Thorsten",
    booktitle = "Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2012",
    address = "Avignon, France",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E12-1023",
    pages = "224--233",
}

@inproceedings{lin2004rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@inproceedings{ouyang2017crowd,
    title = "Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora",
    author = "Ouyang, Jessica  and
      Chang, Serina  and
      McKeown, Kathy",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2008",
    pages = "46--51",
    abstract = "We present an iterative annotation process for producing aligned, parallel corpora of abstractive and extractive summaries for narrative. Our approach uses a combination of trained annotators and crowd-sourcing, allowing us to elicit human-generated summaries and alignments quickly and at low cost. We use crowd-sourcing to annotate aligned phrases with the text-to-text generation techniques needed to transform each phrase into the other. We apply this process to a corpus of 476 personal narratives, which we make available on the Web.",
}

@article{over2002introduction,
      title={Introduction to duc: An intrinsic evaluation of generic news text summarization systems},
        author={Over, Paul and Liggett, Walter},
          journal={Proc. DUC. http://wwwnlpir. nist. gov/projects/duc/guidelines/2002. html},
            year={2002}
}


@article{sandhaus2008new,
      title={The new york times annotated corpus},
        author={Sandhaus, Evan},
          journal={Linguistic Data Consortium, Philadelphia},
            volume={6},
              number={12},
                pages={e26752},
                  year={2008}
}

@inproceedings{carletta2005ami,
author = {Carletta, Jean and Ashby, Simone and Bourban, Sebastien and Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec, Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and Kronenthal, Melissa and Lathoud, Guillaume and Lincoln, Mike and Lisowska, Agnes and McCowan, Iain and Post, Wilfried and Reidsma, Dennis and Wellner, Pierre},
title = {The AMI Meeting Corpus: A Pre-Announcement},
year = {2005},
isbn = {3540325492},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11677482_3},
doi = {10.1007/11677482_3},
abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
booktitle = {Proceedings of the Second International Conference on Machine Learning for Multimodal Interaction},
pages = {28–39},
numpages = {12},
location = {Edinburgh, UK},
series = {MLMI'05}
}




@inproceedings{see2017pointer,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail and Liu, Peter J.  and Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}


@article{jones1999automatic,
  title={Automatic summarizing: factors and directions},
  author={Jones, Karen Sp{\"a}rck},
  journal={Advances in automatic text summarization},
  pages={1--12},
  year={1999},
  publisher={Cambridge, MA: MIT Press}
}

@book{nenkova2011automatic,
  title={Automatic summarization},
  author={Nenkova, Ani and McKeown, Kathleen},
  year={2011},
  publisher={Now Publishers Inc}
}

@inproceedings{cheng2016neural,
    title = "Neural Summarization by Extracting Sentences and Words",
    author = "Cheng, Jianpeng  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1046",
    doi = "10.18653/v1/P16-1046",
    pages = "484--494",
}

@inproceedings{nallapati2017summarunner,
author = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
title = {SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents},
year = {2017},
publisher = {AAAI Press},
abstract = {We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3075–3081},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17},
url = {https://dl.acm.org/doi/10.5555/3298483.3298681}
}

@inproceedings{kim2014convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@inproceedings{cho2014gru,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@inproceedings{conroy2001,
author = {Conroy, John M. and O'Leary, Dianne P.},
title = {Text Summarization via Hidden Markov Models},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.384042},
doi = {10.1145/383952.384042},
abstract = {A sentence extract summary of a document is a subset of the document's sentences that contains the main ideas in the document. We present an approach to generating such summaries, a hidden Markov model that judges the likelihood that each sentence should be contained in the summary. We compare the results of this method with summaries generated by humans, showing that we obtain significantly higher agreement than do earlier methods.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {406–407},
numpages = {2},
keywords = {text summarization, hidden Markov models, extract summaries, document summarization, automatic summarization},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}




@inproceedings{pennington2014glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}





































@article{luhn1958automatic,
  title = {The automatic creation of literature abstracts},
  author = {Luhn, Hans Peter},
  journal = {IBM Journal of research and development},
  volume ={2},
  number= {2},
  pages = {159--165},
  year = {1958},
  publisher = {IBM},
}

@article{yngve1955syntax,
  title={Syntax and the problem of multiple meaning},
  author={Yngve, Victor H},
  journal={Machine translation of language},
  year={1955},
  publisher={John Wiley \& Sons}
}

@book{harris1962string,
  title={String Analysis of Sentence Structure},
  author={Harris, Z.S.},
  lccn={64002402},
  series={Papers on formal linguistics},
  url={https://books.google.com/books?id=4dssAAAAMAAJ},
  year={1962},
  publisher={Mouton}
}

@book{hyman2016automaton,
  title={The Automaton in English Renaissance Literature},
  author={Hyman, Wendy Beth},
  year={2016},
  publisher={Routledge}
}


@inproceedings{novikova2017e2e,
  title={The E2E Dataset: New Challenges For End-to-End Generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  booktitle={Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  pages={201--206},
  year={2017}
}

@inproceedings{vinyals2015grammar,
  title={Grammar as a foreign language},
  author={Vinyals, Oriol and Kaiser, {\L}ukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  booktitle={Advances in neural information processing systems},
  pages={2773--2781},
  year={2015}
}

@inproceedings{konstas2017neural,
  title={Neural AMR: Sequence-to-Sequence Models for Parsing and Generation},
  author={Konstas, Ioannis and Iyer, Srinivasan and Yatskar, Mark and Choi, Yejin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={146--157},
  year={2017}
}

@inproceedings{gardent2017webnlg,
  title={The webnlg challenge: Generating text from rdf data},
  author={Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and Perez-Beltrachini, Laura},
  booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={124--133},
  year={2017}
}

@inproceedings{mille-etal-2018-first,
    title = "The First Multilingual Surface Realisation Shared Task ({SR}{'}18): Overview and Evaluation Results",
    author = "Mille, Simon  and
      Belz, Anja  and
      Bohnet, Bernd  and
      Graham, Yvette  and
      Pitler, Emily  and
      Wanner, Leo",
    booktitle = "Proceedings of the First Workshop on Multilingual Surface Realisation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-3601",
    doi = "10.18653/v1/W18-3601",
    pages = "1--12",
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}


@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@InCollection{sep-roger-bacon,
        author       =  {Hackett, Jeremiah},
            title        =  {Roger Bacon},
                booktitle    =  {The Stanford Encyclopedia of Philosophy},
                    editor       =  {Edward N. Zalta},
                        howpublished =  {\url{https://plato.stanford.edu/archives/sum2020/entries/roger-bacon/}},
                            year         =  {2020},
                                edition      =  {Summer 2020},
                                    publisher    =  {Metaphysics Research Lab, Stanford University}
}

@book{rosenthal1958muqaddimah,
      title={The Muqaddimah : an introduction to history ; in three volumes.},
        author={Rosenthal, F. and Khald{\=u}n, A. R. I. and Dawood, N. J.},
          isbn={9780691017549},
            lccn={74186373},
              series={Bollingen Series (General) Series},
                url={https://books.google.com/books?id=FlNZ5wmo5LAC},
                  year={1958},
                    publisher={Pantheon Books}
}

@incollection{link2010variantology,
      author      = "Link, David",
        title       = "Scrambling T-R-U-T-H: Rotating Letters as a Material Form of Thought",
          editor      = "Zielinski, S.",
            booktitle   = "Variantology 4: On deep time relations of arts, sciences and technologies in the Arabic-Islamic world and beyond",
              publisher   = {Walther K{\"o}nig},
                address     = "Oxford",
                    pages       = "215-266",
                    year="2010"
}

@book{reiter2000building,
    title={Building natural language generation systems},
    author={Reiter, Ehud and Dale, Robert},
    year={2000},
    publisher={Cambridge university press}
}

@inproceedings{kupiec1995trainable,
      title={A trainable document summarizer},
        author={Kupiec, Julian and Pedersen, Jan and Chen, Francine},
          booktitle={Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval},
            pages={68--73},
              year={1995}
}

@inproceedings{osborne2002using,
      title={Using maximum entropy for sentence extraction},
        author={Osborne, Miles},
          booktitle={Proceedings of the ACL-02 Workshop on Automatic Summarization},
            pages={1--8},
              year={2002}
}

@inproceedings{hirao2002extracting,
      title={Extracting important sentences with support vector machines},
        author={Hirao, Tsutomu and Isozaki, Hideki and Maeda, Eisaku and Matsumoto, Yuji},
          booktitle={COLING 2002: The 19th International Conference on Computational Linguistics},
            year={2002}
}

@article{Crupi2019VolvellesOK,
      title={Volvelles of knowledge. Origin and development of an instrument of scientific imagination (13th-17th centuries)},
        author={Gianfranco Crupi},
          journal={JLIS.it},
            year={2019},
              volume={10},
                pages={1-27}
}

@article{kahn1980,
     ISSN = {00211753, 15456994},
      URL = {http://www.jstor.org/stable/230316},
       author = {David Kahn},
        journal = {Isis},
         number = {1},
          pages = {122--127},
           publisher = {[The University of Chicago Press, The History of Science Society]},
            title = {On the Origin of Polyalphabetic Substitution},
             volume = {71},
              year = {1980}
}


@InCollection{sepllull,
        author       =  {Priani, Ernesto},
            title        =  {Ramon Llull},
                booktitle    =  {The Stanford Encyclopedia of Philosophy},
                    editor       =  {Edward N. Zalta},
                        howpublished =  {\url{https://plato.stanford.edu/archives/spr2017/entries/llull/}},
                            year         =  {2017},
                                edition      =  {Spring 2017},
                                    publisher    =  {Metaphysics Research Lab, Stanford University}
}

@book{knuth2013art,
  title={Art of Computer Programming, Volume 4, Fascicle 4,The: Generating All Trees--History of Combinatorial Generation},
  author={Knuth, D.E.},
  isbn={9780132702348},
  url={https://books.google.com/books?id=56LNfE2QGtYC},
  year={2013},
  publisher={Pearson Education}
}
@book{bonner2007art,
  title={The Art and Logic of Ramon Llull: A User's Guide},
  author={Bonner, A.},
  isbn={9789047431923},
  lccn={2007045315},
  series={Studien und Texte zur Geistesgeschichte des Mittelalters},
  url={https://books.google.com/books?id=UDawCQAAQBAJ},
  year={2007},
  publisher={Brill}
}

@article{reiter1997building,
      title={Building applied natural language generation systems},
        author={Reiter, Ehud and Dale, Robert},
          journal={Natural Language Engineering},
            volume={3},
              number={1},
                pages={57--87},
                  year={1997},
                    publisher={Cambridge university press}
}


@article{schafer2006literary,
      title={Literary machines made in Germany. German proto-cybertexts from the baroque era to the present},
        author={Sch{\"a}fer, J{\"o}rgen},
          journal={Markku Eskelinen/Raine Koskimaa (Hg.): The Cybertext Yearbook Database. Theme Issue on Ergodic Histories},
            pages={1--69},
              year={2006},
                publisher={Citeseer}
}

@article{hutchins2003machine,
      title={Machine translation},
        author={Hutchins, W John},
          year={2003},
            publisher={John Wiley and Sons Ltd.}
}

@article{ornstein1955mechanical,
      title={Mechanical Translation},
        author={Ornstein, Jacob},
          journal={Science},
            volume={122},
              number={3173},
                pages={745--748},
                  year={1955},
                    publisher={JSTOR}
}

@book{national1966language,
      title={Language and machines: computers in translation and linguistics; a report},
        author={National Research Council (US). Automatic Language Processing Advisory Committee},
          volume={1416},
            year={1966},
              publisher={National Academies}
}

@article{gatt2018survey,
      title={Survey of the state of the art in natural language generation: Core tasks, applications and evaluation},
        author={Gatt, Albert and Krahmer, Emiel},
          journal={Journal of Artificial Intelligence Research},
            volume={61},
              pages={65--170},
                year={2018}
}

@book{yngve1961random,
      title={Random generation of English sentences},
        author={Yngve, Victor H},
          year={1961},
            publisher={Massachusetts Inst. of Technology}
}

@techreport{mann1981text,
      title={Text Generation: The State of the Art and the Literature.},
        author={Mann, William C and Bates, Madeline and Grosz, Barbara J and McDonald, David D and McKeown, Kathleen R},
          year={1981},
            institution={UNIVERSITY OF SOUTHERN CALIFORNIA MARINA DEL REY INFORMATION SCIENCES INST}
}


@article{mcdonald2010natural,
      title={Natural Language Generation.},
        author={McDonald, David D},
          journal={Handbook of Natural Language Processing},
            volume={2},
              pages={121--144},
                year={2010}
}

@book{halliday2013halliday,
      title={Halliday's Introduction to Functional Grammar},
        author={Halliday, M.A.K. and Matthiessen, C.M.I.M.},
          isbn={9781135983413},
            series={LSE International Studies},
              url={https://books.google.com/books?id=odUqAAAAQBAJ},
                year={2013},
                  publisher={Taylor \& Francis}
}
@article{chomsky1965aspects,
      title={Aspects of the theory of syntax Cambridge},
        author={Chomsky, Noam},
          journal={Multilingual Matters: MIT Press},
            year={1965}
}

@book{gazdar1985generalized,
      title={Generalized phrase structure grammar},
        author={Gazdar, Gerald and Klein, Ewan and Pullum, Geoffrey K and Sag, Ivan A},
          year={1985},
            publisher={Harvard University Press}
}

@inproceedings{appelt1982planning,
      title={Planning Natural-Language Utterances.},
        author={Appelt, Douglas E},
          booktitle={AAAI},
            pages={59--62},
              year={1982}
}

@techreport{hovy1993natural,
      title={Natural Language Processing by the Penman Project at USC/ISI},
        author={Hovy, Eduard H},
          year={1993},
            institution={UNIVERSITY OF SOUTHERN CALIFORNIA MARINA DEL REY INFORMATION SCIENCES INST}
}

@article{mckeown1982text,
      title={The TEXT system for natural language generation: An overview},
        author={McKeown, Kathleen},
          year={1982}
}

@inproceedings{McDonald1981MUMBLEAF,
      title={MUMBLE: A Flexible System for Language Production},
        author={David B. McDonald},
          booktitle={IJCAI},
            year={1981}
}


@inproceedings{reiter1994has,
      title={Has a consensus NL generation architecture appeared, and is it psycholinguistically plausible?},
        author={Reiter, Ehud},
          booktitle={Proceedings of the Seventh International Workshop on Natural Language Generation},
            pages={163--170},
              year={1994}
}


@article{goldberg1994using,
      title={Using natural-language processing to produce weather forecasts},
        author={Goldberg, Eli and Driedger, Norbert and Kittredge, Richard I},
          journal={IEEE Expert},
            volume={9},
              number={2},
                pages={45--53},
                  year={1994},
                    publisher={IEEE}
}

@inproceedings{iordanskaja-etal-1992-generation,
        title = "Generation of Extended Bilingual Statistical Reports",
            author = "Iordanskaja, L.  and
                      Kim, M.  and
                            Kittredge, R.  and
                                  Lavoie, B.  and
                                        Polguere, A.",
                booktitle = "{COLING} 1992 Volume 3: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics",
                    year = "1992",
                        url = "https://www.aclweb.org/anthology/C92-3158",
}

@article{swartout1983xplain,
      title={XPLAIN: A system for creating and explaining expert consulting programs},
        author={Swartout, William R},
          journal={Artificial intelligence},
            volume={21},
              number={3},
                pages={285--325},
                  year={1983},
                    publisher={Elsevier}
}

@book{todd1992introduction,
      title={An introduction to expert systems},
        author={Todd, Bryan S},
          year={1992}
}

@inproceedings{auli2013joint,
  title={Joint Language and Translation Modeling with Recurrent Neural Networks},
  author={Auli, Michael and Galley, Michel and Quirk, Chris and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1044--1054},
  year={2013}
}


@inproceedings{cho2014learning,
  title={Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1724--1734},
  year={2014}
}



@article{dusek2020,
title = "Evaluating the state-of-the-art of {End}-to-{End} {N}atural {L}anguage {G}eneration: The {E2E} {NLG} challenge",
journal = "Computer Speech \& Language",
volume = "59",
pages = "123 - 156",
year = "2020",
issn = "0885-2308",
doi = "https://doi.org/10.1016/j.csl.2019.06.009",
url = "http://www.sciencedirect.com/science/article/pii/S0885230819300919",
author = "Ondřej Dušek and Jekaterina Novikova and Verena Rieser",
abstract = "This paper provides a comprehensive analysis of the first shared task on End-to-End Natural Language Generation (NLG) and identifies avenues for future research based on the results. This shared task aimed to assess whether recent end-to-end NLG systems can generate more complex output by learning from datasets containing higher lexical richness, syntactic complexity and diverse discourse phenomena. Introducing novel automatic and human metrics, we compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures – with the majority implementing sequence-to-sequence models (seq2seq) – as well as systems based on grammatical rules and templates. Seq2seq-based systems have demonstrated a great potential for NLG in the challenge. We find that seq2seq systems generally score high in terms of word-overlap metrics and human evaluations of naturalness – with the winning Slug system (Juraska et al., 2018) being seq2seq-based. However, vanilla seq2seq models often fail to correctly express a given meaning representation if they lack a strong semantic control mechanism applied during decoding. Moreover, seq2seq models can be outperformed by hand-engineered systems in terms of overall quality, as well as complexity, length and diversity of outputs. This research has influenced, inspired and motivated a number of recent studies outwith the original competition, which we also summarise as part of this paper."
}


@inproceedings{wen2015,
    title = "Semantically Conditioned {LSTM}-based Natural Language Generation for Spoken Dialogue Systems",
    author = "Wen, Tsung-Hsien  and
      Ga{\v{s}}i{\'c}, Milica  and
      Mrk{\v{s}}i{\'c}, Nikola  and
      Su, Pei-Hao  and
      Vandyke, David  and
      Young, Steve",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1199",
    doi = "10.18653/v1/D15-1199",
    pages = "1711--1721",
}

@inproceedings{dusek2018,
    title = "Findings of the {E}2{E} {NLG} Challenge",
    author = "Du{\v{s}}ek, Ond{\v{r}}ej  and
      Novikova, Jekaterina  and
      Rieser, Verena",
    booktitle = "Proceedings of the 11th International Conference on Natural Language Generation",
    month = nov,
    year = "2018",
    address = "Tilburg University, The Netherlands",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6539",
    doi = "10.18653/v1/W18-6539",
    pages = "322--328",
    abstract = "This paper summarises the experimental setup and results of the first shared task on end-to-end (E2E) natural language generation (NLG) in spoken dialogue systems. Recent end-to-end generation systems are promising since they reduce the need for data annotation. However, they are currently limited to small, delexicalised datasets. The E2E NLG shared task aims to assess whether these novel approaches can generate better-quality output by learning from a dataset containing higher lexical richness, syntactic complexity and diverse discourse phenomena. We compare 62 systems submitted by 17 institutions, covering a wide range of approaches, including machine learning architectures {--} with the majority implementing sequence-to-sequence models (seq2seq) {--} as well as systems based on grammatical rules and templates.",
}

@inproceedings{dusek2016,
    title = "Sequence-to-Sequence Generation for Spoken Dialogue via Deep Syntax Trees and Strings",
    author = "Du{\v{s}}ek, Ond{\v{r}}ej  and
      Jur{\v{c}}{\'\i}{\v{c}}ek, Filip",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 2: Short Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-2008",
    doi = "10.18653/v1/P16-2008",
    pages = "45--51",
}

@inproceedings{elder2018,
    title = "{E}2{E} {NLG} Challenge Submission: Towards Controllable Generation of Diverse Natural Language",
    author = "Elder, Henry  and
      Gehrmann, Sebastian  and
      O{'}Connor, Alexander  and
      Liu, Qun",
    booktitle = "Proceedings of the 11th International Conference on Natural Language Generation",
    month = nov,
    year = "2018",
    address = "Tilburg University, The Netherlands",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-6556",
    doi = "10.18653/v1/W18-6556",
    pages = "457--462",
    abstract = "In natural language generation (NLG), the task is to generate utterances from a more abstract input, such as structured data. An added challenge is to generate utterances that contain an accurate representation of the input, while reflecting the fluency and variety of human-generated text. In this paper, we report experiments with NLG models that can be used in task oriented dialogue systems. We explore the use of additional input to the model to encourage diversity and control of outputs. While our submission does not rank highly using automated metrics, qualitative investigation of generated utterances suggests the use of additional information in neural network NLG systems to be a promising research direction.",
}

@inproceedings{juraska2018,
    title = "A Deep Ensemble Model with Slot Alignment for Sequence-to-Sequence Natural Language Generation",
    author = "Juraska, Juraj  and
      Karagiannis, Panagiotis  and
      Bowden, Kevin  and
      Walker, Marilyn",
    booktitle = "Proceedings of the 2018 Conference of the North {A}merican Chapter of the Association for Computational Linguistics: Human Language Technologies, Volume 1 (Long Papers)",
    month = jun,
    year = "2018",
    address = "New Orleans, Louisiana",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/N18-1014",
    doi = "10.18653/v1/N18-1014",
    pages = "152--162",
    abstract = "Natural language generation lies at the core of generative dialogue systems and conversational agents. We describe an ensemble neural language generator, and present several novel methods for data representation and augmentation that yield improved results in our model. We test the model on three datasets in the restaurant, TV and laptop domains, and report both objective and subjective evaluations of our best model. Using a range of automatic metrics, as well as human evaluators, we show that our approach achieves better results than state-of-the-art models on the same datasets.",
}

@inproceedings{mairesse2010,
    title = "Phrase-Based Statistical Language Generation Using Graphical Models and Active Learning",
    author = "Mairesse, Fran{\c{c}}ois  and
      Ga{\v{s}}i{\'c}, Milica  and
      Jur{\v{c}}{\'\i}{\v{c}}ek, Filip  and
      Keizer, Simon  and
      Thomson, Blaise  and
      Yu, Kai  and
      Young, Steve",
    booktitle = "Proceedings of the 48th Annual Meeting of the Association for Computational Linguistics",
    month = jul,
    year = "2010",
    address = "Uppsala, Sweden",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P10-1157",
    pages = "1552--1561",
}

@inproceedings{wiseman2017,
    title = "Challenges in Data-to-Document Generation",
    author = "Wiseman, Sam  and
      Shieber, Stuart  and
      Rush, Alexander",
    booktitle = "Proceedings of the 2017 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2017",
    address = "Copenhagen, Denmark",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D17-1239",
    doi = "10.18653/v1/D17-1239",
    pages = "2253--2263",
    abstract = "Recent neural models have shown significant progress on the problem of generating short descriptive texts conditioned on a small number of database records. In this work, we suggest a slightly more difficult data-to-text generation task, and investigate how effective current approaches are on this task. In particular, we introduce a new, large-scale corpus of data records paired with descriptive documents, propose a series of extractive evaluation methods for analyzing performance, and obtain baseline results using current neural generation methods. Experiments show that these models produce fluent text, but fail to convincingly approximate human-generated documents. Moreover, even templated baselines exceed the performance of these neural models on some metrics, though copy- and reconstruction-based extensions lead to noticeable improvements.",
}

@inproceedings{wang2019,
    title = "Revisiting Challenges in Data-to-Text Generation with Fact Grounding",
    author = "Wang, Hongmin",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-8639",
    doi = "10.18653/v1/W19-8639",
    pages = "311--322",
    abstract = "Data-to-text generation models face challenges in ensuring data fidelity by referring to the correct input source. To inspire studies in this area, Wiseman et al. (2017) introduced the RotoWire corpus on generating NBA game summaries from the box- and line-score tables. However, limited attempts have been made in this direction and the challenges remain. We observe a prominent bottleneck in the corpus where only about 60{\%} of the summary contents can be grounded to the boxscore records. Such information deficiency tends to misguide a conditioned language model to produce unconditioned random facts and thus leads to factual hallucinations. In this work, we restore the information balance and revamp this task to focus on fact-grounded data-to-text generation. We introduce a purified and larger-scale dataset, RotoWire-FG (Fact-Grounding), with 50{\%} more data from the year 2017-19 and enriched input tables, and hope to attract research focuses in this direction. Moreover, we achieve improved data fidelity over the state-of-the-art models by integrating a new form of table reconstruction as an auxiliary task to boost the generation quality.",
}


@InProceedings{lake18, title = {Generalization without Systematicity: On the Compositional Skills of Sequence-to-Sequence Recurrent Networks}, author = {Lake, Brenden and Baroni, Marco}, pages = {2873--2882}, year = {2018}, editor = {Jennifer Dy and Andreas Krause}, volume = {80}, series = {Proceedings of Machine Learning Research}, address = {Stockholmsmässan, Stockholm Sweden}, month = {10--15 Jul}, publisher = {PMLR}, pdf = {http://proceedings.mlr.press/v80/lake18a/lake18a.pdf}, url = {http://proceedings.mlr.press/v80/lake18a.html}, abstract = {Humans can understand and produce new utterances effortlessly, thanks to their compositional skills. Once a person learns the meaning of a new verb "dax," he or she can immediately understand the meaning of "dax twice" or "sing and dax." In this paper, we introduce the SCAN domain, consisting of a set of simple compositional navigation commands paired with the corresponding action sequences. We then test the zero-shot generalization capabilities of a variety of recurrent neural networks (RNNs) trained on SCAN with sequence-to-sequence methods. We find that RNNs can make successful zero-shot generalizations when the differences between training and test commands are small, so that they can apply "mix-and-match" strategies to solve the task. However, when generalization requires systematic compositional skills (as in the "dax" example above), RNNs fail spectacularly. We conclude with a proof-of-concept experiment in neural machine translation, suggesting that lack of systematicity might be partially responsible for neural networks’ notorious training data thirst.} }

@inproceedings{juraska2019,
    title = "{V}i{GGO}: A Video Game Corpus for Data-To-Text Generation in Open-Domain Conversation",
    author = "Juraska, Juraj  and
      Bowden, Kevin  and
      Walker, Marilyn",
    booktitle = "Proceedings of the 12th International Conference on Natural Language Generation",
    month = oct # "{--}" # nov,
    year = "2019",
    address = "Tokyo, Japan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W19-8623",
    doi = "10.18653/v1/W19-8623",
    pages = "164--172",
    abstract = "The uptake of deep learning in natural language generation (NLG) led to the release of both small and relatively large parallel corpora for training neural models. The existing data-to-text datasets are, however, aimed at task-oriented dialogue systems, and often thus limited in diversity and versatility. They are typically crowdsourced, with much of the noise left in them. Moreover, current neural NLG models do not take full advantage of large training data, and due to their strong generalizing properties produce sentences that look template-like regardless. We therefore present a new corpus of 7K samples, which (1) is clean despite being crowdsourced, (2) has utterances of 9 generalizable and conversational dialogue act types, making it more suitable for open-domain dialogue systems, and (3) explores the domain of video games, which is new to dialogue systems despite having excellent potential for supporting rich conversations.",
}

@inproceedings{novikova2017,
    title = "The {E}2{E} Dataset: New Challenges For End-to-End Generation",
    author = "Novikova, Jekaterina  and
      Du{\v{s}}ek, Ond{\v{r}}ej  and
      Rieser, Verena",
    booktitle = "Proceedings of the 18th Annual {SIG}dial Meeting on Discourse and Dialogue",
    month = aug,
    year = "2017",
    address = {Saarbr{\"u}cken, Germany},
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W17-5525",
    doi = "10.18653/v1/W17-5525",
    pages = "201--206",
    abstract = "This paper describes the E2E data, a new dataset for training end-to-end, data-driven natural language generation systems in the restaurant domain, which is ten times bigger than existing, frequently used datasets in this area. The E2E dataset poses new challenges: (1) its human reference texts show more lexical richness and syntactic variation, including discourse phenomena; (2) generating from this set requires content selection. As such, learning from this dataset promises more natural, varied and less template-like system utterances. We also establish a baseline on this dataset, which illustrates some of the difficulties associated with this data.",
}

@inproceedings{gasic2014,
author={Ga{\v{s}}i{\'c}, M. and Kim, Dongho and Tsiakoulis, Pirros and Breslin, Catherine and Henderson, Matthew and Szummer, Martin and Thomson, Blaise and Young, Steve},
  editor    = {Haizhou Li and
               Helen M. Meng and
               Bin Ma and
               Engsiong Chng and
               Lei Xie},
  title     = {Incremental on-line adaptation of POMDP-based dialogue managers to
               extended domains},
  booktitle = {{INTERSPEECH} 2014, 15th Annual Conference of the International Speech
               Communication Association, Singapore, September 14-18, 2014},
  pages     = {140--144},
  publisher = {{ISCA}},
  year      = {2014},
  url       = {https://www.isca-speech.org/archive/interspeech_2014/i14_0140.html},
  timestamp = {Wed, 12 Sep 2018 16:38:51 +0200},
  biburl    = {https://dblp.org/rec/conf/interspeech/GasicKTBHSTY14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reddy1977,
  title={Speech understanding systems: A summary of results of the five-year research effort},
  author={Reddy, D Raj and others},
  journal={Department of Computer Science. Camegie-Mell University, Pittsburgh, PA},
  volume={17},
  year={1977}
}

@inproceedings{bahdanau2015,
  author    = {Dzmitry Bahdanau and
               Kyunghyun Cho and
               Yoshua Bengio},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Neural {M}achine {T}ranslation by {J}ointly {L}earning to {A}lign and {T}ranslate},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1409.0473},
  timestamp = {Wed, 17 Jul 2019 10:40:54 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BahdanauCB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@inproceedings{luong2015,
    title = "Effective {A}pproaches to {A}ttention-based {N}eural {M}achine {T}ranslation",
    author = "Luong, Thang  and
      Pham, Hieu  and
      Manning, Christopher D.",
    booktitle = "Proceedings of the 2015 Conference on Empirical Methods in Natural Language Processing",
    month = sep,
    year = "2015",
    address = "Lisbon, Portugal",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D15-1166",
    doi = "10.18653/v1/D15-1166",
    pages = "1412--1421",
}


@article{ba2016,
  author    = {Lei Jimmy Ba and
               Jamie Ryan Kiros and
               Geoffrey E. Hinton},
  title     = {Layer {N}ormalization},
  journal   = {CoRR},
  volume    = {abs/1607.06450},
  year      = {2016},
  url       = {http://arxiv.org/abs/1607.06450},
  archivePrefix = {arXiv},
  timestamp = {Tue, 23 Jul 2019 17:33:23 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/BaKH16.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}
