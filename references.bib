

@InProceedings{glorot2010understanding, title = {Understanding the difficulty of training deep feedforward neural networks}, author = {Xavier Glorot and Yoshua Bengio}, pages = {249--256}, year = {2010}, editor = {Yee Whye Teh and Mike Titterington}, volume = {9}, series = {Proceedings of Machine Learning Research}, address = {Chia Laguna Resort, Sardinia, Italy}, month = {13--15 May}, publisher = {JMLR Workshop and Conference Proceedings}, pdf = {http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf}, url = {http://proceedings.mlr.press/v9/glorot10a.html}, abstract = {Whereas before 2006 it appears that deep multi-layer neural networks were not successfully trained, since then several algorithms have been shown to successfully train them, with experimental results showing the superiority of deeper vs less deep architectures. All these experimental results were obtained with new initialization or training mechanisms. Our objective here is to understand better why standard gradient descent from random initialization is doing so poorly with deep neural networks, to better understand these recent relative successes and help design better algorithms in the future. We first observe the influence of the non-linear activations functions. We find that the logistic sigmoid activation is unsuited for deep networks with random initialization because of its mean value, which can drive especially the top hidden layer into saturation. Surprisingly, we find that saturated units can move out of saturation by themselves, albeit slowly, and explaining the plateaus sometimes seen when training neural networks. We find that a new non-linearity that saturates less can often be beneficial. Finally, we study how activations and gradients vary across layers and during training, with the idea that training may be more difficult when the singular values of the Jacobian associated with each layer are far from 1. Based on these considerations, we propose a new initialization scheme that brings substantially faster convergence.} }

@inproceedings{kingma2014adam,
  author    = {Diederik P. Kingma and
               Jimmy Ba},
  editor    = {Yoshua Bengio and
               Yann LeCun},
  title     = {Adam: {A} Method for Stochastic Optimization},
  booktitle = {3rd International Conference on Learning Representations, {ICLR} 2015,
               San Diego, CA, USA, May 7-9, 2015, Conference Track Proceedings},
  year      = {2015},
  url       = {http://arxiv.org/abs/1412.6980},
  timestamp = {Thu, 25 Jul 2019 14:25:37 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/KingmaB14.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}


@inproceedings{riezler2005pitfalls,
    title = "On Some Pitfalls in Automatic Evaluation and Significance Testing for {MT}",
    author = "Riezler, Stefan  and
      Maxwell, John T.",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W05-0908",
    pages = "57--64",
}


@inproceedings{banerjee2005meteor,
    title = "{METEOR}: An Automatic Metric for {MT} Evaluation with Improved Correlation with Human Judgments",
    author = "Banerjee, Satanjeev  and
      Lavie, Alon",
    booktitle = "Proceedings of the {ACL} Workshop on Intrinsic and Extrinsic Evaluation Measures for Machine Translation and/or Summarization",
    month = jun,
    year = "2005",
    address = "Ann Arbor, Michigan",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W05-0909",
    pages = "65--72",
}

@inproceedings{durrett2016learning,
    title = "Learning-Based Single-Document Summarization with Compression and Anaphoricity Constraints",
    author = "Durrett, Greg  and
      Berg-Kirkpatrick, Taylor  and
      Klein, Dan",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1188",
    doi = "10.18653/v1/P16-1188",
    pages = "1998--2008",
}


@inproceedings{sipos2012large,
    title = "Large-Margin Learning of Submodular Summarization Models",
    author = "Sipos, Ruben  and
      Shivaswamy, Pannaga  and
      Joachims, Thorsten",
    booktitle = "Proceedings of the 13th Conference of the {E}uropean Chapter of the Association for Computational Linguistics",
    month = apr,
    year = "2012",
    address = "Avignon, France",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E12-1023",
    pages = "224--233",
}

@inproceedings{lin2004rouge,
    title = "{ROUGE}: A Package for Automatic Evaluation of Summaries",
    author = "Lin, Chin-Yew",
    booktitle = "Text Summarization Branches Out",
    month = jul,
    year = "2004",
    address = "Barcelona, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W04-1013",
    pages = "74--81",
}

@inproceedings{ouyang2017crowd,
    title = "Crowd-Sourced Iterative Annotation for Narrative Summarization Corpora",
    author = "Ouyang, Jessica  and
      Chang, Serina  and
      McKeown, Kathy",
    booktitle = "Proceedings of the 15th Conference of the {E}uropean Chapter of the Association for Computational Linguistics: Volume 2, Short Papers",
    month = apr,
    year = "2017",
    address = "Valencia, Spain",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/E17-2008",
    pages = "46--51",
    abstract = "We present an iterative annotation process for producing aligned, parallel corpora of abstractive and extractive summaries for narrative. Our approach uses a combination of trained annotators and crowd-sourcing, allowing us to elicit human-generated summaries and alignments quickly and at low cost. We use crowd-sourcing to annotate aligned phrases with the text-to-text generation techniques needed to transform each phrase into the other. We apply this process to a corpus of 476 personal narratives, which we make available on the Web.",
}

@article{over2002introduction,
      title={Introduction to duc: An intrinsic evaluation of generic news text summarization systems},
        author={Over, Paul and Liggett, Walter},
          journal={Proc. DUC. http://wwwnlpir. nist. gov/projects/duc/guidelines/2002. html},
            year={2002}
}


@article{sandhaus2008new,
      title={The new york times annotated corpus},
        author={Sandhaus, Evan},
          journal={Linguistic Data Consortium, Philadelphia},
            volume={6},
              number={12},
                pages={e26752},
                  year={2008}
}

@inproceedings{carletta2005ami,
author = {Carletta, Jean and Ashby, Simone and Bourban, Sebastien and Flynn, Mike and Guillemot, Mael and Hain, Thomas and Kadlec, Jaroslav and Karaiskos, Vasilis and Kraaij, Wessel and Kronenthal, Melissa and Lathoud, Guillaume and Lincoln, Mike and Lisowska, Agnes and McCowan, Iain and Post, Wilfried and Reidsma, Dennis and Wellner, Pierre},
title = {The AMI Meeting Corpus: A Pre-Announcement},
year = {2005},
isbn = {3540325492},
publisher = {Springer-Verlag},
address = {Berlin, Heidelberg},
url = {https://doi.org/10.1007/11677482_3},
doi = {10.1007/11677482_3},
abstract = {The AMI Meeting Corpus is a multi-modal data set consisting of 100 hours of meeting recordings. It is being created in the context of a project that is developing meeting browsing technology and will eventually be released publicly. Some of the meetings it contains are naturally occurring, and some are elicited, particularly using a scenario in which the participants play different roles in a design team, taking a design project from kick-off to completion over the course of a day. The corpus is being recorded using a wide range of devices including close-talking and far-field microphones, individual and room-view video cameras, projection, a whiteboard, and individual pens, all of which produce output signals that are synchronized with each other. It is also being hand-annotated for many different phenomena, including orthographic transcription, discourse properties such as named entities and dialogue acts, summaries, emotions, and some head and hand gestures. We describe the data set, including the rationale behind using elicited material, and explain how the material is being recorded, transcribed and annotated.},
booktitle = {Proceedings of the Second International Conference on Machine Learning for Multimodal Interaction},
pages = {28–39},
numpages = {12},
location = {Edinburgh, UK},
series = {MLMI'05}
}




@inproceedings{see2017pointer,
    title = "Get To The Point: Summarization with Pointer-Generator Networks",
    author = "See, Abigail and Liu, Peter J.  and Manning, Christopher D.",
    booktitle = "Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = jul,
    year = "2017",
    address = "Vancouver, Canada",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P17-1099",
    doi = "10.18653/v1/P17-1099",
    pages = "1073--1083",
    abstract = "Neural sequence-to-sequence models have provided a viable new approach for abstractive text summarization (meaning they are not restricted to simply selecting and rearranging passages from the original text). However, these models have two shortcomings: they are liable to reproduce factual details inaccurately, and they tend to repeat themselves. In this work we propose a novel architecture that augments the standard sequence-to-sequence attentional model in two orthogonal ways. First, we use a hybrid pointer-generator network that can copy words from the source text via pointing, which aids accurate reproduction of information, while retaining the ability to produce novel words through the generator. Second, we use coverage to keep track of what has been summarized, which discourages repetition. We apply our model to the CNN / Daily Mail summarization task, outperforming the current abstractive state-of-the-art by at least 2 ROUGE points.",
}


@article{jones1999automatic,
  title={Automatic summarizing: factors and directions},
  author={Jones, Karen Sp{\"a}rck},
  journal={Advances in automatic text summarization},
  pages={1--12},
  year={1999},
  publisher={Cambridge, MA: MIT Press}
}

@book{nenkova2011automatic,
  title={Automatic summarization},
  author={Nenkova, Ani and McKeown, Kathleen},
  year={2011},
  publisher={Now Publishers Inc}
}

@inproceedings{cheng2016neural,
    title = "Neural Summarization by Extracting Sentences and Words",
    author = "Cheng, Jianpeng  and
      Lapata, Mirella",
    booktitle = "Proceedings of the 54th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)",
    month = aug,
    year = "2016",
    address = "Berlin, Germany",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/P16-1046",
    doi = "10.18653/v1/P16-1046",
    pages = "484--494",
}

@inproceedings{nallapati2017summarunner,
author = {Nallapati, Ramesh and Zhai, Feifei and Zhou, Bowen},
title = {SummaRuNNer: A Recurrent Neural Network Based Sequence Model for Extractive Summarization of Documents},
year = {2017},
publisher = {AAAI Press},
abstract = {We present SummaRuNNer, a Recurrent Neural Network (RNN) based sequence model for extractive summarization of documents and show that it achieves performance better than or comparable to state-of-the-art. Our model has the additional advantage of being very interpretable, since it allows visualization of its predictions broken up by abstract features such as information content, salience and novelty. Another novel contribution of our work is abstractive training of our extractive model that can train on human generated reference summaries alone, eliminating the need for sentence-level extractive labels.},
booktitle = {Proceedings of the Thirty-First AAAI Conference on Artificial Intelligence},
pages = {3075–3081},
numpages = {7},
location = {San Francisco, California, USA},
series = {AAAI'17},
url = {https://dl.acm.org/doi/10.5555/3298483.3298681}
}

@inproceedings{kim2014convolutional,
    title = "Convolutional Neural Networks for Sentence Classification",
    author = "Kim, Yoon",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1181",
    doi = "10.3115/v1/D14-1181",
    pages = "1746--1751",
}

@inproceedings{cho2014gru,
    title = "On the Properties of Neural Machine Translation: Encoder{--}Decoder Approaches",
    author = {Cho, Kyunghyun  and
      van Merri{\"e}nboer, Bart  and
      Bahdanau, Dzmitry  and
      Bengio, Yoshua},
    booktitle = "Proceedings of {SSST}-8, Eighth Workshop on Syntax, Semantics and Structure in Statistical Translation",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W14-4012",
    doi = "10.3115/v1/W14-4012",
    pages = "103--111",
}

@inproceedings{conroy2001,
author = {Conroy, John M. and O'Leary, Dianne P.},
title = {Text Summarization via Hidden Markov Models},
year = {2001},
isbn = {1581133316},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/383952.384042},
doi = {10.1145/383952.384042},
abstract = {A sentence extract summary of a document is a subset of the document's sentences that contains the main ideas in the document. We present an approach to generating such summaries, a hidden Markov model that judges the likelihood that each sentence should be contained in the summary. We compare the results of this method with summaries generated by humans, showing that we obtain significantly higher agreement than do earlier methods.},
booktitle = {Proceedings of the 24th Annual International ACM SIGIR Conference on Research and Development in Information Retrieval},
pages = {406–407},
numpages = {2},
keywords = {text summarization, hidden Markov models, extract summaries, document summarization, automatic summarization},
location = {New Orleans, Louisiana, USA},
series = {SIGIR '01}
}




@inproceedings{pennington2014glove,
    title = "{G}lo{V}e: Global Vectors for Word Representation",
    author = "Pennington, Jeffrey  and
      Socher, Richard  and
      Manning, Christopher",
    booktitle = "Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing ({EMNLP})",
    month = oct,
    year = "2014",
    address = "Doha, Qatar",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/D14-1162",
    doi = "10.3115/v1/D14-1162",
    pages = "1532--1543",
}





































@article{luhn1958automatic,
  title = {The automatic creation of literature abstracts},
  author = {Luhn, Hans Peter},
  journal = {IBM Journal of research and development},
  volume ={2},
  number= {2},
  pages = {159--165},
  year = {1958},
  publisher = {IBM},
}

@article{yngve1955syntax,
  title={Syntax and the problem of multiple meaning},
  author={Yngve, Victor H},
  journal={Machine translation of language},
  year={1955},
  publisher={John Wiley \& Sons}
}

@book{harris1962string,
  title={String Analysis of Sentence Structure},
  author={Harris, Z.S.},
  lccn={64002402},
  series={Papers on formal linguistics},
  url={https://books.google.com/books?id=4dssAAAAMAAJ},
  year={1962},
  publisher={Mouton}
}

@book{hyman2016automaton,
  title={The Automaton in English Renaissance Literature},
  author={Hyman, Wendy Beth},
  year={2016},
  publisher={Routledge}
}


@inproceedings{novikova2017e2e,
  title={The E2E Dataset: New Challenges For End-to-End Generation},
  author={Novikova, Jekaterina and Du{\v{s}}ek, Ond{\v{r}}ej and Rieser, Verena},
  booktitle={Proceedings of the 18th Annual SIGdial Meeting on Discourse and Dialogue},
  pages={201--206},
  year={2017}
}

@inproceedings{vinyals2015grammar,
  title={Grammar as a foreign language},
  author={Vinyals, Oriol and Kaiser, {\L}ukasz and Koo, Terry and Petrov, Slav and Sutskever, Ilya and Hinton, Geoffrey},
  booktitle={Advances in neural information processing systems},
  pages={2773--2781},
  year={2015}
}

@inproceedings{konstas2017neural,
  title={Neural AMR: Sequence-to-Sequence Models for Parsing and Generation},
  author={Konstas, Ioannis and Iyer, Srinivasan and Yatskar, Mark and Choi, Yejin and Zettlemoyer, Luke},
  booktitle={Proceedings of the 55th Annual Meeting of the Association for Computational Linguistics (Volume 1: Long Papers)},
  pages={146--157},
  year={2017}
}

@inproceedings{gardent2017webnlg,
  title={The webnlg challenge: Generating text from rdf data},
  author={Gardent, Claire and Shimorina, Anastasia and Narayan, Shashi and Perez-Beltrachini, Laura},
  booktitle={Proceedings of the 10th International Conference on Natural Language Generation},
  pages={124--133},
  year={2017}
}

@inproceedings{mille-etal-2018-first,
    title = "The First Multilingual Surface Realisation Shared Task ({SR}{'}18): Overview and Evaluation Results",
    author = "Mille, Simon  and
      Belz, Anja  and
      Bohnet, Bernd  and
      Graham, Yvette  and
      Pitler, Emily  and
      Wanner, Leo",
    booktitle = "Proceedings of the First Workshop on Multilingual Surface Realisation",
    month = jul,
    year = "2018",
    address = "Melbourne, Australia",
    publisher = "Association for Computational Linguistics",
    url = "https://www.aclweb.org/anthology/W18-3601",
    doi = "10.18653/v1/W18-3601",
    pages = "1--12",
}

@article{radford2019language,
  title={Language models are unsupervised multitask learners},
  author={Radford, Alec and Wu, Jeffrey and Child, Rewon and Luan, David and Amodei, Dario and Sutskever, Ilya}
}

@article{lewis2019bart,
  title={Bart: Denoising sequence-to-sequence pre-training for natural language generation, translation, and comprehension},
  author={Lewis, Mike and Liu, Yinhan and Goyal, Naman and Ghazvininejad, Marjan and Mohamed, Abdelrahman and Levy, Omer and Stoyanov, Ves and Zettlemoyer, Luke},
  journal={arXiv preprint arXiv:1910.13461},
  year={2019}
}

@inproceedings{sutskever2014sequence,
  title={Sequence to sequence learning with neural networks},
  author={Sutskever, Ilya and Vinyals, Oriol and Le, Quoc V},
  booktitle={Advances in neural information processing systems},
  pages={3104--3112},
  year={2014}
}

@article{bahdanau2014neural,
  title={Neural machine translation by jointly learning to align and translate},
  author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
  journal={arXiv preprint arXiv:1409.0473},
  year={2014}
}

@inproceedings{vaswani2017attention,
  title={Attention is all you need},
  author={Vaswani, Ashish and Shazeer, Noam and Parmar, Niki and Uszkoreit, Jakob and Jones, Llion and Gomez, Aidan N and Kaiser, {\L}ukasz and Polosukhin, Illia},
  booktitle={Advances in neural information processing systems},
  pages={5998--6008},
  year={2017}
}

@article{wu2016google,
  title={Google's neural machine translation system: Bridging the gap between human and machine translation},
  author={Wu, Yonghui and Schuster, Mike and Chen, Zhifeng and Le, Quoc V and Norouzi, Mohammad and Macherey, Wolfgang and Krikun, Maxim and Cao, Yuan and Gao, Qin and Macherey, Klaus and others},
  journal={arXiv preprint arXiv:1609.08144},
  year={2016}
}

@InCollection{sep-roger-bacon,
        author       =  {Hackett, Jeremiah},
            title        =  {Roger Bacon},
                booktitle    =  {The Stanford Encyclopedia of Philosophy},
                    editor       =  {Edward N. Zalta},
                        howpublished =  {\url{https://plato.stanford.edu/archives/sum2020/entries/roger-bacon/}},
                            year         =  {2020},
                                edition      =  {Summer 2020},
                                    publisher    =  {Metaphysics Research Lab, Stanford University}
}

@book{rosenthal1958muqaddimah,
      title={The Muqaddimah : an introduction to history ; in three volumes.},
        author={Rosenthal, F. and Khald{\=u}n, A. R. I. and Dawood, N. J.},
          isbn={9780691017549},
            lccn={74186373},
              series={Bollingen Series (General) Series},
                url={https://books.google.com/books?id=FlNZ5wmo5LAC},
                  year={1958},
                    publisher={Pantheon Books}
}

@incollection{link2010variantology,
      author      = "Link, David",
        title       = "Scrambling T-R-U-T-H: Rotating Letters as a Material Form of Thought",
          editor      = "Zielinski, S.",
            booktitle   = "Variantology 4: On deep time relations of arts, sciences and technologies in the Arabic-Islamic world and beyond",
              publisher   = {Walther K{\"o}nig},
                address     = "Oxford",
                    pages       = "215-266",
                    year="2010"
}

@book{reiter2000building,
    title={Building natural language generation systems},
    author={Reiter, Ehud and Dale, Robert},
    year={2000},
    publisher={Cambridge university press}
}

@inproceedings{kupiec1995trainable,
      title={A trainable document summarizer},
        author={Kupiec, Julian and Pedersen, Jan and Chen, Francine},
          booktitle={Proceedings of the 18th annual international ACM SIGIR conference on Research and development in information retrieval},
            pages={68--73},
              year={1995}
}

@inproceedings{osborne2002using,
      title={Using maximum entropy for sentence extraction},
        author={Osborne, Miles},
          booktitle={Proceedings of the ACL-02 Workshop on Automatic Summarization},
            pages={1--8},
              year={2002}
}

@inproceedings{hirao2002extracting,
      title={Extracting important sentences with support vector machines},
        author={Hirao, Tsutomu and Isozaki, Hideki and Maeda, Eisaku and Matsumoto, Yuji},
          booktitle={COLING 2002: The 19th International Conference on Computational Linguistics},
            year={2002}
}

@article{Crupi2019VolvellesOK,
      title={Volvelles of knowledge. Origin and development of an instrument of scientific imagination (13th-17th centuries)},
        author={Gianfranco Crupi},
          journal={JLIS.it},
            year={2019},
              volume={10},
                pages={1-27}
}

@article{kahn1980,
     ISSN = {00211753, 15456994},
      URL = {http://www.jstor.org/stable/230316},
       author = {David Kahn},
        journal = {Isis},
         number = {1},
          pages = {122--127},
           publisher = {[The University of Chicago Press, The History of Science Society]},
            title = {On the Origin of Polyalphabetic Substitution},
             volume = {71},
              year = {1980}
}


@InCollection{sepllull,
        author       =  {Priani, Ernesto},
            title        =  {Ramon Llull},
                booktitle    =  {The Stanford Encyclopedia of Philosophy},
                    editor       =  {Edward N. Zalta},
                        howpublished =  {\url{https://plato.stanford.edu/archives/spr2017/entries/llull/}},
                            year         =  {2017},
                                edition      =  {Spring 2017},
                                    publisher    =  {Metaphysics Research Lab, Stanford University}
}

@book{knuth2013art,
  title={Art of Computer Programming, Volume 4, Fascicle 4,The: Generating All Trees--History of Combinatorial Generation},
  author={Knuth, D.E.},
  isbn={9780132702348},
  url={https://books.google.com/books?id=56LNfE2QGtYC},
  year={2013},
  publisher={Pearson Education}
}
@book{bonner2007art,
  title={The Art and Logic of Ramon Llull: A User's Guide},
  author={Bonner, A.},
  isbn={9789047431923},
  lccn={2007045315},
  series={Studien und Texte zur Geistesgeschichte des Mittelalters},
  url={https://books.google.com/books?id=UDawCQAAQBAJ},
  year={2007},
  publisher={Brill}
}

@article{reiter1997building,
      title={Building applied natural language generation systems},
        author={Reiter, Ehud and Dale, Robert},
          journal={Natural Language Engineering},
            volume={3},
              number={1},
                pages={57--87},
                  year={1997},
                    publisher={Cambridge university press}
}


@article{schafer2006literary,
      title={Literary machines made in Germany. German proto-cybertexts from the baroque era to the present},
        author={Sch{\"a}fer, J{\"o}rgen},
          journal={Markku Eskelinen/Raine Koskimaa (Hg.): The Cybertext Yearbook Database. Theme Issue on Ergodic Histories},
            pages={1--69},
              year={2006},
                publisher={Citeseer}
}

@article{hutchins2003machine,
      title={Machine translation},
        author={Hutchins, W John},
          year={2003},
            publisher={John Wiley and Sons Ltd.}
}

@article{ornstein1955mechanical,
      title={Mechanical Translation},
        author={Ornstein, Jacob},
          journal={Science},
            volume={122},
              number={3173},
                pages={745--748},
                  year={1955},
                    publisher={JSTOR}
}

@book{national1966language,
      title={Language and machines: computers in translation and linguistics; a report},
        author={National Research Council (US). Automatic Language Processing Advisory Committee},
          volume={1416},
            year={1966},
              publisher={National Academies}
}

@article{gatt2018survey,
      title={Survey of the state of the art in natural language generation: Core tasks, applications and evaluation},
        author={Gatt, Albert and Krahmer, Emiel},
          journal={Journal of Artificial Intelligence Research},
            volume={61},
              pages={65--170},
                year={2018}
}

@book{yngve1961random,
      title={Random generation of English sentences},
        author={Yngve, Victor H},
          year={1961},
            publisher={Massachusetts Inst. of Technology}
}

@techreport{mann1981text,
      title={Text Generation: The State of the Art and the Literature.},
        author={Mann, William C and Bates, Madeline and Grosz, Barbara J and McDonald, David D and McKeown, Kathleen R},
          year={1981},
            institution={UNIVERSITY OF SOUTHERN CALIFORNIA MARINA DEL REY INFORMATION SCIENCES INST}
}


@article{mcdonald2010natural,
      title={Natural Language Generation.},
        author={McDonald, David D},
          journal={Handbook of Natural Language Processing},
            volume={2},
              pages={121--144},
                year={2010}
}

@book{halliday2013halliday,
      title={Halliday's Introduction to Functional Grammar},
        author={Halliday, M.A.K. and Matthiessen, C.M.I.M.},
          isbn={9781135983413},
            series={LSE International Studies},
              url={https://books.google.com/books?id=odUqAAAAQBAJ},
                year={2013},
                  publisher={Taylor \& Francis}
}
@article{chomsky1965aspects,
      title={Aspects of the theory of syntax Cambridge},
        author={Chomsky, Noam},
          journal={Multilingual Matters: MIT Press},
            year={1965}
}

@book{gazdar1985generalized,
      title={Generalized phrase structure grammar},
        author={Gazdar, Gerald and Klein, Ewan and Pullum, Geoffrey K and Sag, Ivan A},
          year={1985},
            publisher={Harvard University Press}
}

@inproceedings{appelt1982planning,
      title={Planning Natural-Language Utterances.},
        author={Appelt, Douglas E},
          booktitle={AAAI},
            pages={59--62},
              year={1982}
}

@techreport{hovy1993natural,
      title={Natural Language Processing by the Penman Project at USC/ISI},
        author={Hovy, Eduard H},
          year={1993},
            institution={UNIVERSITY OF SOUTHERN CALIFORNIA MARINA DEL REY INFORMATION SCIENCES INST}
}

@article{mckeown1982text,
      title={The TEXT system for natural language generation: An overview},
        author={McKeown, Kathleen},
          year={1982}
}

@inproceedings{McDonald1981MUMBLEAF,
      title={MUMBLE: A Flexible System for Language Production},
        author={David B. McDonald},
          booktitle={IJCAI},
            year={1981}
}


@inproceedings{reiter1994has,
      title={Has a consensus NL generation architecture appeared, and is it psycholinguistically plausible?},
        author={Reiter, Ehud},
          booktitle={Proceedings of the Seventh International Workshop on Natural Language Generation},
            pages={163--170},
              year={1994}
}


@article{goldberg1994using,
      title={Using natural-language processing to produce weather forecasts},
        author={Goldberg, Eli and Driedger, Norbert and Kittredge, Richard I},
          journal={IEEE Expert},
            volume={9},
              number={2},
                pages={45--53},
                  year={1994},
                    publisher={IEEE}
}

@inproceedings{iordanskaja-etal-1992-generation,
        title = "Generation of Extended Bilingual Statistical Reports",
            author = "Iordanskaja, L.  and
                      Kim, M.  and
                            Kittredge, R.  and
                                  Lavoie, B.  and
                                        Polguere, A.",
                booktitle = "{COLING} 1992 Volume 3: The 15th {I}nternational {C}onference on {C}omputational {L}inguistics",
                    year = "1992",
                        url = "https://www.aclweb.org/anthology/C92-3158",
}

@article{swartout1983xplain,
      title={XPLAIN: A system for creating and explaining expert consulting programs},
        author={Swartout, William R},
          journal={Artificial intelligence},
            volume={21},
              number={3},
                pages={285--325},
                  year={1983},
                    publisher={Elsevier}
}

@book{todd1992introduction,
      title={An introduction to expert systems},
        author={Todd, Bryan S},
          year={1992}
}

@inproceedings{auli2013joint,
  title={Joint Language and Translation Modeling with Recurrent Neural Networks},
  author={Auli, Michael and Galley, Michel and Quirk, Chris and Zweig, Geoffrey},
  booktitle={Proceedings of the 2013 Conference on Empirical Methods in Natural Language Processing},
  pages={1044--1054},
  year={2013}
}


@inproceedings{cho2014learning,
  title={Learning Phrase Representations using RNN Encoder--Decoder for Statistical Machine Translation},
  author={Cho, Kyunghyun and van Merri{\"e}nboer, Bart and Gulcehre, Caglar and Bahdanau, Dzmitry and Bougares, Fethi and Schwenk, Holger and Bengio, Yoshua},
  booktitle={Proceedings of the 2014 Conference on Empirical Methods in Natural Language Processing (EMNLP)},
  pages={1724--1734},
  year={2014}
}


@inproceedings{bahdanau2015neural,
      title={Neural machine translation by jointly learning to align and translate},
        author={Bahdanau, Dzmitry and Cho, Kyunghyun and Bengio, Yoshua},
          booktitle={3rd International Conference on Learning Representations, ICLR 2015},
            year={2015}
}

