  \section{Contributions}
  
  We now briefly summarize the contributions of this thesis described
  in the previous sections.
  
  %This thesis  makes the following contributions to NLG.
  %In the areas of text-to-text generation,
  
  \begin{enumerate}
          \item We propose a systematic evaluation of deep learning models
              for extractive single document summarizations (\autoref{}). 
              Our evaluation on several popular neural architectures shows 
              that:
              \begin{itemize}
                  \item Position features, even when not explicitly represented
                      in the model architecture are a dominant feature
                      exploited by the model.
                  \item Content features exist across a variet of word classes
                      but are not as strong of a signal as position.
                  \item Word embedding averaging is about as effective as 
                      recurrent or convolutional sentence encoders 
              \end{itemize}
          \item Additionally, in the task of query focused streaming news 
              summarization, we propose two models for providing 
              extractive update summaries. (\autoref{})
              \begin{itemize}
  
                  \item The first method processes the stream in  batches. 
                      It uses a regression model
                      to estimate the salience of individual 
                      sentences, and a biased clustering algorithm to select
                      the most representative and salient outputs.
                  \item The second method processes the stream in a fully
                      online manner. A linear model makes extraction
                      decisions, and we experiment with a learning-to-search
                      algorithm for training. 
              \end{itemize}
      \end{enumerate}
  
      In the area of data-to-text generation, we make the following 
      contributions to faithful and controllable generation of 
      text from a meaning representation. 
      \begin{enumerate}
          \item We propose a noise injection and self-training method
              for obtaining a faithful NLG model.
          \item We propose an encoder input linearization called alignment
              training which  yields an NLG model with surface level
              realisation ordering control.
      \end{enumerate}
  
  
  Finally, in chapter \autoref{conc} we conclude with a discussion of the 
  limitations and future 
  directions this work might take. In particular, we focus on how 
  faithful generation might be applied to summarization or \machinetranslation
  where an explicit representation of the content meaning is not available. 
