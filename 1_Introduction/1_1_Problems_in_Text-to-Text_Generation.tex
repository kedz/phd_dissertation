\section{Problems in Text-to-Text Generation}
  
Amongst the natural language processsing (NLP) research community, machine
learning, and especially deep learning, has become the \latin{de facto}
epistemological and methodological framework  for solving text-to-text
generation problems.  One need only to obtain a large collection of
input-output texts, and under this paradigm, the details of the particular
generation task can be abstracted away. It is sufficient to re-pose the
generation problem as a one of optimization, for which a general purpose
neural network model can be trained such that it minimizes the relevant loss
function, and in so doing, learns a mapping from the input text to the output
text \citep{sutskever2014,bahdanau2015,rush2015,nallapati2016,see2017}. 

That is to say, the deep learning framework de-emphasizes possessing a
``theory of the problem'' to be solved, and prefers instead a hands off
approach: the theory of the problem is implicit in the dataset, so let the
neural network, whose inductive bias is very different from that of the human
researcher, learn directly from the data the representations that are most
useful to its satisfaction of the optimization criteria.\footnote{One could
reasonably argue that language models have been evolving in the direction of
fewer inductive priors: ngram-models assert that the last few words are most
important; recurrent neural networks, the current word and as much as what can
be stuffed into a single memory vector; recurrent neural networks with
attention, the current word and any previous memory vector; and transformers,
random access memory to any input time step.} The logical extension of this
central dogma is that the best data is more data \citep{halevy2009}.  

And indeed that is precisely what has happened. Large language model
pre-training, where a language model, typically using a transformer
architecture \citep{vaswani2017}, is trained in a self-supervised way on
web-scale text, has dramatically expanded the quality of natural language
utterances that can be generated by a computational model
\citep{radford2019,brown2020}, while also capturing the attention of the
popular press \citep{simonite2019,vincent2019}. In the NLP research community,
pretrained sequence-to-sequence models, the family of deep learning model most
commonly used for text-to-text generation, like PEGASUS \citep{zhang2019}, T5
\citep{raffel2020}, or BART \citep{lewis2020}, have led to impressive gains in
the fluency and coherence of modestly-sized paragraphs, as well as automatic
task metrics like \bleu~\citep{papineni2002} and \rouge~\citep{lin2004}.

There are downsides to this approach however. By learning only from surface
level forms, it is possible for the models to appear like they contain
knowledge, but in reality, they are only modeling the probability of word
sequences \citep{bender2020}. This can cause them to hallucinate information
not present in the input or imply propositions contrary to what was given
\citep{wiseman2017,kryscinski2019,maynez2020,kryscinski2020}.  Without an
understanding of pragmatics, they cannot know how an argument or chain of
propositions holds together, only that they are statistically likely.
Moreover, they will learn many implicit and harmful biases of the society that
produced the corpora they are trained on, including but not limited to
negative stereotypical word associations \citep{bolukbasi2016,nissim2020} and
outright hate speech \citep{lee2016}.

With all of that in mind, we would like this thesis to emphasize the theory of
the problems we are trying to solve. While we do not abandon machine learning
and deep learning, we instead show how they may be used parsimoniously to
solve the actual problems at hand and not simply learn the dataset. Where we
do use machine learning models, we try to design experiments which reveal
which signals relevant to the actual problem are being used to make
predictions, and to establish some empirical limitations on their ability to
represent the input data and perform successfully. 
 
In order to learn the theory of the problem, as we have been calling it, we
must have some problems. We now describe some of the central problems of
summarization and text-to-text generation that we are interested in solving.
In most summarization tasks, we are generally interested in a text's
\textit{salience}, that is to say, the general importance or relevance of a
given text unit with respect to its context. Salience is usually the primary
dimension of the input data that we wish to measure or predict for determining
summary content.  

In \autoref{ch:dlsum}, we study salience estimationin the \emph{sentence
extractive, single document summarization task}, where the goal is to classify
which sentences in an input document should be included in an extract summary.
In this case, the input document is the context, and the units of text for
which we are estimating salience are the document sentences.  An extractive
summary is a subset of sentences that have maximum salience while sastifing a
length budget constraint, typically in the summary word or byte length. While
the constrained subset selection problem is interesting and has been studied
previously \citep{goldstein1998,mcdonald2007,lin2010}, we focus on modeling
the salience estimation task specifically. 
 
We study a variety of popular and novel deep learning architectures for
implementing the salience prediction task. Our key contributions here are not
only a model architecture, but also our systematic study of the combination of
sentence and context (i.e. document) level encoders as well as the
manipulation of the input documents to ablate which surface features are
available to a given model. Through these input data ablations, we can gain a
better understanding of how the salience prediction mechanism is working.
 
But what about more difficult summarization tasks? In \autoref{ch:mlsum}, we
study salience estimation for \emph{query focused, streaming, sentence
extractive summarization}. In this task, we add a search query and time as
additional elements to the summarization problem. As input we are given a
time-ordered stream of news articles and a query, typically a notable
real-world event, e.g., \texttt{``hurricane sandy''}. Our objective is to
extract sentences that are relevant to the query event while minimally
redundant to previously extracted sentences. Unlike the previous problem, the
salience of a given sentence is not constant but monotonically decreases as
time progresses. Due to the large volume of input texts, in attempting to
model the salience of a sentence we must now also model the redundancy between
sentences and previous extraction decisions to perform competently.
      
We incorporate a salience estimation model into two possible approaches to
extracting query-relevant summary sentences from the news stream.  The first
method uses the salience estimates to bias an affinity propagation clustering
algorithm \citep{dueck2009} to identify exemplar sentences which we extract
for the summary.  The clustering algorithm must trade off representativeness
versus the salience of individual sentences when selecting exemplars, i.e. an
exemplar must be a good representative of its cluster but also be important
under the salience estimator.  This approach works in hourly batches,
predicting the salience of sentences, clustering them, and then adding the
exemplars to a rolling update summary. The salience estimates also adaptively
control the number of clusters produced, allowing the model to adapt the
number of updates to fit the volume of salient sentences found in the stream. 

The first method has several draw backs. The summary selection is not done to
optimize the final summary evaluation measure and the predictions of salience
are static and cannot take into account previous decisions the summarizer has
made. Additionally, the use of clustering means that there will always be some
latency between when important information is known and when we can extract it
for the summary. In domains like crisis informatics, minimizing these delays
are critical \citep{starbird2013}.

To address these limitations, we recast stream summarization as a sequential
decision-making problem \citep{littman1996}, where we learn a policy for
extracting a sentence based on its estimated salience as well as its relation
to previously extracted sentences.  The sequential decision-making view opens
our model up to exposure bias as the learned policy will suffer from a
train-test distribution mismatch if our reference policies are overly
optimistic or pessimistic.  To mitigate this, we employ a learning-to-search
style training regime \citep{chang2015} to train a policy to make locally
optimal decisions when following either a noisy learned policy or an oracle
reference policy.  The result is a fully online summarization model whose
local decisions positively correlate with good overall summary evaluation
measures.  Additionally, because the model is greedy, it does not suffer as
much from the negative effects of latency as the clustering model does.
      
In our experiments with single-document extractive summarization, we found
that neural models heavily exploited position-based heurstics (i.e., did the
sentence occur in the lead paragraph) to determine sentence salience, which
arguably does not capture the essence of the summarization problem. In our
work on stream summarization, we show that with careful feature and model
design, we can capture salience beyond such heuristics.  In particular, we
show that content, location, and redundancy features can be used to predict
salience in this more challenging scenario.

In \autoref{ch:nlg}, we move to problems of content realization, after the
content selection process has been performed.  Here we focus on the related
goals of \textit{faithful} and \textit{controllable} generation using neural
NLG models. A neural NLG model is faithful if it can generate utterances that
are semantically correct with respect to the information extracted in the
content selection stage.  One of the central tensions in a neural NLG model is
that between the encoder, which creates a representation of the input, and the
decoder, which is functionally a language model conditioned on the encoder
representation.  The decoder language model must simultaneously place high
probability on output word sequences that are likely given the training
corpus, but also prefer output word sequences that are correlated with the
encoder representation of the input sequence. This conflict can lead to
hallucinations as the language model may occasionally put more probability
mass on a sequence of words that is frequently observed in the training data,
but not necessarily licensed by a particular input.

We hypothesize that this failure mode happens in part because the decoder, by
predicting next word continuations, is both implicitly planning the layout of
the utterance but also trying to satisfy the constraints given by the input,
and that alleviating the decoder language model of the planning task may
improve the faithfulness.  We propose developing controllable neural NLG
models, i.e. models that can follow an explicit plan determining  the surface
realization order of the intended utterance.  Controllable models learn to
represent the layout of the intended utterance implicitly in their encoder,
and thus the decoder language model has less flexibility in selecting the next
words, which can lower the chances of hallucinating text and improve the
overall faithfulness of the generation model.

We also believe that controllable generation has additional benefits beyond
increased faithfulness. For one, it will enable more integration of neural NLG
models into large NLG pipelines, \citep{castroferreira2019}.  Controllable
generation at the level of shallow phrase chunk ordering like we are proposing
may also lead to implimentations of cognitively plausbile discourse ordering
theories like Centering Theory \citep{grosz1995} or Accessibility Theory
\citep{ariel2001}, which place constraints on the discourse ordering of
entities.

Evaluating the faithfulness of a language generation model for open-world
summary generation tasks is non-trivial \citep{kryscinski2020,maynez2020}. In
order to simplify things, we study faithful and controllable generation in the
context of task-oriented dialogue generation, where given an explicit
representation of a dialogue agent's belief state and goals, we must generate
an appropriate natural language utterance. Because the input is an explicit,
formalized representation of the meaning of the intended utterance, manual and
even automatic checking of the faithfulness of an utterance/meaning
representation pair becomes much simpler. Since the concerns of faithful and
controllable generation are still incredibly important to generating
summaries, we consider the explicit meaning representations as an idealized
version of summarization system's content selection stage.  Faithfulness is
critical for any real summarization application; the reader has to be able to
trust that content is correct or it is functionally useless. Controllable
generation will further increase reliability and faithfulness, while also
allowing the tailoring of the summary to focus on particular user needs, for
example targeting generation to focus on a particular set of entities. 
      
For our contribution to faithful generation, we propose a novel data
augmentation method for sequence-to-sequence models.  We observe that a
popular sequence-to-sequence NLG model trained on a task-oriented dialogue
generation dataset produces fluent and natural utterances that are
unfortunately frequently semantically incorrect with respect to the input
meaning representation.  We then present evidence that the reason for these
errors are spurious correlations between the inputs and outputs in the
training data.  We also note that by injecting random noise into the
unfaithful NLG model we can cause it behave in an uncontrolled but useful
manner: it generates utterances that are not faithful to the input but do not
exhibit the spurious correlations or exhibit them to a lesser degree.  We
generate a synthetic corpus of these utterances, and then use another semantic
parsing model to given them correct meaning representations.  Remarkably,
sequence-to-sequence models trained on the union of original training data and
the synthetically generated training examples exhibit increased faithfulness
without hurting their fluency. We also find that in the union dataset, the
problematic spurious correlations are diminished.

While this data augmentaiton method helps reduce semantic errors, it leaves
the surface realization of utterances up to the decoder language model.  Our
second contribution to neural NLG models is an alignment training method that
reliably produces controllable language generators. Our method works by
aligning the indvidual components of a meaning representation to their
reference utterances on the training set. Given this alignment, we then map a
meaning representation into a linear sequence of tokens, such that the order
of the individual components corresponds to the realization order in the
training reference. Training an arbitrary sequence-to-sequence model to map
this linearized meaning representation to its reference utterance induces the
ability to control the model at test time. I.e., we can use a planning model
to propose an ordering of the meaning representation's sub-components, and the
controllable sequence-to-sequence model will attempt to realize them in that
order.  To achieve a different ordering, one need only permute the input
sequence.

In our experiments, we also evaluate how well models are able to follow
adversarially generated plans that do not have human, English language
ordering preferences and show that models struggle to realize utterances
correctly in this setting. We finally propose another data augmentation scheme
to generate constituent phrase data that gives explicit examples of how
phrases can be composed into larger units and how that systematically changes
the meaning representation. We find that this additional data improves the
robustness of the control behavior on these more difficult to follow plans.
