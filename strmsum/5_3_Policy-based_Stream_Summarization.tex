\subsection{Policy-based Stream Summarization}

In the induced search problem, each search state $\state_t \in \states$
corresponds to observing the first $t$ sentences in the stream $\left[
\strmSent_1,\ldots, \strmSent_t \right] \subset \sentstream$ and a sequence of
$t-1$ actions $\bsal_1, \ldots, \bsal_{t-1}$.  For each state $\state_t \in
\states$, the set of actions is $\bsal_t \in \{0,1\}$ with $\bsal_t =1$
indicating we extract the $t$-th sentence and add it to our update summary,
and $\bsal_t = 0$ indicating we skip it. For simplicity of exposition, we
assume a fixed length stream of size $T$ but this is not strictly necessary. 

\input{strmsum/figures/policysum.tex}

A policy, $\policy : \states \rightarrow \{0,1\}$, is a mapping from states to
an extraction decision. Given a policy, the policy-based stream summarization
algorithm (\autoref{alg:policysum}) is trivial, iterating sequentially over
sentences in the stream, and adding each sentence to the update summary if it
is the current action determined by $\policy$.  In practice, we use a linear
cost-sensitive classifier to implement $\policy_\params$, with \[ \bsal_t =
\policy_\params(\state_t) = \argmin_{\bsal \in \{0,1\}} \Feat(\state_t, \bsal)\cdot
\params_\bsal\] where we encode each state-potential action pair
$(\state_t,\bsal)$ as a $d$-dimensional feature vector  $\Feat(\state_t,
\bsal) \in \mathbb{R}^d$ and $\params_0, \params_1 \in \reals^{d}$ are learned
parameters.  Note that $\Feat(\state_t,\bsal)\cdot \params_\bsal$ is a linear
regression to predict the cost, which we define shortly, associated with
taking action $\bsal$ in state $\state_t$. Given estimates of our two
available actions, extracting a sentence or ignoring it, our policy $\policy_\policy$
selects the action with minimum cost.

Before we can define cost more concretely, we must first introduce some additional notation and concepts.
For a given sequence of states $\state_1,\ldots,\state_T$ explored by a policy $\policy$, let $\bsals= \left[\bsal_1,\ldots,\bsal_T\right]$ be the associated 
sequence of actions taken by $\policy$, i.e. $\bsal_t = \policy(\state_t)$.
A loss function, $\lossfunc : \{0,1\}^T \rightarrow \reals$, measures the quality of an action 
sequence $\bsals$. In our present case this might be the negative \textsc{Rouge} 
score of the update summary that results from $\bsals$ or 
some other relevant measure of performance. 

Let $\policy$ be a policy, $\state_1,\ldots,\state_T$ a sequence of states
explored by $\policy$, and the corresponding action sequence $\bsals$. We also
define a second policy, $\rolloutpolicy$, which we call the roll-out policy
and which may or may not be distinct from $\policy$.  For any state $\state_t$
we can then define two additional action sequences,
\[
    \roaseqt = \left[
        \bsal_1, 
        \ldots, 
        \bsal_{t-1},
        \hat{\bsal}_t,
        \roat_{t+1}, 
        \ldots, 
        \roat_{T}, 
    \right] \quad \forall \predbsal_t \in \{0,1\} 
\] 
where $\roaseqt$ is the sequence of actions that result from following
$\policy$ on the first $t-1$ states, taking action $\predbsal_t$ at state
$\state_t$ and then following $\rolloutpolicy$ on the remaining $T - t$
states. That is, for $k > t$, $\roa_{k} = \rolloutpolicy(\state^\prime_k)$,
where $\state^\prime_k$ is the state that results from taking action
$\predbsal_t$ in $\state_t$ and following $\rolloutpolicy$ on a sequence of
states $\state^\prime_{t+1}, \ldots, \state^\prime_k$. The loss
$\lossfunc\left( \roaseq{t}{0} \right)$ then reflects the evaluation measure
for an update summary where sentence $\strmSent_t$ was not extracted, and
$\rolloutpolicy$ completed the summary of the stream. Similarly,
$\lossfunc\left( \roaseq{t}{1}  \right)$ reflects the evaluation measure for
an update summary where sentence $\strmSent_t$ was extracted, and
$\rolloutpolicy$ completed the summary of the stream. 
 
We can now define the cost of an action $\bsal$ in state $\state_t$ as 
\[ 
    \cost(\state_t, \bsal) = 
        \lossfunc\left(\roaseq{t}{\bsal}   \right) 
            - \min_{\bsal^\prime \in \{0,1\} }
                \lossfunc\left( \roaseq{t}{\bsal^\prime} \right).
\]
Note that the cost is also a function of $\rolloutpolicy$, which determines
how the action sequence is completed after $\state_t$. 
The costs connect the overall summary loss $\lossfunc\left(\roaseqt\right)$
to a particular action in $\state_t$ that builds the summary.
We depict an example of the cost computation in \autoref{fig:rollouts}.
 We discuss how to collect costs and learn $\params_0$
and $\params_1$ such that they are good estimators of cost in the next section.
 

\input{strmsum/figures/rollouts.tex}


