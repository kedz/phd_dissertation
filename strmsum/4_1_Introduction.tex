\subsection{Introduction}

We now present our first streaming summarization model, the salience-biased
affinity propagation (SAP) summarizer. The SAP summarizer predicts
sentence salience with respect to a query $\query$, and integrates these
predictions into a clustering based multi-document summarization system. We
demonstrate that combining salience with clustering produces more relevant
summaries compared to baselines using clustering or salience alone.  Our
experiments suggest that this is because our system is better able to adapt to
dynamic changes in input volume that adversely affect methods that use
redundancy as a proxy for salience. 

In addition to the tight integration between clustering and salience
prediction, our approach also exploits knowledge about the event to determine
salience. Thus, salience represents both how typical a sentence is of the
event category (e.g., industrial accident, hurricane, riot) and whether it
specifies information about this particular event.  Our feature representation
includes a set of language models, one for each event category, to measure the
typicality of the sentence with regard to the current event, the distance of
mentioned locations from the center of the event, and the change in word
frequencies over the time of the event.  While we evaluate these features in
the domain of disasters, this approach is generally applicable to many update
summarization tasks.

We evaluate the SAP summarizer with two main experiments. First, we present 
the results of our model in the TREC 2014 Temporal
Summarization shared-task \citep{aslam2015}, where the SAP summarizer 
achieved top performance on the
main evaluation metric ($\mathcal{H}$), and was also shown to have higher
precision relative to other participant systems.  Second, we perform our own
independent evaluation, and show our approach achieves a statistically
significant improvement in \rouge~scores compared to multiple baselines
in addition to the expected gain and comprehensiveness metrics.
We also perform a feature ablation experiment to see which features
are most important in our salience estimation component.
%Additionally, we introduce novel methods for estimating the average
%information gain each update provides and how completely the update summary
%covers the event it is tracking; our system's updates contain more relevant
%information on average than the competing baselines. 
%
%
%We begin by discussing our methods and summarization model, before
%discussing the datasets used,the experiments, and results.
%The remaindSection~\ref{sec:methods} outlines the details of our salience 
%and summarization models. Next we describe our data (Section~\ref{sec:data}) 
%and experiments (Section~\ref{sec:exper}). Finally, we discuss our results 
%(Section~\ref{sec:results}) and conclude the paper.
