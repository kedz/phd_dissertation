 \subsection{Expanding Relevance Judgments}
 \label{sec:reljudge}

 Because of the large size of the corpus,
 less than 1\% of the sentences received
 manual review by NIST assessors during the 2013-15 shared-task evaluations.  
  In order to increase the amount of data for training and
 evaluation of our system, we augmented the manual sentence-nugget match
judgements with automatic matches.
 Sentence-nugget matches are critical for our experiments because they
 enable us to compute evaluation metrics, but also the oracle summarization
 policy used in the LOLS algorithm.

 In order to automatically tag sentences in the corpus with additional
 nugget matches,
 we trained separate decision-tree classifier for
 each nugget with more than 10 manual sentence matches.  Manually matched
 sentences were used as positive training data and an equal number of
 manually judged non-matching sentences were used as negative examples.
 Sentence n-grams (1-5), percentage of nugget terms covered by the sentence,
 semantic similarity of the sentence to nugget were used as features, along
 with an interaction term between the semantic similarity and coverage. 
 After training the classifiers we used them to automatically tag 
 corpus sentences with nugget matches.
 When
 augmenting the relevance judgments with these nugget match labels, we
 only include those that have a probability greater than 90\% under the
 classifier. %Overall these additional labels increase the number of matched
 %sentences by 1600\%.
 For evaluation, the summarization system only has access to the query and
 the document stream, without knowledge of any nugget matches (manual or
 automatic).


