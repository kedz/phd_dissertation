\subsection{Affinity Propagation Clustering}
\label{sec:exsel}

Once we have predicted the salience for a batch of sentences, we must now
select a set of update candidates, i.e. sentences that are both salient and
representative of the current batch. To accomplish this, we combine the output
of our salience prediction model with the affinity propagation clustering
algorithm \citep{frey2007clustering}.

Affinity propagation (AP)  identifies a subset of data points as exemplars and
forms clusters by assigning the remaining points to one of the exemplars.  Let
$\sentstream = \left\{\strmSent_1,\ldots,\strmSent_{\strmSentSize} \right\}$
be a set of $\strmSentSize$ data points to be clutstered and let $\exemplars =
\left[\exemplar_1,\ldots, \exemplar_\strmSentSize \right]$ be a vector of
corresponding exemplar assignments, i.e. $\exemplar_i \in \{1,\ldots,
\strmSentSize\}$ and $\exemplar_i = k$ means that $\strmSent_k$ is the
exemplar for $\strmSent_i$.

AP attempts to maximize the constrained net similarity 
objective,
\[ F(\mathbf{\exemplars}) = e^{\left(\sum_{i=1}^\strmSentSize s\left(\strmSent_i,\strmSent_{\exemplar_i}\right) + \sum_{k=1}^\strmSentSize \log g_k(\mathbf{\exemplars}) \right)}
\]
where $s\left(\strmSent_i, \strmSent_{\exemplar_i}\right) \in \reals$ measures the 
affinity of $\strmSent_i$ for its exemplar $\strmSent_{\exemplar_i}$ and 
\[g_k = \begin{cases} 0 & \textrm{$\exemplar_k \ne k$ but $\exists i: \exemplar_i = k$ } \\ 1 & \textrm{otherwise}      \end{cases}\] expresses the 
constraint that if $\strmSent_k$ is some point's exemplar, it must be its
own exemplar, i.e. all clusters must have one exemplar. The affinities
can be interpreted as a log-probabilities of the exemplar-assignments, i.e.
$p(\exemplars) = \frac{F(\exemplars)}{ \sum_{\boldsymbol{\exemplars^\prime }  } F( \boldsymbol{\exemplars^\prime }  )  }.$ \cite{frey2007clustering} show the 
net similarity objective can be viewed as a factor graph and an
 optimal configuration can be found using max-product message passing. 
For the affinity function $s$, we use
\[ s\left(\strmSent_i, \strmSent_{\exemplar_i}\right) = \begin{cases}
 f(\phi(\strmSent_i,\query,\category))   & \textrm{if $i = \exemplar_i$ }\\
 \operatorname{similarity}\left(\strmSent_i,\strmSent_{\exemplar_i}\right)    & \textrm{otherwise}
\end{cases}
\]
where $f(\phi(\strmSent_i,\query,\category))$ is the salience estimate of
sentence $\strmSent_i$ (\autoref{eq:approxsal}) and $\operatorname{similarity}(\strmSent_i, \strmSent_{\exemplar_i})$ is the WMF-based similarity
method (\autoref{eq:semsim}).







%?\sum_{\strmSent_i : \strmSent_i \neq e_i} \operatorname{sim}(\strmSent_i,e_i) 
%?+ \sum_{e_i} \operatorname{salience}(e_i)  \]
%?where $e_i$ is the exemplar of the $i$-th data point, and functions
%?$\operatorname{sim}$ and $\operatorname{salience}$ express the pairwise 
%?similarity of data points and our predicted \textit{apriori} preference of a 
%?data point to be an exemplar respectively. 
%?AP differs from other $k$-centers algorithms in that it simultaneously 
%?considers all data points as exemplars, making it less prone to finding local 
%?optima as a result of poor initialization. Furthermore, the second term in 
%?$\mathcal{S}$ incorporates the individual importance of data points as 
%?candidate exemplars; most other clustering algorithms only make use of the 
%?first term, i.e. the pairwise similarities between data points.
 

AP has several useful properties relevant to our stream summarization task. Chiefly, the number
of clusters $k$ is not a model hyper-parameter. Given that our task requires
clustering many batches of data with potentially large variations in the volume of data per batch, searching for an optimal $k$ 
would be computationally prohibitive. With AP, $k$ is determined by the 
self-affinity,
$s(\strmSent_i,\strmSent_i)$, of the data,  with lower overall values of $s(\strmSent_i,\strmSent_i)$ yielding a 
smaller number of clusters.
%Recall that $\operatorname{salience}(\strmSent, \query) \approx s(\strmSent_i,strmSent_i)$ is a prediction of the semantic 
%similarity of $\strmSent$ to information about the event be summarized, i.e. the set 
%of event nuggets. 
%Intuitively, when maximizing the objective function $F(\exemplars)$, AP must
%balance between best representing the input data and representing the most
%salient input. Additionally, 
When the volume of input is high but the
salience predictions are low, the self-affinity term will guide AP toward a
solution with fewer clusters; \textit{vice-versa} when input is very salient
on average but the volume of input is low. The adaptive nature of our model
differentiates our method from most other update summarization systems.

In the summarization pipeline, given a batch of sentences
$\sentstream^{(\query)}_{\systemtime_{t-1}:\systemtime_t}$ and their salience
estimates $\predbsals_t$, we run the AP clustering algorithm and obtain the
set of exemplars $\mathcal{E}_t =
\operatorname{APCluster}(\sentstream^{(\query)}_{\systemtime_{t-1}:\systemtime_t},
\predbsals_t)$. The exemplars $\mathcal{E}_t$ are then passed to a redundancy
filtering stage which we describe next.
