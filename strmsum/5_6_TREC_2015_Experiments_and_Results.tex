\subsection{TREC 2015 Experiments and Results}

There were three tasks at the TREC 2015 Temporal Summarization
shared-task evaluation \citep{aslam2016}:
\begin{enumerate}
\item  \textbf{Full Filtering and Summarization} Participants must summarize
a \textbf{very} high volume stream of documents where very few documents are 
likely to be similar to the query.
\item  \textbf{Partial Filtering and Summarization} Participants must summarize
a high volume stream of documents that has been pre-filtered for query
relevance
by an IR component developed by the task organizers.
\item \textbf{Summarization Only} Participants must summarize a low-volume
stream of on-topic documents.
\end{enumerate}

We participated in tasks 1 and 2, submitting, with Team ID \textit{cunlp},
the following Run IDs:

\begin{itemize}
\item \textbf{Task 1. Full Filtering and Summarization}
\begin{itemize}
    \item  2LtoSnofltr20 -- The L2S summarizer, where documents were
truncated to there first 20 sentences.
\end{itemize}
\item \textbf{Task 2. Partial Filtering and Summarization}
\begin{itemize}
\item  1LtoSfltr20 -- The L2S summarizer, where documents were
truncated to the first 20 sentences.
\item  3LtoSfltr5 -- The L2S summarizer, where documents were truncated to 
their first 5 sentences.
\item  4APSAL -- Our SAP summarizer that was the overall best system 
    at TREC 2014. We updated the salience predictions with additional training
data from the TREC 2014 Temporal Summarization query events.
\end{itemize}
\end{itemize}

To train the L2S summarizer we randomly selected 3 events to be a development
set and used the remaining 21 events from the 2013-2014 Temporal Summarization
query events as our training set. The document streams for the events
are unfortunately too large to be used directly with \autoref{fig:lols}.
%We traing the L2S summarizer using the traing query events 
% Even after filtering,  each training query's document stream is still too
% large to be used directly in our combinatorial search space. 
In order to
 make training time reasonable yet representative, we downsample each stream
 to a length of 100 sentences. The downsampling is done uniformly over the
 entire stream. This is repeated 10 times for each training event to create a
 total of 210 training streams. In the event that a downsample contains no
 nuggets (either human or automatically labeled) we resample until at least
 one exists in the sample. 
 We select the best model iteration based on the automatically computed
$\mathcal{H}(\updateSummary)$ on the development set.

\input{strmsum/tables/trec15t1.tex}

\autoref{tab:trec15t1} shows the official results for task 1, the full
filtering and summarization task. Our L2S run, 2LtoSnofltr20, was the top run
in this track, although only one other team submitted runs due to the high
computational cost of running on the large document stream.  2LtoSnofltr20 had
the lowest precision (i.e., $n\mathbb{E}[\gain](\updateSummary)$) of the
submitted runs, and only average recall (i.e., $\comp(\updateSummary)$),
relative to the other runs, it had lower latency, yielding the highest 
latency weighted comprehensiveness, $\comp_L(\updateSummary)$, which 
put it over the top relative to the competitors. Here we can see that 
fully online/greedy nature of L2S summarizer pays off in terms of latency,
as salient content is identified relativey quickly once it has been observed
in the stream.


\input{strmsum/tables/trec15t2.tex}

\autoref{tab:trec15t2} shows the official results for Task 2, the partial
filtering and summarization task. The top runs by WaterlooClarke examined
only the titles or first sentences of the documents, while our systems did not
use titles, and used the first five or twenty sentences of documents (3LtoSfltr5 and
3LtoSfltr20 respectively). The WaterlooClarke runs had significantly
higher $n\mathbb{E}[\gain](\updateSummary)$ than the other systems but lower
than average $\comp(\updateSummary)$. Our L2S runs, 3LtoSfltr5 and
3LtoSfltr20, had higher $\comp(\updateSummary)$, although not the highest overall. The fourth and five place overall performance (i.e., $\mathcal{H}_L(\updateSummary)$) by 3LtoSfltr5 and
3LtoSfltr20 are due both to the high $\comp(\updateSummary)$ but also low-latency at which it added important information to the summary.

The SAP run, 4APSAL, was also above the track average for all evaluation
measures. It is, however, dominated by both L2S runs on all all measures.
This suggests that our L2S is an overall improvement over the SAP model in
both precision, recall, and latency. 




