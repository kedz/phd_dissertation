\chapter{Introduction}

\introquote{
    \textsc{Caliban} \\[-5pt]
    You taught me language, and my profit on â€™t \\[-5pt]
    Is I know how to curse. The red plague rid you  \\[-5pt]
    For learning me your language!\\
}{
    William Shakespeare, \textit{The Tempest} (I.ii.362-364)
}

\introquote{
    \textsc{[Prompt]} Q: How many eyes does a horse have? \\[-5pt]
    \textsc{[Gpt-3]} A: 4. It has two eyes on the outside and two eyes on the 
        inside.
}{
    \textsc{Gpt-3} \citep{brown2020}, responding to a prompt by \cite{shane2020}
}

\addvspace{6\bigskipamount}

\startglyph Somewhere out of the inky-black terminals and  unseeable valleys of
high-dimensional loss-surfaces, an unexpectedly expressive capacity to generate
natural language has emerged in contemporary models of machine learning.  In
particular, deep learning-based language models have dramatically improved the
overall quality of computer generated text, opening the doors to many exciting
applications like creative writing tools
\citep{huggingface2019,samuel2019,seabrook2019} and interactive fiction
\citep{robertson2019}.  While these new natural language generation (NLG)
methods are quite powerful, they are in practice very difficult to control or
constrain. This is unfortunate as it limits responsible application to domains
with high fault tolerance, like the those mentioned above and other,
essentially low stakes, human guided creative exploration tools. It is unclear
if such tools are ready for real deployments in crisis informatics
\citep{starbird2013} or medicine \citep{gatt2009}.

This thesis is focused on a particular text-to-text generation problem,
automatic summarization, where the goal is to map a large input text to a much
shorter summary text, and the research presented aims to both understand and
tame these machine learning d{\ae}mons, hopefully paving the way for more
reliable text-to-text~generation algorithms.  Somewhat against the prevailing
trends, we eschew end-to-end training of an abstractive summarization model,
and instead break down the text summarization problem into its constituent
tasks.  At a high level, we divide these tasks into two categories:
\textit{content selection}, or ``what to say'' and \textit{content
realization}, or ``how to say it'' \citep{mckeown1985}.  Within these
categories we propose models and learning algorithms for the following
problems:
    
\begin{itemize}
  \item Content Selection
  \begin{itemize}
    \item Salience Estimation with Deep Learning Content Selection Models
    \item Salience Estimation with Structured Content Selection Models
  \end{itemize}
  \item Content Realization
  \begin{itemize}
    \item Data Augmentation and Self-Training for Faithful Neural NLG Models
    \item Meaning Representation Linearization Strategies for Content Planning
          and Controllable Neural NLG Models
  \end{itemize}
\end{itemize}

The first part, content selection, explores various issues around salience
estimation; that is, predicting the importance of an arbitrary unit of text
given some surrounding context (e.g., the larger document that contains that
text or a user query). In particular, we experiment with a variety of popular
and novel deep learning models for salience estimation, and design several
ablation experiments to gain some insight into which input signals are most
important for making predictions. Understanding these signals is critical for
designing reliable summarization models. 

We then consider a more difficult problem of estimating salience in a large
document stream, and propose two alternative approaches using classical machine
learning techniques from unsupervised clustering and structured prediction.
These models incorporate salience estimates into larger text extraction
algorithms that also consider redundancy and previous extraction decisions.
    
In part two, content realization, we assume content selection has already been
performed and focus on methods for faithful generation, i.e., ensuring that
output text utterances respect the semantics of the input content. Since they
can generate very fluent and natural text, deep learning-based NLG~models are a
popular approach to this problem. However, they often omit, misconstrue, or
otherwise generate text that is not semantically correct given the input
content. In this section, we develop a data augmentation and self-training
technique to mitigate this problem. Additionally, we propose a training method
for making deep learning-based NLG models capable of following a content plan,
allowing for more control over the output utterance generated by the model.
Finally, under a stress test evaluation protocol, we demonstrate some empirical
limits on several neural NLG models' ability to encode and properly realized a
content plan.
 
It is important to note that the approaches in part two are implemented not for
summarization, but for solving \textit{task-oriented dialogue generation}
\citep{mairesse2010}, where the problem is one of modeling the response of a
dialogue agent interacting with a human user. The input in this setting is a
formal meaning representation of both the communicative goal that the dialogue
agent is trying to achieve and the content that is to be realized. While this
is technically a data-to-text and not a text-to-text generation problem, the
aims of faithful and controllable generation are relevant to both problem
classes.  With data-to-text generation, however, we get an explicit
representation of meaning that makes it easier to measure progress in model
faithfulness and control. We therefore consider task-oriented dialogue
generation as an idealized form of text-to-text generation, where a content
selection module has mapped input text units to a discrete meaning
representation for realizing the content.  We anticipate that the development
of summarization models on discrete semantic representations
\citep{falke2017,liao2018} will continue and that our findings in neural
generation methods will carry over to the task of generating summaries from
semantic representations \citep{hardy2018}. Possible methods of transferring
these approaches to the text-to-text regime will be discussed in the final
chapter of this thesis.
