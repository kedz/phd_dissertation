\section{Problem Definition}



\begin{table}[t]
    \begin{center}
    \begin{tabular}{cp{0.85\textwidth}}
        \toprule
        Symbol & Definition \\
        \midrule
        $\naturals$ & The set of natural numbers $1,2,\ldots$.\\
        $\reals$ & The set of real numbers.\\
        $\wordVocab$ & The input word vocabulary.\\[5pt] 
        $\word_{i,j}$ & The $j^\textrm{th}$ word from the 
                        $i^\textrm{th}$ sentence. Words are discrete
                        tokens drawn from the vocabulary $\wordVocab$. \\[5pt]
                        $\sentSize_i$ & The length of the $i^\textrm{th}$ sentence in words. \\[5pt]
    $\sent_i$ & The $i^\textrm{th}$ sentence from a
                         document. A sentence is an ordered sequence of 
                         words $\word_{i,1}, \word_{i,2}, \ldots \word_{\sentSize_i}$.\\[5pt]
                         $\docSize$ & The length of a document in sentences.\\[5pt]
                         $\bsal_i$ & The salience of the $i^\textrm{th}$ sentence. Salience is binary, with $\bsal_i = 1$ or $\bsal_i=0$ indicating sentence $i$ is salient or not salient respectively.\\
                         $\bsals$ & A binary vector of $\docSize$ salience judgements.\\
                         $\doc$ & A document. A document is a sequence of $\docSize$ sentences.\\
                         $\docSpace$ & The space of all possible input documents.\\
                         $\labelSpace$ & The space of all possible label sequences of length 1 or more (i.e., $\labelSpace = (0,1)^+$).\\

       % w &~~ & \textrm{A word.}\\
       % \sentSize_i &~~& \textrm{The length of the $i$-th sentence in words.}\\
       % \sent_i & ~~& \textrm{A sentence which is an ordered sequence of words $w_1, w_2, \ldots$.}\\
                         \bottomrule
    \end{tabular}
    \end{center}
    \caption{Notation key for this chapter.}
    \label{dlsalnote}
\end{table}

We now formally define the sentence extractive, single document summarization 
task as a sequence tagging problem, following \cite{conroy2001}. A 
list of notation can be found in \autoref{dlsalnote}. Let a 
document $\doc\in \docSpace$ be a sequence of $\docSize$ sentences, 
\[ \doc = \sent_1, \sent_2, \ldots, \sent_\docSize.\] 
Sentences are themselves sequences of words,
\[\sent_i=\word_{i,1}, \word_{i,2}, \ldots, \word_{i,{\sentSize_i}},\]
where $\sentSize_i\in\naturals$ is the length of sentence $\sent_i$ in words.
The words themselves are drawn from a finite vocabulary $\wordVocab$.

The binary \salience~of a sentence $\sent_i$ is $\bsal_i \in\{0,1\}$. $\bsal_i=1$ indicates that sentence $\sent_i$ is \salient~and
should be included in the extract summary while $\bsal_i=0$ is assigned
to non-\salient~sentences that should be excluded from the summary.
We indicate the vector of salience judgements for the $\docSize$ 
sentences in $\doc$ as $\bsals = \left[\bsal_1,\ldots,\bsal_\docSize\right]\in  \labelSpace$.
The objective of this sequence tagging problem is to learn a function
$f : \docSpace \rightarrow \labelSpace$ which maps a document $\doc$ to a 
sequence of \salience~labels $\bsals$. In this work, we learn a probabilistic
mapping $\model(\bsals|\doc;\params)$ where $\model$ is a neural network with parameters
$\theta$ and $\model(\cdot|\doc;\params) : \labelSpace \rightarrow (0,1)$. 









Prediction is achieved by finding 
$\predbsals = \operatorname{arg\;max}_{\bsals \in \labelSpace} \model(\bsals|\doc;\params)$, either by approximation or when the structure of $\model$ allows, exactly.
Additionally, a typical constraint on summarization is that the 
extract summary not excede a word budget $\wordbudget \in \naturals$, that is, $\sum_{i=1}^\docSize \hat{\bsal}_i \cdot \sentSize_i \le \wordbudget$.
Since it is not trivial to incorporate this constraint into the sequence
labeling formulation, we instead rely a on a greedy heuristic to enforce the
budget constraint in practice. More details on test time inference 
can be found in \autoref{sec:inference}.



%?Specifically, given a document containing $\docSize$ sentences 
%?$\sent_1, \ldots, \sent_{\docSize}$ we generate a summary by predicting a 
%?corresponding label sequence $\bsal_1, \ldots, \bsal_{\docSize} 
%?\in \{0, 1\}^{\docSize}$,
%? where $\bsal_i = 1$ 
%?indicates the $i$-th sentence is to be included in the summary.
%?Each sentence is itself a sequence of word embeddings 
%?$\sent_i = \wordEmb_1^{(i)}, \ldots, \wordEmb_{|\sent_i|}^{(i)}$ where
%?$|\sent_i|$ is the length of the sentence in words.
%?The word budget $c \in \mathbb{N}$ 
%?enforces a constraint that the total summary word length 
%?$\sum_{i=1}^\docSize \bsal_i \cdot |\sent_i| \le c$.
%?
%?
%?
%?
%?The goal of extractive text summarization is to select a subset of a 
%?document's text to use as a summary, i.e. a short gist or excerpt of the 
%?central content.
%?Typically, we impose a budget on the length of the summary in either 
%?words or bytes. In this work, we focus on \textit{sentence} extractive 
%?summarization, 
%?where the basic unit of extraction is a sentence and impose a word limit as 
%?the budget.
%?
%?
%?
