
\section{Related Work}

The introduction of the CNN-DailyMail corpus by \citet{nips15_hermann} 
allowed for the application of large-scale training of deep learning models 
for summarization.
\citet{cheng2016neural} developed a sentence extractive model that uses a 
word level CNN to encode sentences and a sentence level sequence-to-sequence 
model to predict which sentences to include in the summary. Subsequently, 
\citet{nallapati2017summarunner} proposed a different model using word-level 
bidirectional RNNs along with a sentence level 
bidirectional RNN for predicting which sentences should be extracted. 
Their sentence extractor creates representations of the whole document and 
computes separate scores for salience, novelty, and location.
These works represent the state-of-the-art for deep learning-based extractive
summarization and we analyze them further in this paper.

Other recent neural network approaches include, \citet{yasunaga2017graph},
who learn a graph-convolutional network (GCN) for multi-document summarization.
They do not 
closely examine the choice of sentence encoder, which is one of the focuses
of the present paper; rather, they study the best choice of graph 
structure for the GCN, which is orthogonal to this work. 

Non-neural network learning-based approaches have also been applied
to summarization. Typically they involve learning n-gram feature weights 
in linear models along with other non-lexical word or 
%KM ``structure''? or ``structural''?
structural features 
\cite{berg2011jointly,sipos2012large,durrett2016learning}.
In this paper, we study representation learning in
neural networks that can capture more complex word level feature interactions
and whose dense representations are more compatible with current practices
in NLP.




%KM - I think you should use \namecite or \citet to have the name appear in text and then use ``who''
% which 
%who
%learn a graph-convolutional network for
%KM Inserted sentence boundary.
%multi-document summarization. However, they do not extensively \hal{can you drop extensively?} study the 
%choice of sentence encoder, focusing more on the importance of the 
%graph structure, which is orthogonal to this work.



%\hal{I think this section can be organized better. I don't have a solid proposal, but i might be tempted to go reverse chronological, and start with the recent stuff and then ground it in what came before. but generally i feel like this is basically a bulletted list and it needs some coherence/cohesion.}

%Extractive summarization has been extensively studied, although typically 
%in the multi-document setting. 
%\kathy{Might want to indicate that this is not recent.}
%Pre-neural network
%approaches to single document summarization
%are often formulated as graph-based ranking problems, where
%sentences are nodes in a graph and edges are determined by pairwise 
%similarity of bag-of-words (BOW) representations 
%\citep{erkan2004lexrank}.  % mihalcea2005language,
%More recently \citet{wan2010towards}
%jointly performed single and multi-document summarization in this framework. 
%Generally, this line of work does not learn sentence representations for 
%computing the underlying graph structures, which is the focus of this paper.
%%
%A recent extension of this line of work is that of \citet{yasunaga2017graph}
%KM - I think you should use \namecite or \citet to have the name appear in text and then use ``who''
% which 
%?who
%?learn a graph-convolutional network for
%?%KM Inserted sentence boundary.
%?multi-document summarization. However, they do not extensively \hal{can you drop extensively?} study the 
%?choice of sentence encoder, focusing more on the importance of the 
%?graph structure, which is orthogonal to this work.

%\kathy{Again, you need to signal that this was earlier work. Not really sure you need this paragraph. Doesn't seem very relevant.}
%?Since these two models are very different in design, it is unclear 
%?what model choices are most important for indentifying summary content 
%?in the input document. We use the sentence extractor designs of 
%?\citet{cheng2016neural} and \citet{nallapati2017summarunner} as points of 
%?%KM - removed ``presented in'' as slightly awkward.
%?comparison in our experiments 
%?%presented in 
%?(Section~\ref{models}). \hal{can probably drop this paragraph for space}

%KM -  I don't think ``have'' is needed.
The previously mentioned works have
focused on news summarization. To further
understand the content selection process, we also explore other domains 
of summarization. In particular, we explore 
%KM I added ``summarization of'' to keep it parallel with the rest.
personal narrative summarization based on stories shared
on Reddit \cite{ouyang2017crowd}, workplace meeting summarization
\cite{carletta2005ami}, and medical journal article summarization 
\cite{mishra2014text}. 

While most work on these summarization tasks
 often exploit domain-specific features (e.g. speaker identification in meeting summarization \cite{galley2006skip,gillick2009global}),
%\kathy{``eschew'' has a negative connotation. Implies you think this is the wrong way to go. I don't think you mean that. I think what you really mean is you want to understand whether general neural net features can control content selection without adding domain specific features. People will otherwise argue with ``understand generally'' since such features may play an important role.}
we purposefully avoid such features in this work in order to understand 
the extent to which deep learning models can perform content 
selection using only surface lexical features.
Summarization of academic literature (including medical journals), has long 
been a research topic in NLP
\cite{kupiec1995trainable,elhadad2005customization}, but most approaches have
explored facet-based summarization \cite{jaidka2017insights}, which is not the focus of our work.


%generally how content selection is learned.










%Extractive single document summarization 
%Cheng and Lapata\\
%
%\textbf{Abstractive Deep Learning Based Summarization}
%
%Rush\\
%Chopra\\
%See at al.\\ 
%Socher\\
%
%\textbf{Extractive Single Doc Summarization}
%Durret et. al\\
%
%\textbf{Non Newswire Summarization}
%meeting summarization\\
%reddit stories \\
%journal articles/pubmed \\


%?While there has a been a recent flurry of work on abstractive summarization
%?\cite{paulus,see,chenglapata,nallapati},
%?these papers treat this problem as a pure sequence to sequence 
%?transduction task. Admittedly, this view allows us to apply very powerful, 
%?general-purpose deep learning archictures to generate summaries.
%?At the same time, it obscures a principal subtask in summarization, the 
%?process of selecting the most salient units of meaning in the source material,
%?i.e. the key ingredients in the final summary, a process which we 
%?broadly refer to as content selection \cite{possiblyMcKeownAndNenkova}.

%?As is also the case in other NLP tasks, it is not immediately obvious how a
%?deep learning model is making its predictions, or what correlations 
%?are being exploited. There is a concerning and growing body of work that 
%?find models functioning as mere \hal{why mere? nneighbor is good in a lot of settings!} nearest neighbors search 
%?\cite{chen2016thorough}, 
%?exploiting annotator artifacts 
%?\cite{gururangan2018annotation}, or open to fairly trivial adversarial 
%?exploitation \cite{jia2017adversarial}. 
%?These lines of research are critical for finding model shortcomings, and over
%?time, guiding improvements in technique. Unfortunately,
%?to the best of our knowledge, there has been
%?no such undertaking for the summarization task.  \hal{are we doing it? if not, perhaps this does belong here?}
%?

%%% Local Variables:
%%% mode: latex
%%% TeX-master: "dlextsum.emnlp18"
%%% End:
