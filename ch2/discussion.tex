
\input{ch2/figures/output.tex}

\section{Discussion}

Learning content selection for summarization in the news domain is severely inhibited by the lead bias. 
The summaries generated by all systems described here--the prior work and our proposed simplified models--are highly similar to each other and to the lead 
baseline. The Cheng \& Lapata and Seq2Seq 
extractors (using the averaging encoder) share 87.8\% of output sentences on average on the CNN/DM data,
with similar numbers for the other news domains (see \autoref{tab:output}
for a typical example).  
Also on CNN/DM, 58\% of the Seq2Seq %, with averaging encoder, 
selected sentences also occur
in the lead summary, with similar numbers for DUC, NYT, and Reddit. Shuffling
reduces lead overlap to 35.2\% but the overall system performance drops
    significantly; the models are not able to identify important information
    without position.
    
    The relative robustness of the news domain to part of speech ablation also 
    suggests that models are mostly learning to recognize the stylistic 
    features unique to the beginning of the article, and not the content.
    Additionally, the drop in performance when learning word embeddings on 
    the news domain suggests that word embeddings alone do not provide 
    very generalizable content features compared to recognizing the lead.

The picture is rosier for non-news summarization where part of speech ablation leads
to larger performance differences and shuffling either does not inhibit content
selection significantly or leads to modest gains. Learning better
word-level representations on these domains will likely require much
larger corpora, something which might remain unlikely for personal stories
and meetings.



The lack of distinction among sentence encoders is interesting because 
it echoes findings in the generic sentence embedding literature 
where word embedding averaging is frustratingly difficult to 
outperform  \citep{iyyer2015deep,wieting2015towards,arora2016simple,wieting2017revisiting}.
The inability to learn useful sentence representations is also 
borne out in the 
SummaRunner model, where there are explicit similarity computations
between document or summary representations and sentence embeddings;
these computations do not seem to add much to the performance as the 
Cheng \& Lapata and Seq2Seq models which lack these features generally
perform as well or better.
Furthermore, the Cheng \& Lapata and SummaRunner extractors both construct
a history of previous selection decisions to inform future choices but this
does not seem to significantly improve performance over the Seq2Seq extractor 
(which does not). This suggests that we need to rethink or find novel forms 
of sentence representation for the summarization task.%\hal{I'M NOT SURE ABOUT THAT. ISN'T IT MORE SAYING THAT THE INPUT REPRESENTATION IS SUFFICIENTLY RICH THAT DEPENDENCIES IN THE OUTPUT DO NOT NEED TO BE MODELED EXPLICITLY?}


A manual examination of the outputs revealed some interesting failure modes,
although in general it was hard to discern clear patterns of behaviour 
other than lead bias. On the news domain, the models consistently learned 
to ignore quoted material in the lead, as often the quotes provide
color to the story but are unlikely to be included in the summary (e.g. \textit{``It was like somebody slugging a punching bag.''}). 
This behavior was most likely triggered by the presence of quotes, as the
quote attributions, which were often tokenized as separate sentences,
would subsequently be included in the summary despite also not containing 
much information 
(e.g. \textit{Gil Clark of the National Hurricane Center said Thursday}). %\hal{DOES THE POS ABLATION HAVE ANYTHING TO SAY HERE?}




%The Reddit corpus was particularly challenging as often there was no concise
%way to represent the story using just extracted sentences. Authors often inserted a 
%fairly brief summary of the story, either at the end or beginning, but
%this was so abstractive as to not have much overlap with the reference
%summaries. For example, the first sentence, \textit{I took a dog off the street, and she changed my grandparent's lives}, has little overlap with the reference
%\textit{The dog I found for my grandparents still gets really excited to see
%me whenever I come to visit}, but is still functionally a reasonable 
%summary of the story. These ``micro summaries'' do not seem to be 
%consistently found by any summarization model. 
%


