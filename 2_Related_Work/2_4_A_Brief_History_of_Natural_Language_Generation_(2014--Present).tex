\section{A Brief History of Natural Language Generation (2014-present)}
  
While feed-forward neural networks had been used previously as part of
phrased-based SMT systems \citep{schwenk2006}, there was an increased interest
in the early-mid 2010's around using recurrent neural network (RNN) language
models \citep{mikolov2010} as a rescoring method for an SMT decoder
\citep{auli2013joint,cho2014learning}.  RNNs could exploit (in theory)
unbounded source and target prefix information that was difficult to capture
in ngram or feed-forward models. \citet{cho2014learning} is particularly
noteable because they propose separate encoder/decoder RNNs, and while
intended for rescoring and not generation directly, this general architecture
constitutes the ``sequence-to-sequence'' backbone of most neural MT (NMT) and
neural NLG models.
  
Shortly thereafter, \citet{sutskever2014sequence} proposed the now ubiquitous
sequence-to-sequence model to perform translation directly.
\citet{bahdanau2015}  also proposed a sequence-to-sequence model with an
attention mechanism, which both made optimization easier (error feedback,
i.e., gradients, could now be routed directly from any decoder word prediction
step to any arbitrarily  distant timesteps in the encoder) and allowed for
visualization of the NMT decoder's alignment with the encoder.  NMT models,
while conceptually simpler than phrase-based SMT, were starting to achieve
state-of-the-art results \citep{bojar2016} and wide-spread industry adoption
\citep{wu2016google,gehring2017}.  It did not take long for researchers to
adapt the sequence-to-seqeunce model to other language generation problems,
e.g. generating captions from images \citep{vinyals2015a}, sports summaries for box scores
\cite{lebret2016,wiseman2017}, or from semantic representations
\citep{wen2015,dusek2016}. 
  
The introduction of the CNN-DailyMail corpus by \cite{hermann2015} allowed for
the application of large-scale training of deep learning models for
summarization. In sentence extractive summarization, researchers proposed 
a variety of hierarchical models with distinct sentence and document
level encoder networks that enabled sequential prediction of which sentences 
or words should be included in the summary \citep{cheng2016neural,nallapati2017summarunner}.


% \citet{cheng2016neural} developed a sentence extractive model
%that uses a word level convolutional neural network to encode sentences and a
%sentence level sequence-to-sequence model to predict which sentences to
%include in the summary. \citet{nallapati2017summarunner} proposed a different
%model using word-level bidirectional recurrent neural networks (RNNs) along
%with a sentence level bidirectinoal RNN for predicting which sentences should
%be extracted.  Additionally, the sentence extractor creates representations of
%the whole document and computes separate scores for salience, novelty, and
%location.

Perhaps more exciting was the surge in abstractive summary generation.
\cite{rush2015} developed an attention-based deep learning  model capable of
generating headlines from the lead sentence of an article. Subsequently,
\citet{nallapati2016} showed that a sequence-to-sequence model could encode a
whole news article and then generate an abstractive summary word by word. 
Additionally, the
line between extractive and abstractive summarization was blurred by the
addition of learned copy-mechanisms which could selectively transfer named
entities and other out-of-vocabulary terms into the output summary, further
improving summary quality \citep{see2017}.  

Historically, the field of NLG has relied heavily on grammars and templates to
generate text. The fact that neural models could yield reasonbly fluent and
acceptable summaries given so little pre-specfied structure or features is
truly impressive.  Interestingly, term frequency, was not explicitly
represented in either the neural extractive or abstractive summarization
models, begging the question as to how they were learning to identify salient
content.

Model architecture continued to evolve and in \citeyear{vaswani2017}, \citeauthor{vaswani2017}
  proposed a recurrence-free neural sequence model, built around so-called 
  \textit{transformer} layers,
  which rely on mulitiple parallel self- and context-attention mechanisms.
  This model was designed with optimization speed in mind, and was subsequently
  used in large scale language model pretraining on web-scale text, 
  spawning the BERT-family
  of models \citep{devlin2019}. Large language model pre-training with task-specific
fine-tuning became a dominant 
paradigm in NLP with BERT and its descendants setting records on many 
downstream text prediction tasks \citep{ruder2019}.

%BERT has since been used as a non-autoregressive means
 % of generating text \citep{}. 
  
The transformer layer was also used in the generative pre-training
(GPT)-family of models \citep{radford2018improving}, which was  also trained
on web-scale data, but with an auto-regressive language modeling objective.
The second generation of these models, GPT-2 \citep{radford2019language},
received notoriety both amongst NLP researchers but also the wider public, as
its release was initially delayed given ``ethical concerns'' about releasing
such a powerful language generation model \citep{vincent2019,seabrook2019}.
%Regardless, the model was eventually released
GPT-2 exhibited impressive completions of prompt texts, and could be used to 
generate natural looking paragraphs on arbitrary topics, with longer spans of
fluent text than previously thought possible. 
  
GPT-2 could also be fine-tuned to perform task specific conditional generation 
\citep{ziegler2019,golovanov2019}, however, its architecture was that of a
neural language model without distinct encoder/decoder networks; conditional
generation was achieved by encoding problem instances as prompts to be continued.
Proper sequence-to-sequence variants of the 
  large, transformer-based language model were subsequently developed
using various sequence-level denoising autoencoder objectives 
\citep{zhang2019,raffel2020, lewis2020}.
%  followed with sequence-to-sequence variants of the 
%  large ,transformer-based model pre-trained on denoising autoencoder objectives. 
The intention of such models was that they could be fine-tuned for more
task-specific sequence-to-sequence problems like summarization, translation,
or even arbitrary data-to-text tasks like dialogue generation.

While these models were producing text at a level of quality that had not 
previously been realized with traditional NLG approaches, a closer 
examination of model outputs revealed glaring flaws in the semantic correctness
of the generated text \citep{kryscinski2019,kryscinski2020,maynez2020}. 
\cite{dusek2020} found that the quality of neural NLG models
 can vary significantly, with relatively
similar architectures yielding both poor and competitve performance
with respect to the semantic correctness of model outputs. In practice
most competitive neural NLG models use a variety of beam reranking techniques 
to improve output faithfulness to the inputs
\citep{dusek2016,juraska2018,wen2015},
%All of the cited work on the E2E or Laptops and TVs datasets 
%uses beam search to achieve competitive performance.
%In addition, they often employ reranking to ensure that all attributes
%are realized 
%It's also possible that encoders are not strictly necessary, \cite{wen2015semantically} use discourse act embeddings
%to control the gating in a decoder LSTM cell; they too use beam search
%and reranking to ensure attributes are realized. 
as well as copy and coverage mechanisms to improve the recall 
\citep{see2017,elder2018}.
%pointer generators to directly copy attribute values, while also 
%using coverage penalties on the attention weights to ensure that 
%%all attribute slots are attended to. 
%Unlike these approaches, we do not require
%beam search, reranking, or other specialized attention mechanisms or loss
%functions to obtain low error rates. Instead we use data-augmentation
%to obtain a more reliable but simpler model.
%Data augmentation has also been used by prior neural generation models.
%\citet{juraskaslug2slug} breaks multi-sentence utterances into separate training
%instances. They also try training on more complex sentences alone but this
%model was less reliably able to realize all attributes correctly.
%They also  
%do not generate new utterance/MR pairs for training as we do. 
%complete utterances that are of high enough
%quality to train with as we do.

NLG researchers have also begun to explore the degree to which
neural models can be constrained to follow discourse plans or other 
structured objects.
\citet{nayak2017} and \citet{reed2018} explore several ways of incorporating
shallow sentence planning into dialogue generation via the  grouping of input 
sequences into distinct subsequences or by inserting discourse variable 
tokens into the encoder input sequences to 
indicate contrasts, comparisons, or other groupings. 
%entities in into sentence 
%  models, comparing a flat
%alignment order (equivalent to the alignment order used in this paper)
%against various sentence level groupings.  \citet{reed2018} add additional
%sentence and discourse structuring variables to indicate contrasts or
%sentential groupings.  
\citet{balakrishnan2019} experiment
both with tree structured input meaning representations  and encoders and compare them to linearized trees with standard sequence-to-sequence models. 
While these papers find that neural NLG models can consistently follow these
discourse ordering constraints,
%
%They also find that properly
%aligned linearization can lead to a controllable generator.
%\citet{moryossef2019} develop a planning model and train an S2S model to
%follow it.  
they do not  systematically explore how other linearization
strategies compare in terms of faithfulness, and they do not evaluate the
degree to which a sequence-to-sequence model can follow realization orders 
not drawn from the training distribution.

\citet{castroferreira2017} compare a neural NLG model using various
linearizations of abstract meaning representation (AMR) graphs, including a
model-based alignment very similar to the alignment-training 
linearization presented
in this work. However, they evaluate only on automatic quality measures and do
not explicitly measure the semantic correctness of the generated text or the
degree to which the model realizes the text in the order implied by the
linearized input.

Works like \citet{moryossef2019a,moryossef2019b} and
\citet{castroferreira2019} show that treating various planning tasks as
separate components in a pipeline, where the components themselves are
implemented with neural models, improves the overall quality and semantic
correctness of generated utterances relative to a completely end-to-end neural
NLG model. However, they do not test the systematicty of the neural generation
components, i.e. the ability to perform correctly when given an arbitrary or
random input from the preceding component, as we do here with the random
permutation stress test.% experiments. 

%
%First, in the 2017 paper, they compare a neural seq2seq model when using the
%depth first or model based alignment linearization for encoding the input.
%While their alignment model is analogous to our alignment training
%linearization, they evaluate only on automatic quality measures and do not
%explicitly measure the semantic correctness of the generated text or the
%degree to which the model realizes the text in the order implied by the
%linearized input.


Many papers mention input linearization order anecdottally but do quantify its
impact. For example, \citet{juraska2018} experiment with random
linearization orderings during development, but do not use them in the final
model or report results using them, and \citet{gehrmann2018} report that
using a consistent linearization strategy worked best for their models but do
not specify the exact order.  \citet{juraska2018} also used sentence
level data augmentation, i.e. splitting a multi-sentence example in multiple
single sentence examples, similar in spirit to our proposed phrase based
method, but they do not evaluate its effect independently.
\citet{wiseman2018} uses an order invariate encoder to produce a latent
plan which guides the decoder. Ignoring the encoder and specifying 
a latent plan would allow for some control over realization order but 
the degree to which arbitrary realization orders can be achieved is
under explored. Additionally, it is not guaranteed that latent plan
states uniquely correspond to different meaning representation components.




%Their model offers some control over the output ordering by way of 
%model ensembling but the exact degree of controlabilily is not extensively
%explored and relying on separate models for plan realization is probably
%not scalable.
%


%%%
