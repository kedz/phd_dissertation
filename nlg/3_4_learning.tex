\subsection{Learning}

Given a dataset of \meaningrepresentation/utterance pairs, $\corpus = \left\{\left(\mr^{(1)}, \utttoks^{(1)}\right), \ldots, \left(\mr^{(\corpusSize)},\utttoks^{(\corpusSize)} \right)\right\}$, 
and a 
\linearizationstrategy, $\ls$, the parameters, $\params$, of $\gen$~can be learned by approximately
minimizing the negative log-likelihood of the data,
\[ \hat{\params} \approx \argmin_{\params \in \Params} -\frac{1}{\corpusSize}\sum_{\left(\mr, \utttoks \right) \in \corpus} 
    \log \gen\left(\utttoks|\ls\left(\mr\right);\params\right).
\]
In practice, this is done with some variant of mini-batch stochastic gradient descent, e.g., Adam
\citep{kingma2014adam}.
Additionaly, label smoothing \citep{szegedy2016} and non-linear learning rates are often 
necessary in practice for optimizing the transformer-based \sequencetosequence~model.
Dropout is also typically applied to both the GRU and transformer models during training. 
