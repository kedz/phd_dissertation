\subsection{Phrase-based Data Augmentation}
\label{sec:pbda}

\input{nlg/figures/pbda.tex}

While the alignment training linearization induces control in a 
\sequencetosequence~model, the resulting model will still likely have trouble
following utterance plans that are not well represented in the training 
data. As we discussed in \autoref{whymodelsfail}, \sequencetosequence~models
do not seem understand the compositional nature of phrase structure
in language data. With this problem in mind, we propose a 
phrase-based data-augmentation method for creating additional training examples
from constituent phrases of the training data. In doing so, we directly
expose the model to instances of syntactic composition, and how that composition systematically changes the semantics of the utterance.
%there is evidence that sequence-to-sequence models do not behave very
%systemically with standard training methods 
%\cite{lake2018generalization,loula2018rearranging}. By systematic, we mean that
%$h = enc_\theta(\ls(\mr))$ is highly training data dependent and 
% small changes in $\ls(\mr)$ may lead to large, non-linear
%changes in h, and as a consequence, the decoder may succeed on
%on one $\ls(\mr)$ but fail on another $\ls^\prime(\mr)$ even when the 
%differences between the permutations are small. 
%With this in mind, we propose
%two data augmentations to make explicit examples of  modular pairwise 
%transitions and constituent structure reuse so that models trained with alignment training
%may systemically recombine these elements to generate any ordering.
%See \autoref{tab:augdat} for statistics of the generated training data.


%We augment the training data with MR/utterance pairs taken from constituent
%phrases in the original training data. 
We parse all training utterances and enumerate all constituent phrases
governed by 
NP, VP, ADJP, ADVP, PP, S, Sbar
non-terminals.\footnote{We used the \href{https://stanfordnlp.github.io/CoreNLP/}{Stanford CoreNLP parser v3.9.2}.} We then apply the utterance tagger
used for alignment training (see \autoref{sec:align})
to obtain a corresponding \meaningrepresentation, keeping the dialogue act 
of the original utterance. We discard
phrases with no realized attributes.
%See \autoref{tab:main.dataset.stats} for augmented data statistics.

Because 
we reclassify the \meaningrepresentation~of phrases using the utterance tagger,
the augmented  data includes examples of
how to invert binary attributes.
See for example in \autoref{fig:pbdaexample} where we extract the noun phrase ``a family-friendly establishment'' which implies \AV{family\_friendly}{yes} and its composition with a verb phrase ``is not'', which changes the meaning to \AV{family\_friendly}{no}.

%, e.g. from the phrase
%``is not on Mac,'' which implies \AV{has\_mac\_release}{no},
%we obtain the phrase ``on Mac'' which implies 
%\AV{has\_mac\_release}{yes}.
When presenting the linearized \meaningrepresentation of phrase examples to 
the model encoder
we prepend and append phrase specific start and stop tokens respectively
(e.g., \textit{<<s-NP>>} and \textit{<<e-NP>>}) to prevent the model
from ever producing an incomplete sentence when generating for a complete 
\meaningrepresentation.



