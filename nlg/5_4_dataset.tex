\subsection{Datasets}

We run our alignment training experiments on the E2E Challenge dataset as well
 as the more recently released 
 ViGGO corpus \citep{juraska2019} another English language, task-oriented dialogue
dataset.\footnote{\url{https://nlds.soe.ucsc.edu/viggo}}
The ViGGO corpus comes from the video game domain (e.g. conversations with
a video game recommendation agent)  and  contains 14 attribute types
and nine dialogue acts. %(\DA{Request Explanation}, \DA{Recommend}, etc.).  
In
addition to binary and categorical valued attributes, the corpus also features
list-valued attributes which can have a variable number of values, and an
open-class \Atr{specifier} attribute. 



%?% the \href{http://www.macs.hw.ac.uk/InteractionLab/E2E/}{E2E
%?%Challenge} corpus \cite{novikova2017} and the
%?The ViGGO dataset comes from the video game domain.
%? %These
%?datasets provide MR/utterance pairs from the restaurant and video game
%?domains, respectively. Examples from the E2E corpus (33,523 train/1,426
%?dev/630 test) can have up to eight unique attributes.  There is only one
%?dialogue act for the corpus, \textsc{Inform}.  Attribute-values are either binary
%?or categorical valued.

\subsubsection{MR/Utterance Alignments} \label{sec:align}

The original datasets do not have alignments between individual
attribute-value pairs and the subsequences of the utterances they occur in, 
which we
need for the alignment training linearization strategy.  We manually developed a
list of heuristic pattern matching rules (e.g. \textit{not kid-friendly}
$\rightarrow$ \AV{family\_friendly}{no}) which we use to tag the utterance
tokens.  For ViGGO, we started from scratch,
but for the E2E Challenge dataset we greatly expanded the rule-set created by \citet{dusek2019}.  To
ensure the correctness of the rules, we iteratively added new matching rules,
ran them on the training and validation sets, and verified that they produced
the same \meaningrepresentation~as was provided in the dataset. This process took the author
roughly two weeks to produce approximately 25,000 and 1,500 rules for the E2E
and ViGGO datasets respectively. Note that the large number of rules is
obtained programmatically, i.e. creating template rules and inserting matching
keywords or phrases (e.g., enumerating variants such as \textit{not
kid-friendly}, \textit{not child-friendly}, \textit{not family-friendly}, etc.).


\input{nlg/tables/cgdata.tex}

In cases where the matching rules produced different \meaningrepresentations~than provided in the
original dataset, we manually checked them. If the rule was incorrect,
we added a new rule to account for the exception. 
In many cases in the E2E Challenge
dataset
and several times in the ViGGO corpus, we found the rule to be correct and the \meaningrepresentation~to be
incorrect for the given utterance. In those cases, we used the corrected \meaningrepresentations~for
training and validation. %To maintain comparability to prior work, 
We
do not modify the test sets in any way. We follow \citet{dusek2019} and remove from the training and validation
sets any modified exampels that share a \meaningrepresentation~also
found in the test set. This creates slightly different training and validation
set numbers for the E2E Challenge
dataset than in the faithful generation experiments. See \autoref{tab:cgdata} for 
statistics.
We use the matching rules to develop  a rule-based utterance 
tagger to implement the alignment training linearization, phrase-based data augmentation
protocol, and as a reranker when generating utterances in our experiments.
%, we can
%determine alignments between the provided \meaningrepresentation~and the realized utterances.





For most cases, the attribute-values uniquely correspond to a non-overlapping
subsequences of the utterance. The \Atr{rating} attribute in the ViGGO dataset,
however, could have multiple reasonable mappings to the utterance, so we treat
it in practice like an addendum to the dialogue act, occurring directly after the
dialogue act as part of a ``header'' section in any \meaningrepresentation~linearization strategy
(see \autoref{fig:linstrats} where \AV{rating}{N/A} occurs after the dialogue
act regardless of choice of linearization strategy).

\paragraph{Delexicalization} The ViGGO corpus is relatively small and the
attributes \Atr{name}, \Atr{developer}, \Atr{release\_year},
\Atr{expected\_release\_date}, and \Atr{specifier}~can have values that are
only seen several times during training. Neural models often struggle to learn
good representations for infrequent inputs, which can, in turn, lead to poor
test-set generalization. To alleviate this, we delexicalize these values in
the utterance. That is, we replace them with an attribute specific placeholder
token.

\label{app:specifier} Additionally, for \Atr{specifier} whose values come from the open class of
adjectives, we represent the specified adjective with a placeholder which
marks two features, whether it is consonant (C) or vowel initial (V) (e.g.
``\uline{d}ull'' vs. ``\uline{o}ld'') and whether it is in regular (R) or
superlative (S) form (e.g. ``dull'' vs. ``dullest'') since these features can
effect the surrounding context in which the adjective is realized.  See the
following lexicalized/delexicalized examples:
\begin{itemize}
        \item \AV{specifier}{oldest}~-- vowel initial, superlative
\begin{itemize}
    \item \textit{What is the oldest game you've played?}
    \item \textit{What is the SPECIFIER\_V\_S game you've played?}
\end{itemize}
        \item \AV{specifier}{old}~-- vowel initial, regular

\begin{itemize}
    \item \textit{What is an old game you've played?}
    \item \textit{What is an SPECIFIER\_V\_R game you've played?}
\end{itemize}

        \item \AV{specifier}{new}~-- consonant initial, regular

\begin{itemize}
    \item \textit{What is a new game you've played?}
    \item \textit{What is a SPECIFIER\_C\_R game you've played?}
\end{itemize}
\end{itemize}

All generated delexicalized utterances are post-processed with the
corresponding attribute-values before computing evaluation metrics (i.e., 
they are re-lexicalized with the appropriate value strings from the input \meaningrepresentation). Unlike in the faithful generation experiments, we 
do not perform any delexicalization of the E2E Challenge corpus.






