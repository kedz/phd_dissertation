\section{Controllable Generation}
\label{sec:nlgcg}

Let $\mr \in \mrspace$ be a \meaningrepresentation~with a \dialogueact~$\mrtok_0$
and $\mrSize$ \attributevalues~$\mrtok_1,\ldots,\mrtok_\mrSize$, and let
$\utttoks = \left[\utttok_1,\ldots,\utttok_\uttSize\right]$ be 
an utterance that denotes $\mr$, which we write as $\denotes{\utttoks} = \mr$.
A contiguous span of utterance tokens $\utttoks_{i:j} = \left[\utttok_i,\ldots,\utttok_j\right]$, for $i \le j$, 
can denote zero or more \attributevalues, that is,
$\denotes{\utttoks_{i:j}} = \{\mrtok_{k_1}, \mrtok_{k_2}, \ldots\}$. We further define a ``denotation
set'' of $\mr$ and $\utttoks$ as $\denotationset = \left\{(i^{(k)},j^{(k)}): k \in \left\{1,\ldots,\mrSize\right\} \right\}$ where 
$\denotes{\utttoks_{i^{(k)}:j^{(k)}}} = \{ \mrtok_k \}$. In other words,
$\denotationset$ is the set of utterance token spans that denote a single
\attributevalue.

We say that a \surfacerealization~model $\model$ is controllable if 
we can specify the \surfacerealizationorder~for $\model$ to follow. That is,
given a \linearizationstrategy~$\ls$ such that $\ls(\mr) = \mrtoks = \left[\mrtok_0, \mrtok_{\pi_1}, \ldots, \mrtok_{\pi_\mrSize}\right]$, a controllable
\surfacerealization~model $\model\left(\cdot|\mrtoks;\params\right)$
will put more probability mass on utterances $\utttoks$ where the 
$\denotationset_{\mr,\utttoks}$ has the property that $j^{(\pi_k)} < i^{(\pi_{k+1})}$. In other words, the \surfacerealizationorder~of the \attributevalues~follows the permutation implied by $\ls$.

\autoref{fig:explans} shows an example of controllable \surfacerealization~for the \meaningrepresentation
 \begingroup
 \renewcommand*\arraystretch{.6}
\begin{align} \mr =  \left[\!\!\left[ \begin{array}{l} (\mrtok_0)\; \textsc{Inform} \\ (\mrtok_1)\; \textrm{name=Aromi} \\ (\mrtok_2)\; \textrm{area=city centre} \\ (\mrtok_3)\; \textrm{eat\_type=coffee shop} \end{array} \right]\!\!\right]\label{eqn:mr1}\end{align}
\endgroup
when following either one of two different linearizations \[\ls^{(1)}(\mr) = \left[ \mrtok_0, \mrtok_1, \mrtok_3, \mrtok_2 \right]\quad \textrm{and} \quad \ls^{(2)}(\mr) = \left[ \mrtok_0, \mrtok_3, \mrtok_2, \mrtok_1 \right].\] When using a controllable model, 
we refer to a linearization $\ls$ as an \utteranceplan. In this work, 
a \linearizationstrategy~will only permute the location of \attributevalue~tokens, while the \dialogueact~token $\mrtok_0$ will always occupy the first
position of any \meaningrepresentation~token sequence $\mrtoks$. 



\begin{figure}
\begin{subfigure}{\textwidth}
\caption{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}
\center
\fbox{\begin{minipage}{0.87\textwidth}
\begin{tabular}{cccccc}
\multirow{2}{*}{$\ls^{(1)}(\mr) = \Bigg[$} & $x_0$ &   $x_{\pi_1}$ & $x_{\pi_2}$ & $x_{\pi_3}$ & \multirow{2}{*}{$\Bigg]$}\\
& inform & name=Aromi & eat\_type=coffee shop & area=city center \\
\end{tabular}

~\\[5pt]

\begin{tabular}{cccccccccccc}
\multirow{2}{*}{~~~~~~$\utttoks^{(1)} = \Bigg[$} & $y_1$ & $y_2$ & $y_3$ & $y_4$ & $y_5$ & $y_6$ & $y_7$ & $y_8$ & $y_9$ & $y_{10}$ & \multirow{2}{*}{$\Bigg]$} \\
&Aromi & is & a & coffee & shop & in & the & city & centre & . \\
\end{tabular}

~\\[5pt]

\begin{tabular}{ccccc}
 \multirow{2}{*}{~~$\denotationset_{\mr,\utttoks^{(1)}} = \Bigg\{$} & $(i^{(\pi_1)},j^{(\pi_1)})$ & 
    $(i^{(\pi_2)},j^{(\pi_2)})$ & 
    $(i^{(\pi_3)},j^{(\pi_3)})$ & \multirow{2}{*}{$\Bigg\}$}  \\
   &  (1, 1) & (2, 5) & (6, 9) 
\end{tabular}


\begin{center}
\begin{tabular}{ccc}
$\utttoks^{(1)}_{i^{(\pi_1)}:j^{(\pi_1)}}$ & 
$\utttoks^{(1)}_{i^{(\pi_2)}:j^{(\pi_2)}}$ &
$\utttoks^{(1)}_{i^{(\pi_3)}:j^{(\pi_3)}}$   \\
\cmidrule(lr){1-1}
\cmidrule(lr){2-2}
\cmidrule(lr){3-3}
    $\left[\textrm{Aromi}\right]$ & $\left[\textrm{is a coffee shop}\right]$ &
        $\left[\textrm{in the city center}\right]$ \\ 
\end{tabular}
\end{center}
\end{minipage}}
\end{subfigure}

%Aromi\textsubscript{1} is\textsubscript{2} a\textsubscript{3} coffee\textsubscript{4} shop\textsubscript{5} in\textsubscript{6} the\textsubscript{7} city\textsubscript{8} centre\textsubscript{9} .\textsubscript{10}

~\\
~\\

\begin{subfigure}{\textwidth}
\caption{~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~}
\begin{tabular}{cccccc}
\multirow{2}{*}{$\ls^{(2)}(\mr) = \Bigg[$} & $x_0$ &   $x_{\pi_1}$ & $x_{\pi_2}$ & $x_{\pi_3}$ & \multirow{2}{*}{$\Bigg]$}\\
& inform & eat\_type=coffee shop & area=city center & name=Aromi\\
\end{tabular}

~\\[5pt]

\begin{tabular}{cccccccccccccc}
\multirow{2}{*}{~~~~~~$\utttoks^{(2)} = \Bigg[$} & $y_1$ & $y_2$ & $y_3$ & $y_4$ & $y_5$ & $y_6$ & $y_7$ & $y_8$ & $y_9$ & $y_{10}$ & $y_{11}$& $y_{12}$ & \multirow{2}{*}{$\Bigg]$} \\
&For & coffee & in & the & centre & of & the & city & , & try & Aromi & .\\
\end{tabular}

~\\[5pt]

\begin{tabular}{ccccc}
 \multirow{2}{*}{~~$\denotationset_{\mr,\utttoks^{(2)}} = \Bigg\{$} & $(i^{(\pi_1)},j^{(\pi_1)})$ & 
    $(i^{(\pi_2)},j^{(\pi_2)})$ & 
    $(i^{(\pi_3)},j^{(\pi_3)})$ & \multirow{2}{*}{$\Bigg\}$}  \\
   &  (1, 2) & (3, 8) & (11, 11) 
\end{tabular}

~\\[5pt]

\begin{tabular}{ccc}
$\utttoks^{(2)}_{i^{(\pi_1)}:j^{(\pi_1)}}$ & 
$\utttoks^{(2)}_{i^{(\pi_2)}:j^{(\pi_2)}}$ &
$\utttoks^{(2)}_{i^{(\pi_3)}:j^{(\pi_3)}}$   \\
    $\left[\textrm{For coffee}\right]$ & $\left[\textrm{in the centre of the city}\right]$ &
        $\left[\textrm{Aromi}\right]$ \\ 
\end{tabular}
\end{subfigure}

~\\

\caption{Examples of two possible utterance plans for $\mr$ (defined \autoref{eqn:mr1}) implied by
the linearizations $\ls^{(1)}$ (a) and $\ls^{(2)}$ (b), their realizations $\utttoks$, denotation sets $\denotationset_{\mr,\utttoks}$, and corresponding
attribute-value spans $\utttoks_{i:j}$.}
\label{fig:explans}
\end{figure}





\subsection{Alignment Linearization}
As was mentioned in \autoref{mrtproblemdef}, there are many choices of
\linearizationstrategy~$\ls$ when modeling \surfacerealization~with
a \sequencetosequence~archictecture. One particular \linearizationstrategy,
which we call the \alignmenttraining~linearization, yields a controllable \sequencetosequence~model which is capable of following an utterance plan for \surfacerealization. 


Unlike the {\color{red}arbitrary linearization} used in \autoref{somewhere}, \alignmenttraining~linearization~is not soley a function of $\mr$, but is determined by both $\mr$ and a reference utterance $\utttoks$. Given a $\left(\mr, \utttoks\right)$ pair, the \alignmenttraining~linearization finds a linearization
$\ls$ such that the order of the \attributevalues in $\ls(\mr)$ corresponds
to the order they are realized in $\utttoks$. Formally, this means that 
for any $\left(i^{(\ls_k)},j^{(\ls_k)}\right), \left(i^{(\ls_{k+1})},j^{(\ls_{k+1})}\right) \in \denotationset_{\mr,\utttoks}$, we have $j^{(\ls_k)} < i^{(\ls_{k+1})}$.


\autoref{fig:atexamples} shows some examples of the 
\alignmenttraining~linearization, including some special cases. When
linearizing list-valued attributes, for instance, we treat them as distinct
\attributevalue~pairs. Occasionally, we encounter repeated \attributevalues~in the training set, and in that case we include extra \attributevalue~pairs
in the correpsonding location in the linearization. We ignore any instances
of  ungrounded information, as in example \autoref{} where (such and such)
has no explicit representation.  

\subsubsection{Alignment Training Implementation}


\begin{algorithm}[t]
    \caption{Alignment Training Linearization Algorithm}
    \label{alg:at}
    \DontPrintSemicolon
    \KwData{\utterance~$\utttoks \in \outSpace$, \attributevalue~tagger $\tagger$}  
    Tag the utterance, $\mathbf{t} \gets \tagger(\utttoks)$.   \\
    Segment $\mathbf{t}$ into $k$ spans by splitting $\mathbf{t}$ on tags mapped to no \attributevalue~(i.e. $t_i = \emptyset$): $\boldsymbol{\hat{t}} = \left[\ldots,\left[t_{{i_k}},t_{i_{k}+1},\ldots, t_{j_k}\right],\ldots\right]$ \\
    Segment each of the $k$ sub-segments into maximal length contiguous
    spans sharing the same tag.
\end{algorithm}

In practice, there are two distinct and important issues when using the 
\alignmenttraining~linearization. The first, which occurs during training,
is that we need for each training set example $\left(\mr, \utttoks\right)$
to tag the utterance tokens 
$\utttoks = \left[\utttok_1,\ldots,\utttok_\uttSize\right]$ 
with a corresponding tag sequence 
$\mrtags=\left[\mrtag_1,\ldots, \mrtag_\uttSize\right]$ where each 
tag $\mrtag_i$ is equal to an \attributevalue~$\mrtok_j \in \mr$ or 
the null tag $\nulltag$. We assume that we have access to such a tagger
$\tagger : \outSpace \rightarrow \inSpace$ (see \autoref{sec:tagger}
for implementation details). After producing the tag sequence $\mrtags^{(1)} = \tagger\left(\utttoks\right)$ (Lines ? in alg and Figure ???.b), we then segment $\mrtags$ into contiguous sequences of tags containing no null tags $\nulltag$,
$\mrtags^{(2)}$ (Line ? in alg and Figure ???.c). Each subsequence of $\mrtags^{(2)}$ is further split into the longest contiguous tag segments such that all 
tags a segment are equal to the same \attributevalue~pair, $\mrtags^{(3)} = {}$ (Line ? in alg and Figure ???.d).
Finally, $\mrtoks$ is constructed by 
by prepending the \dialogueact~$\mrtok_0$ of $\mr$ to the ordered sequence 
of \attributevalue~pairs $???$ implied by $\mrtoks^{(3)}$ (Line ? in alg and Figure ???.e). 

The second issue is at test time, a \surfacerealization~model is only
presented with a \meaningrepresentation~$\mr$ and we don't have a reference 
utterance $\utttoks$ with which to apply \autoref{alg:at}. In this case,
we can use either an utterance planning model $\planner : \mrspace \rightarrow \inSpace$ which maps a \meaningrepresentation~$\mr$ to a
linearized $\mr$, $\mrtoks$ or alternatively a linearization implied by 
a human reference utterance. In the latter case, this represents an
unrealistic case where the model has clairvoyant knowledge of the 
discourse ordering of the desired utterance but is also representative
of discourse orderings prefered by humans. In either case, we refer to 
a linearization $\mrtoks$ obtained either from $\planner$ or a human reference
as an \utteranceplan~since a \surfacerealization~model trained with
\alignmenttraining~linearizations will attempt to follow it during 
the generation of the utterance.




\tikzset{mynode/.style={anchor=center, minimum height=1.5em, 
      text height=1.5ex, text depth=.25ex, align=center}}

      \begin{figure}[p]
    \fbox{\begin{minipage}{\textwidth}
            ~\\
        \center
        \scalebox{0.75}{
    \begin{tikzpicture}
        \def\titlex{-3.0}
        \def\colwidth{1.25}
        \def\utttitleheight{2}
        \def\uttheightA{0.0}
        \def\uttheightB{-1.0}
        \def\tagtitleheight{-3.0}
        \def\tagheightA{-5.0}
        \def\tagheightB{-6.0}
        \def\nullsegtitleheight{-12.0}
        \def\nullsegheightA{-14.0}
        \def\mrsegtitleheight{-16.0}
        \def\mrsegheightA{-18.0}
        \def\attitleheight{-20.0}
        \def\atheightA{-22.0}
        \def\atheightB{-23.0}



        \def\city{{area=city centre}};
        \def\coffee{{eat\_type=coffee shop}};
       % \def\nulltag{{$\emptyset$}};
        \def\name{{name=Aromi}};
        \def\words{For, coffee, in, the, centre, of, the, city, {,}, try, 
                   Aromi, .};
        \def\tags{\coffee, \coffee, \city, \city, \city, \city, \city, \city,
                     \nulltag, \nulltag, \name, \nulltag};

        %%% Input Utterance
        \node[mynode,anchor=west] at (\titlex*\colwidth,\utttitleheight) 
            {\Large (a) Input Utterance};

        \foreach \w [count=\wi from 1] in \words {
            \node[mynode] at (\wi*\colwidth,\uttheightA) 
                {$\utttok_{\wi}\ifthenelse{\wi = 12}{}{,}$};
            \node[mynode] at (\wi*\colwidth,\uttheightB) {\w};
        }
        \node[mynode] at (0*\colwidth,\uttheightA) {$\utttoks = \Bigg[$};
        \node[mynode] at (12*\colwidth+0.5,\uttheightA) {$\Bigg]$};


        %%% Tagged Utterance
        \node[mynode,anchor=west] at (\titlex*\colwidth,\tagtitleheight) 
            {\Large (b) Tagged Utterance};
        \foreach \t [count=\ti from 1] in \tags {
            \node[mynode] at (\ti*\colwidth,\tagheightA) 
                {$\mrtag_{\ti}\ifthenelse{\ti = 12}{}{,}$};
            \node[mynode,rotate=90,anchor=east] at 
                (\ti*\colwidth,\tagheightB) {\t};
        }
        \node[mynode] at (0*\colwidth,\tagheightA) {$\mrtags^{(1)} = \Bigg[$};
        \node[mynode] at (12*\colwidth+0.5,\tagheightA) {$\Bigg]$};


        %%% Null Segmented Tags
        \node[mynode,anchor=west] at (\titlex*\colwidth,\nullsegtitleheight) 
            {\Large (c) Null Segmented Tags};
        \node[mynode] at (0*\colwidth,\nullsegheightA) 
            {$\mrtags^{(2)} = \Bigg[$};
        \node[mynode] at (12*\colwidth+0.5,\nullsegheightA) {$\Bigg]$};        
        \foreach \tprime in {1,...,8} {
            \node[mynode] at (\tprime*\colwidth,\nullsegheightA) 
            {$\mrtag_{\tprime}\ifthenelse{\tprime = 8}{}{,}$};
        }
        \foreach \tprime in {11} {
            \node[mynode] at (\tprime*\colwidth,\nullsegheightA) 
                {$\mrtag_{\tprime}$};
        }

        \node[mynode] at (\colwidth-0.50,\nullsegheightA) {$\bigg[$};
        \node[mynode] at (8*\colwidth+0.50,\nullsegheightA) {$\bigg],$};
        \node[mynode] at (11*\colwidth-0.50,\nullsegheightA) {$\bigg[$};
        \node[mynode] at (11*\colwidth+0.50,\nullsegheightA) {$\bigg]$};


        %%% MR Segmented Tags
        \node[mynode,anchor=west] at (\titlex*\colwidth,\mrsegtitleheight) 
            {\Large (d) MR Segmented Tags};
        \node[mynode] at (0*\colwidth,\mrsegheightA) 
            {$\mrtags^{(3)} = \Bigg[$};
        \node[mynode] at (12*\colwidth+0.5,\mrsegheightA) {$\Bigg]$};        
        \foreach \tprime in {1,...,2} {
            \node[mynode] at (\tprime*\colwidth,\mrsegheightA) 
            {$\mrtag_{\tprime}\ifthenelse{\tprime = 2}{}{,}$};
        }
        \foreach \tprime in {3,...,8} {
            \node[mynode] at (\tprime*\colwidth,\mrsegheightA) 
            {$\mrtag_{\tprime}\ifthenelse{\tprime = 8}{}{,}$};
        }
        \foreach \tprime in {11} {
            \node[mynode] at (\tprime*\colwidth,\mrsegheightA) 
                {$\mrtag_{\tprime}$};
        }

        \node[mynode] at (\colwidth-0.50,\mrsegheightA) {$\bigg[$};
        \node[mynode,anchor=center] at (2*\colwidth+0.65,\mrsegheightA) {$\Bigg]_1, \Bigg[$};
    \node[mynode] at (8*\colwidth+0.50,\mrsegheightA) {$\bigg]_2,$};
        \node[mynode] at (11*\colwidth-0.50,\mrsegheightA) {$\bigg[$};
        \node[mynode] at (11*\colwidth+0.50,\mrsegheightA) {$\bigg]_3$};


        %%% Alignment Training Linearization
        \node[mynode,anchor=west] at (\titlex*\colwidth,\attitleheight) 
            {\Large (e) Alignment Training Linearization};
        \node[mynode,anchor=east] at (-1.5*\colwidth-0.5,\atheightA) 
            {$\mrtoks = \Bigg[$};
        \node[mynode] at (-1.5*\colwidth,\atheightA) {$\mrtok_0,$};
        \node[mynode] at (1.5*\colwidth,\atheightA) {$\mrtok_1,$};
        \node[mynode] at (5.5*\colwidth,\atheightA) {$\mrtok_2,$};
        \node[mynode] at (11*\colwidth,\atheightA) {$\mrtok_3$};
        \node[mynode] at (11*\colwidth+0.5,\atheightA) {$\Bigg]$};
%
        \node[mynode] at (-1.5*\colwidth,\atheightB) {inform,};
        \node[mynode] at (1.5*\colwidth,\atheightB) {eat\_type=coffee shop,};
        \node[mynode] at (5.5*\colwidth,\atheightB) {area=city centre,};
        \node[mynode] at (11*\colwidth,\atheightB) {name=Aromi};



    \end{tikzpicture}
    }
    ~\\
    \caption{Example steps of the \alignmenttraining~linearization algorithm
        (\autoref{alg:at})
        for producing a linearized \meaningrepresentation~$\mrtoks$.}
        \label{fig:at}
\end{minipage}}
\end{figure}



%Assume we have a
%training corpus $\corpus = \left\{\left(\mr^{(1)}, \utttoks^{(1)}\right), \ldots, \left( \mr^{(\corpusSize)}, \utttoks^{(\corpusSize)}\right) \right\}$
%
%%That is, given such a model $\model$,
%% a \dialogueact~$\mrtok_0$ a series of \attributevalues~$\mrtok_1,\mrtok_2,\ldots,\mrtok_\mrSize$, the conditional distribution,
%%\[\model\left(\cdot|\mrtok_0,\mrtok_1,\ldots,\mrtok_\mrSize;\params\right)\]
%%prefers utterances $\utttoks$ such that \attributevalues~are realized 
%%in the order specified 
%
%In the \alignmenttraining~linearization,
%during training the order of
%attribute-value pairs $\attr_1, \attr_2, \ldots, \attr_\size{\mr}$
%matches the order in which they are realized in the
%corresponding training utterance.
%This is feasible because in the majority of cases, there is a one-to-one
%mapping of attribute-values and utterance sub-spans.
%
%
%We obtain this ordering using a manually constructed set of matching rules
%to identify which utterance sub-spans correspond to each attribute-value
%pair (see \autoref{sec:align}).

\begin{figure}
    \includegraphics{nlg/inputordering.pdf}
\end{figure}



\newcommand{\lsname}[1]{\textsc{#1}}
\newcommand{\lsshort}[1]{\textsc{#1}}
\newcommand{\acount}{n}
\newcommand{\size}[1]{|#1|}
\newcommand{\lin}{\pi}
\newcommand{\valstr}[1]{\textit{#1}}
\newcommand{\uttstr}[1]{\textit{#1}}
\newcommand{\alignshort}{AT}
\newcommand{\enc}{Enc}
\newcommand{\rep}{h}
\newcommand{\attrval}[2]{#1=#2}
\newcommand{\phraseAug}{+p}
\newcommand{\bleu}{\textsc{Bleu}}
\newcommand{\rougel}{\textsc{Rouge-L}}
\newcommand{\biGRU}{biGRU}
\newcommand{\BgUP}{\textsc{BgUP}}
\newcommand{\NUP}{\textsc{NUP}}
\newcommand{\DA}[1]{\textsc{#1}}
\newcommand{\Oracle}{\textsc{Oracle}}
\newcommand{\BART}{BART}
\newcommand{\Transformer}{Transformer}

\subsubsection{Alternative Linearization Strategies}

\paragraph{Random (\textsc{Rnd})} In the \lsname{random} linearization (\lsshort{Rnd}), we randomly
order the attribute-value pairs for a given MR. This strategy 
serves as a baseline for determining
if linearization ordering matters at all for faithfulness. \lsshort{Rnd} 
is similar to token level noise used in
sequential denoising autoencoders \cite{wang2019denoising} and might even improve faithfulness.
During training, we resample the ordering for each example at every epoch.
We do not resample the validation set in order to obtain stable results 
from which to pick the best model.

\paragraph{Increasing Frequency (\textsc{If})} 
In the \lsname{increasing frequency} linearization (\lsshort{If}), 
we order the attribute-value pairs by increasing frequency of 
occurrence in the training data
i.e. $\acount(\attr_i) \le \acount(\attr_{i+1})$.
We hypothesize that placing frequently occurring items in a consistent location
may make it easier for $\model$ to realize those items correctly, possibly
at the expense of rarer items.

\paragraph{Fixed Position (\textsc{Fp})} We take  consistency one step further 
and create a fixed ordering of all attributes, \textit{n.b.} not attribute-values, ordering them in increasing
frequency of occurrence on the training set (i.e. every instance has the same
order of attributes in the encoder input). In this \lsname{fixed position}
linearization (\lsshort{Fp}), attributes that are not present 
in an MR are explicitly represented with an \valstr{N/A} value. 
For list-valued slots, we determine the maximum length list in the training
data and create that many repeated slots in the input sequence.
This linearization is feasible for datasets with a modest number of 
unique attributes (in our case Viggo has 14 attributes and
E2E eight) but would not easily scale to 10s, 100s, or larger
attribute vocabularies. 




Crucially, \alignmenttraining~stands in contrast to the  first three strategies
(\lsshort{Rnd}, \lsshort{If}, and \lsshort{Fp}) which do not have any
correspondence between the the order of attribute-value pairs in $\lin(\mr)$
and the order in which they are realized in the corresponding
utterance $\utttoks$.


%At test time, when there is no reference utterance \lsshort{At}
%cannot specify a linearization. However, models trained with \lsshort{At}
%can generate an  utterance from an arbitrary utterance plan
%$\attr_1, \attr_2, \ldots, \attr_{\size{\mr}}$
%provided by an external source, such as an utterance planner model or human
%reference.



\subsection{Phrase-based Data Augmentation}
While the \alignshort~linearization leads to a controllable model,
there is evidence that sequence-to-sequence models do not behave very
systemically with standard training methods 
\cite{lake2018generalization,loula2018rearranging}. By systematic, we mean that
$\rep = \enc_\theta(\lin(\mr))$ is highly training data dependent and 
 small changes in $\lin(\mr)$ may lead to large, non-linear
changes in \rep, and as a consequence, the decoder may succeed on
on one $\lin(\mr)$ but fail on another $\lin^\prime(\mr)$ even when the 
differences between the permutations are small. With this in mind, we propose
two data augmentations to make explicit examples of  modular pairwise 
transitions and constituent structure reuse so that \alignshort~trained models
may systemically recombine these elements to generate any ordering.
See \autoref{tab:augdat} for statistics of the generated training data.


We augment the training data with MR/utterance pairs taken from constituent
phrases in the original training data. 
We parse all training utterances and enumerate all constituent phrases
governed by 
NP, VP, ADJP, ADVP, PP, S, Sbar
non-terminals.\footnote{We used the \href{https://stanfordnlp.github.io/CoreNLP/}{Stanford CoreNLP parser v3.9.2}.} We then apply the attribute-value 
matching rules
used for \lsshort{At} (see \autoref{sec:align})
to obtain a corresponding MR, keeping the dialog act 
of the original utterance. We discard
phrases with no realized attributes.
See \autoref{tab:main.dataset.stats} for augmented data statistics.

Because 
we reclassify the MR of phrases using the matching rules, 
the augmented  data includes examples of
how to invert binary attributes, e.g. from the phrase
``\uttstr{is not on Mac,}'' which implies \attrval{has\_mac\_release}{no,}
we obtain the phrase ``\uttstr{on Mac}'' which implies 
\attrval{has\_mac\_release}{yes.}
When presenting the linearized MR of phrase examples to the model encoder
we prepend and append phrase specific \textit{start} and \textit{stop} tokens respectively
(e.g., \textit{start-NP} and \textit{stop-NP}) to prevent the model
from ever producing an incomplete sentence when generating for a complete MR.







\subsection{Models}
\subsubsection{Generation Models}
We examine the effects of linearization strategy and data augmentation
on a bidirectional gated recurrent units with attention (biGRU)
and Transformer based S2S models.
Hyper-parameters were found using grid-search, selecting the model
with best validation \textsc{Bleu} \cite{papineni2002bleu} score. We performed a separate
grid-search for each architecture-linearization strategy pairing in case
there was no one best hyper-parameter setting.

Additionally, we
fine-tune BART \cite{lewis2019bart}, a large pretrained Transformer-based S2S model. We stop fine-tuning after validation set cross-entropy stops decreasing.
%lowest validation set cross-entropy. 

\paragraph{biGRU Model}
Let $\mathcal{V}$ be the encoder input vocabulary, and  $V \in\mathbb{R}^{\size{\mathcal{V}}\times 512}$ its associated embedding matrix, and let
$V_v \in \mathbb{R}^{512}$ be the associated embedding for
each $v \in \mathcal{V}$.

Let the encoder input $\lin(\mr) = v_1, v_2, \ldots, v_{\size{\mr}}$.
The hidden states of the first GRU encoder layer are computed as
\begin{align*}
        \overrightarrow{h}_0 & =\overleftarrow{h}_{\size{x} + 1}  = 0 \\
        \overrightarrow{h}_i &= \overrightarrow{\operatorname{GRU}}(V_{v_i}, \overrightarrow{h}_{i-1}) &
        \textrm{for $i \in 1,\ldots,\size{\mr}$}\\
            \overleftarrow{h}_i &= \overleftarrow{\operatorname{GRU}}(V_{v_i}, \overleftarrow{h}_{i+1}) &
            \textrm{for $i \in 1,\ldots,\size{\mr}$}\\
                h_i^{(1)} & = \left[\overrightarrow{h}_i, \overleftarrow{h}_i\right]
        \end{align*} When using a two layer GRU, we similarly compute
        \begin{align*}
                \overrightarrow{h}^{(2)}_0 & =\overleftarrow{h}^{(2)}_{\size{x} + 1}  = 0 \\
                \overrightarrow{h}^{(2)}_i &= \overrightarrow{\operatorname{GRU}}(h^{(1)}_i, \overrightarrow{h}^{(2)}_{i-1}) &
                \textrm{for $i \in 1,\ldots,\size{\mr}$}\\
                    \overleftarrow{h}^{(2)}_i &= \overleftarrow{\operatorname{GRU}}(h^{(1)}_i, \overleftarrow{h}^{(2)}_{i+1}) &
                    \textrm{for $i \in 1,\ldots,\size{\mr}$}\\
                        h_i^{(2)} & = \left[\overrightarrow{h}^{(2)}_i, \overleftarrow{h}^{(2)}_i\right]
                \end{align*}
                Going forward, let $h_i$ correspond to the final encoder, i.e. $h_i = h_i^{(1)}$
                in the one-layer biGRU case, and $h_i=h_i^{(2)}$ in the two layer case.
                Let $\mathcal{W}$ be the vocabulary of utterance tokens, and $W\in\mathbb{R}^{
                \mathcal{W} \times 512}$ it's associated embedding matrix, and let
                $W_w \in \mathbb{R}^{512}$ be the associated embedding for
                each $w \in \mathcal{W}$.

                Given the decoder input sequence $y = w_1, w_2, \ldots, w_{\size{y}}$,
                we compute the hidden states of the decoder as,
                \begin{align*}
                        g_0 & = \tanh(U^Th_{\size{x}} + u) \\
                        g_i & = \operatorname{GRU}(W_{w_i}, g_{i-1}) & \textrm{for $i \in 1,\ldots, \size{y}$}\\
                        \bar{h}_i & = \sum^{\size{x}}_{j=1}\alpha_{i,j}h_j & \textrm{for $i \in 1,\ldots, \size{y}$}
                \end{align*}
                where $U \in \mathbb{R}^{1024 \times 512}$ and $u \in \mathbb{R}^{512}$
                and $\alpha_{i,j} \in (0, 1), \sum^{\size{x}}_{j=1}\alpha_{i,j} = 1$ is the attention weight of decoder state $i$ on encoder state $j$. We compute
                attention in one of two ways (the attention method is a hyper-paremeter
                option):
                \begin{enumerate}
                    \item Feed-forward ``Bahdanau'' style attention \cite{cho2014learning},
                            also known as ``concat'' \cite{luong2015effective}:
                        \[ \alpha_{i,j} = k^T\tanh\left(K^T \left[\begin{array}{c} g_i \\ h_j\end{array}\right] \right)  \] with $K\in \mathbb{R}^{1536 \times 512}$ and $k \in \mathbb{R}^{512}$.
                        \item ``general'' \cite{luong2015effective} :
                                \[ \alpha_{i,j} = g_i^T K  h_j  \] with $K \in \mathbb{R}^{512 \times 1024}$.
                        \end{enumerate}

                        Finally, for $i \in 1, \ldots, \size{y}-1$ we compute
                        \begin{align*}
                        \bar{g}_i & =  \tanh\left(Q^T\left[\begin{array}{l} g_i\\ \bar{h}\end{array}\right] + q\right) \\
                                p\big(w_{i+1}|w_{\le i}, \lin(\mr)\big) & \propto \exp\left(M_{w_{i+1}}^T\hat{g}_i + m_{w_{i+1}}\right)  
                        \end{align*}
                        where $Q \in \mathbb{R}^{1024 \times 512}$, $q\in\mathbb{R}^{512}$,
                        $m \in \mathbb{R}^{\size{\mathcal{W}}}$,
                        and $M \in \mathbb{R}^{\size{\mathcal{W}} \times 512}$ is the output
                            embedding matrix.
                                As a hyper-paramter setting, we consider tieing the decoder
                                    input and output
                                        embedding matrices, i.e. $W = M$.
                                            Dropout of 0.1 is applied to all embedding, GRU outputs, and linear
                                                layer outputs.


\paragraph{Transformer}
We used the Transformer S2S as implemented in
\href{https://pytorch.org/}{PyTorch}.
The input embedding dimension is 512 and inner hidden layer size is 2048.
We used 8 heads in all multi-head attention layers.
We used Sinusoidal position embeddings following those described in
\citet{rush2018annotated}. Additionally, we used Adam with the learning
rate schedule provided in that work (factor=1, warmup=8000).
Dropout was set to 0.1.


\newcommand{\utt}{\ensuremath{\mathbf{y}}}
\newcommand{\uttVocab}{\ensuremath{\mathcal{W}}}
\newcommand{\da}{\ensuremath{a}}
\newcommand{\inseq}{\mathbf{x}}
\newcommand{\Attrs}{\ensuremath{\mathcal{V}}}
\newcommand{\inSize}{m}
\newcommand{\outSize}{n}

\newcommand{\mmhAttn}{\operatorname{maskedMHAttn}}
\newcommand{\mhAttn}{\operatorname{MHAttn}}

\newcommand{\mrEmb}{\mathbf{W}}
\newcommand{\uttEmb}{\mathbf{V}}
\newcommand{\decInput}{\mathbf{G}}
\newcommand{\decInputi}{\mathbf{g}_i}


\newcommand{\tfeA}{\boldsymbol{\check{\encInput}}^{(i)}}
\newcommand{\tfeB}{\boldsymbol{\bar{\encInput}}^{(i)}}
\newcommand{\tfeC}{\boldsymbol{\hat{\encInput}}^{(i)}}
\newcommand{\tfeD}{\boldsymbol{\dot{\encInput}}^{(i)}}
\newcommand{\tfeE}{\boldsymbol{\ddot{\encInput}}^{(i)}}

\newcommand{\tfdA}{\boldsymbol{\check{\decInput}}^{(i)}}
\newcommand{\tfdB}{\boldsymbol{\bar{\decInput}}^{(i)}}
\newcommand{\tfdC}{\boldsymbol{\hat{\decInput}}^{(i)}}
\newcommand{\tfdD}{\boldsymbol{\grave{\decInput}}^{(i)}}
\newcommand{\tfdE}{\boldsymbol{\tilde{\decInput}}^{(i)}}
\newcommand{\tfdF}{\boldsymbol{\acute{\decInput}}^{(i)}}
\newcommand{\tfdG}{\boldsymbol{\dot{\decInput}}^{(i)}}
\newcommand{\tfdH}{\boldsymbol{\ddot{\decInput}}^{(i)}}


%\subsection{Transformer Model Definition}
%
%Each Transformer layer is divided into blocks which each have three
%parts, (i) layer norm, (ii) feed-forward/attention, and  (iii) skip-connection.
%We first define the components used in the transformer blocks before
%describing the overall S2S transformer. 
%Starting with layer norm \cite{ba2016}, let $\encInput \in \reals^{m\times n}$, then we have
%$\layerNorm : \reals^{m \times n} \rightarrow \reals^{m \times n}$,
%\[\layerNorm(\encInput; \lnweightv, \lnbias) = \lnweight \odot (\encInput - \boldsymbol{\mu}) \odot \Lambda + \mathbf{b} \]
%
%where $\lnweightv, \lnbias \in \reals^n$ are learned parameters, $\odot$ is the elementwise product, $\lnweight = \left[\lnweightv,\ldots,\lnweightv\right] \in \reals^{m\times n}$ is a tiling of the parameter vector, $\lnweightv$, $m$ times, and  $\boldsymbol{\mu}, \boldsymbol{\Lambda} \in \reals^{m\times n}$ are
%defined elementwise as
%\[\boldsymbol{\mu}_{i,j} = \frac{1}{n} \sum_{k=1}^n \encInput_{i,k}\]
%and 
%\[\boldsymbol{\Lambda}_{i,j} = \left(
%    \sqrt{ \frac{1}{n-1} \sum_{k=1}^n \left( 
%\encInput_{i,k} - \boldsymbol{\mu}_{i,j} \right)^2  + \epsilon}\right)^{-1}\]
%respectively. The $\epsilon$ term is a small constant for numerical stability,
%set to $10^{-5}$.
%
%The inplace feed-forward layer, $\feedforward$, is a simple single-layer perceptron
%with $\relu$ activation 
%($\relu(\encInput) = \max\left(\zeroEmb, \encInput\right)$) \cite{nair2010}, applied to each row of an $m \times n$ input matrix, i.e. a sequence of $m$ objects
%with $n$ features,\\
%
%
%\noindent $\feedforward\left(\encInput;\weight{i},\weight{j},\bias{i},\bias{j}\right) =$
%\[  \relu\left(\encInput\weight{i} + \bias{i}\right)\weight{j} + \bias{j}     \]
%where $\weight{i} \in \reals^{\embDim \times \hidDim}$, $\bias{i} \in \reals^{\hidDim}$,
%$\weight{j} \in \reals^{\hidDim \times \embDim}$, $\bias{j} \in \reals^{\embDim}$ are learned parameters and 
%matrix-vector additions (i.e. $\mathbf{X} + \mathbf{b}$) are broadcast across
%the matrix rows.
%
%
%The final component to be defined is the multi-head attention, $\MultiAttn$ which is defined
%as\\
%
%\noindent  $\MultiAttn(\Query, \Key; \weight{a_1}, \weight{a_2}) =$
%\[ \left[ \begin{array}{c} 
%            \Attn\left(\Query\weight{a_1}_{1,1}, \Key\weight{a_1}_{2,1}, \Key \weight{a_1}_{3,1} \right),\\
%    \vdots \\
%            \Attn\left(\Query\weight{a_1}_{1,H}, \Key\weight{a_1}_{2,H}, \Key \weight{a_1}_{3,H} \right)
%\end{array} \right] \weight{a_2}
%\]
%
%where $\left[\cdot \right]$ indicates column-wise concatenation,  $\weight{a_1}_{1,*} \in \reals^{\embDim\times \embDim / H}$
%and $\weight{a_2} \in \reals^{\embDim\times \embDim}$ are learned parameters,
%$H$ is the number of attention heads, and  
%$\Attn$ is defined,
%
%\[\Attn\left(\mathbf{Q}, \mathbf{K}, \mathbf{V}\right) = \softmax\left(\frac{\mathbf{Q} \mathbf{K}^T}{\sqrt{\embDim}} \right)\mathbf{V}. \]
%
%Additionally, there is a masked variant of attention, $\MultiAttn_{M}$
%where the attention is computed 
%
%\[\Attn\left(\mathbf{Q}, \mathbf{K}, \mathbf{V}\right) = \softmax\left(\frac{\mathbf{Q} \mathbf{K}^T \odot \Mask }{\sqrt{\embDim}} \right)\mathbf{V} \]
%where $\Mask \in \reals^{\outSize \times \inSize}$ 
%is a lower triangular matrix, i.e. values on or below the diagonal are 1
%and all other values are $-\infty$. 
%
%Given these definitions, we now define the S2S transformer.
%Let $\Attrs$ be the encoder input vocabulary, and  $\encEmbs \in
%\reals^{\size{\Attrs} \times \embDim}$ an associated word embedding matrix
%where $\encEmbs_\attr \in \reals^{\embDim}$ denotes the $\embDim$-dimensional
%embedding for each $\attr \in \Attrs$. 
%Given a linearized MR $\lin(\mr) = \inseq= \left[ \da, \attr_1, \attr_2, \ldots,
%\attr_{\size{\mr}}\right] \in \Attrs^{\inSize}$ where the length
%of the sequence is $\inSize = \size{\mr} + 1$,
%let $\encWordEmb_i = \encEmbs_{\inseq_i}$ for $i \in \{1, \ldots \inSize\}$.
%
%Additionally let $\posEmb \in \reals^{\inSize_{max} \times \embDim}$ be a sinusoidal position embedding matrix
%defined elementwise with 
%\begin{align*}
%    \posEmb_{i,2j} & = \sin\left(\frac{i}{10,000^{ \frac{2j}{\embDim} }}\right) \\
%    \posEmb_{i,2j+1} & = \cos\left(\frac{i}{10,000^{ \frac{2j}{\embDim} }}\right). 
%\end{align*}
%The encoder input sequence $\encInput^{(0)} \in \reals^{\inSize \times \embDim}$ is then defined by
%\[\encInput^{(0)} = \left[\begin{array}{c} 
%            \encWordEmb_1 + \posEmb_1,\\
%            \encWordEmb_2 + \posEmb_2,\\
%            \vdots \\
%        \encWordEmb_\inSize + \posEmb_\inSize
%    \end{array}
%                        \right] \]
%
% A sequence of $l$ transformer encoder layers are then applied to the encoder
% input, i.e. $\encInput^{(i+1)} = \operatorname{TF}^{(i)}_{enc}\left(\encInput^{(i)}\right)$.
%Each encoder transformer layer computes the following, \\
%
%\noindent \textit{(Self-Attention Block)}\\
%\[\tfeA = \layerNorm\left(\encInput^{(i)}; \lnweightv^{(i,1)}, \lnbias^{(i,1)}\right)\]
%\[\tfeB = \MultiAttn\left(\tfeA, \tfeA; \weight{i,a_1},  \weight{i,a_2}\right)\]
%\[\tfeC = \encInput^{(i)} + \tfeB\]
%
%\noindent \textit{(Feed-Forward Block)}\\
%\[\tfeD = \layerNorm\left(\tfeC; \lnweightv^{(i,2)}, \lnbias^{(i,2)}\right)\]
%\[\tfeE = \feedforward\left(\tfeD;\weight{i,1},\weight{i,2},\bias{i,1},\bias{i,2}\right)\]
%\[ \encInput^{(i+1)} = \tfeC + \tfeE \]
%
%
%We denote the final encoder output for $l$ layers as $\encInput = \encInput^{(l)}$.  
%
%Let $\uttVocab$ be the vocabulary of utterance tokens, and 
%$\decEmbs \in \reals^{\size{\uttVocab} \times \embDim}$
%an associated embedding matrix, where
%$\decEmbs_\utttok \in \reals^{\embDim}$ denotes a  $\embDim$-dimensional embedding for
%each $\utttok \in \uttVocab$.
%
%%\placeholder{TODO: Make this true for all layers}
%Given the decoder input sequence $\utt = \utttok_1, \utttok_2, \ldots, 
%\utttok_\size{\utt}$, 
%let $\decWordEmb_i = \decEmbs_{\utttok_i}$ for $i \in \{1, \ldots \outSize\}$.
%where $\outSize = \size{\utt} - 1$
%
%\[\decInput^{(0)} = \left[\begin{array}{c} 
%            \decWordEmb_1 + \posEmb_1,\\
%            \decWordEmb_2 + \posEmb_2,\\
%            \vdots \\
%        \decWordEmb_\outSize + \posEmb_\outSize
%    \end{array}
%                        \right]. \]
%
%
%A sequence of $l$ transformer decoder layers are then applied to the decoder
% input, i.e. $\decInput^{(i+1)} = \operatorname{TF}^{(i)}_{dec}\left(\decInput^{(i)}\right)$.
%Each decoder transformer layer computes the following, \\
%
%~\\~\\ ~\\
%
%\noindent \textit{(Masked Self-Attention Block)}\\
%\[\tfdA = \layerNorm\left(\decInput^{(i)}; \lnweightv^{(i,1)}, \lnbias^{(i,1)}\right)\]
%\[\tfdB = \MultiAttn_M\left(\tfdA, \tfdA; \weight{i,a_1}, \weight{i,a_2} \right)\]
%\[\tfdC = \decInput^{(i)} + \tfdB\]
%
%\noindent \textit{(Encoder-Attention Block)}\\
%\[\tfdD = \layerNorm\left(\tfdC; \lnweightv^{(i,2)}, \lnbias^{(i,2)}\right)\]
%\[\tfdE = \MultiAttn\left(\tfdD, \encInput; \weight{i,a_3}, \weight{i,a_4}\right)\]
%\[\tfdF = \tfdC + \tfdE\]
%
%\noindent \textit{(Feed-Forward Block)}\\
%\[\tfdG = \layerNorm\left(\tfdF; \lnweightv^{(i,3)}, \lnbias^{(i,3)}\right)\]
%\[\tfdH = \feedforward\left(\tfdG;\weight{i,1},\weight{i,2},\bias{i,1},\bias{i,2}\right)\]
%\[ \decInput^{(i+1)} = \tfdF + \tfdH \]
%
%Let the $\decInput = \decInput^{(l)}$ denote the final decoder output,
%and let $\decInputi$ be the $i$-th row of $\decInput$ corresponding
%to the decoder representation of the $i$-th decoder state. The probability of 
%the next word is\\
%
%\noindent $\model\left(\utttok_{i+1}|\utttok_{\le i},\lin(\mr)\right)$
%\[  = \softmax\left( \weight{o}\decInputi + \bias{o} \right)_{\utttok_{i+1}} \]
%where $\weight{o} \in \reals^{\size{\uttVocab} \times \embDim}$ 
%and $\bias{o} \in \reals^{\embDim}$ are learned parameters. 
%
%
%%We used the Transformer S2S as implemented in 
%%\href{https://pytorch.org/}{PyTorch}.
%The input embedding dimension is $\embDim= 512$ and inner hidden layer size 
%is $\hidDim=2048$. The encoder and decoder have separate parameters.
%We used $H=8$ heads in all multi-head attention layers. 
% We used Adam with the learning
%rate schedule provided in  \citet{rush2018} (factor=1, warmup=8000).
%Dropout was set to 0.1 was applied to input embeddings and each skip 
%connection (i.e. the third line in each block definition). As a 
%hyperparameter, we optionally tie the decoder input and output embeddings,
%i.e. $\decEmbs = \weight{o}$.


\paragraph{BART}
We use the same settings as the fine-tuning for the CNN-DailyMail
summarization task,
although we modify the maximum number of updates to be roughly to be
equivalent to 10 epochs on the training set when using a 500 token batch
size, since
the number of updates effects
the learning rate scheduler. We selected the model iterate with
lowest validation set cross-entropy.

While BART is unlikely to have seen any linearized MR in its pretraining
data, its use of sub-word encoding  allows it to encode
arbitrary strings. Rather than extending it's encoder input vocabulary to
add the MR tokens, we simply format the input MR as a string
(in the correpsonding linearization order), e.g. ``inform rating=good name=NAME platforms=PC platforms=Xbox''.

\section{Experiments}

\subsection{Test-Set Evaluation}

In our first experiment, we compare performance of the proposed models and
linearization strategies on the E2E and ViGGO test sets. For the \lsshort{If}
and \lsshort{At+NUP} models we also include variants trained on
the union of original training data and phrase-augmented data (see
\autoref{sec:dataaug}), which we denote \phraseAug.

\paragraph{Evaluation Measures} For automatic quality measures, we report
\bleu~and \rougel~\cite{lin2004} scores.\footnote{We use the
\href{https://github.com/tuetschek/e2e-metrics}{official E2E evaluation
script} to compute these numbers.} Additionally, we use the matching rules to
automatically annotate the attribute-value spans of the model generated
utterances, and then manually verify/correct them. With the attribute-value
annotations in hand we compute the number of missing, wrong, or added
attribute-values for each model. From these counts, we compute the semantic
error rate (SER) \cite{dusek2020} where \[ \textrm{SER} = \frac{\#missing +
\#wrong + \#added}{\#attributes}.\]  On ViGGO, we do not include the
\Atr{rating} attribute in this evaluation since we consider it part of the
dialogue act.  Additionally, for \lsshort{At} variants, we report the order
accuracy (OA) as the percentage of generated utterances that correctly follow
the provided utterance plan. Utterances with wrong or added attribute values
are counted as not following the utterance plan. Additional metrics
and SER error break downs can be found in \autoref{app:exp.results}.

All models are trained five times with different random seeds; we report
the mean of all five runs. We report statistical significance
using Welch's $t$-test \cite{welch1947}, comparing the score distribution of the five runs from the best linearization strategy against all other strategies
at the $0.05$ level.

\paragraph{Baselines} On the ViGGO dataset we compare to the Transformer
baseline of \citet{juraska2019}, which used a beam search of size 10 and
heuristic slot reranker (similar to our matching rules).



On the E2E dataset, we report the results of 
TGen+ \cite{dusek2019}, an
LSTM-based S2S model, which also uses beam search with a matching rule based
reranker to select the most semantically correct utterance and is
trained on a cleaned version of the corpus (similar to our approach).
%, using matching
%rules to correct erroneous MRs in the train data (similar to our approach).
   
\subsection{Random Permutation Stress Test}



Differences between an \lsshort{At} model following a utterance planner model
and the human oracle are often small so we do not learn much about the limits
of controllability of such models, or how they behave in extreme conditions
(i.e. on an arbitrary, random utterance plan, not drawn from the training data
distribution). In order to perform such an experiment we generate random
utterance plans (i.e. permutations of attribute-values) and have the
\lsshort{At} models generate utterances for them, which we evaluate with
respect to SER and OA (we lack ground truth references with which to evaluate
\bleu~or \rougel).  We generate random permutations of size $3,4,\ldots, 8$ on
the E2E dataset, since there are 8 unique attributes on the E2E dataset. For
ViGGO we generate permutations of size $3,4,\ldots,10$ (96\% of the ViGGO
training examples fall within this range). For each size we generated 100
random permutations and all generated plans were given the \DA{Inform}
dialogue act. In addition to running the \lsshort{At} models on these random
permutations, we also compare them to the same model after using the NUP  to
reorder them into an easier\footnote{Easier in the sense that the
\NUP~re-ordering is closer to the training set distribution of \lsshort{At}
utterance plans.} ordering.  Example outputs can be
found in \autoref{app:examples}.  



\subsection{Human Evaluation} In our final experiment, we had human evaluators
rank the 100 outputs of the size 5 random permutations for three \BART~models
on both datasets: (i) \lsshort{At+p} model with \NUP,  (ii) \lsshort{At+p}
model, and (iii) \lsshort{At} model.  The first model, which uses an utterance
planner, is likely to be more natural since it doesn't have to follow the
random order, so it serves as a ceiling.  The second and third models will try
to follow the random permutation ordering, and are more likely to produce
unnatural transitions between awkard sequences of attribute-values.
Differences between these models will allow us to understand how the
phrase-augmented data affects the fluency of the models.  The annotators were
asked to rank outputs by their naturalness/fluency.  Each set was annotated
twice by different annotators so we can compute agreement. More details can be
found in \autoref{app:humaneval}.



\section{Results}


\paragraph{\lsshort{At} models accurately follow utterance plans.} See
\autoref{tab:main.e2e.test} and \autoref{tab:main.viggo.test} for results on
E2E and ViGGO test sets respectively.  
The best non-\Oracle~results are bolded for each model and results
that are not different with statistical significance to the best results
are underlined.
We see that the \lsshort{At+NUP}
strategy consistently receives the lowest semantic error rate and highest 
order accuracy, regardless of
architecture or dataset, suggesting that alleviating the model's decoder of content
planning is highly beneficial to avoiding errors. The Transformer \lsshort{At} model is able to consistently achieve virtually zero semantic error on E2E using either
the bigram or neural planner model.

We also see that fine-tuned BART is able to learn to follow an utterance plan
as well. When following the neural utterance planner,
BART is highly competitive with the trained from scratch Transformer
on E2E and surpassing it on ViGGO in terms of semantic error rate.

\input{nlg/tables/main_e2e_test.tex}
\input{nlg/tables/main_viggo_test.tex}




%{\color{red}
    Generally, the \lsshort{At} models had a smaller variance in test-set
evaluation measures over the five random initializations as compared to the
other strategies. This is reflected in some unusual equivalency classes
by statistical significance. For example, on the E2E dataset biGRU models,
the \lsshort{At+NUP+p} strategy acheives 0\% semantic error and is significantly
different than all other linearization strategies \textbf{except} 
the \lsshort{Fp} strategy even though the absolute difference in score is 
6.54\%. This is unusual because the \lsshort{At+NUP+p} strategy \textbf{is} 
significantly different from \lsshort{At+NUP} but the absolute difference is
only 0.26\%. This happens because the variance in test-set results
is higher for \lsshort{Fp} making it harder to show signficance with only
five samples.


%the difference in semantic error rate between the \lsshort{At} 
%model with and without phrase augmenation \textbf{is} significant at 0.2\% SER
%absolute. The \lsshort{If} model with phrase augmentation
%is \textbf{not} significantly different 

%\lsshort{At (NUP)+p} is significantly better than \lsshort{At (BgUP)}
%(0\% vs 0.2\%), but \textbf{not} \lsshort{If +p} (0\% vs 0.3\%).}





%and is highly competitive with the trained from scratch Transformer
%on the E2E dataet (Transformer \textsc{At} (NUP), 0\% SER vs. BART
%\textsc{At} (NUP) 0.2\% SER), and surpasses the trained from scratch model in
%the small data ViGGO setting (Transformer \textsc{At} (NUP), 2.7\% SER vs.
%BART \textsc{At} (NUP) 0.54\% SER). 

\paragraph{Transformer-based models are more faithful than biGRU on
\textsc{Rnd, Fp}, and \textsc{If} linearizations.} On the ViGGO dataset, BART
and Transformer \lsshort{If} achieve 1.86\% and 7.50\% semantic error rate 
respectively, while
the biGRU \lsshort{If} model has 19.20\% semantic error rate. These trends hold for \lsshort{Fp}
and \lsshort{Rnd}, and on the E2E dataset as well. Because there is no
sequential correspondence in the input, it is possible that the recurrence in
the biGRU makes it difficult to ignore spurious input ordering effects.
Additionally, we see that \lsshort{Rnd} does offer some benefits of denoising;
\lsshort{Rnd} models have lower semantic error rate than \lsshort{If} models in 3 of 6 cases 
and \lsshort{Fp} models in 5 out of 6 cases.

\paragraph{Model based plans are easier to follow than human reference plans.
} On E2E, there is very little difference in semantic error rate when following either the
bigram-based utterance planner, \BgUP, or neural utterance planner,
\NUP. This is also true of the ViGGO \BART~models as well.  In the
small data (i.e. ViGGO) setting, \biGRU~and \Transformer~models achieve better semantic error rate when following the neural utterance planner.  In most cases,
neural utterance planner models have slightly higher \bleu~and \rougel~than
the bigram utterance planner, suggesting the neural planner produces utterance plans closer to
the reference orderings. The neural and bigram planner models have slightly lower semantic error rate
than when following the \Oracle~utterance plans.  This suggests that the models
are producing orders more commonly seen in the training data, similar to how
neural language generators frequently learn the least interesting, lowest
entropy responses \cite{serban2016}.  On the other hand, when given
the \Oracle~orderings, models achieve much higher word overlap with the
reference, e.g. achieving an E2E \textsc{Rouge-L} $\ge 77$.

\input{nlg/tables/main_perm.tex}

\paragraph{Phrase-training reduces SER.} We see that phrase data improves semantic error rate
in 8 out of 12 cases, with the largest gains coming from the biGRU
\lsshort{If} model.  Where the base semantic error rate was higher, phrase training has a more
noticeable effect. After phrase training, all E2E models are operating at near
zero semantic error rate and almost perfectly following the neural utterance planner. Model performance on ViGGO
is more varied, with phrase training slighting hurting the biGRU
\lsshort{At+NUP} model, but otherwise helping performance.
%while improving for BART
%\lsshort{At (NUP)} models, but improving the Transformer.

\paragraph{Random Permutation Stress Test} Results of the random permutation
experiment are shown in \autoref{tab:perm}.  Overall, all models have an
easier time following the neural utterance planner's reordering of
the random
permutations. Phrase training also generally improved semantic error rate.  All models perform
quite well on the E2E permutations.  
%Models had an easier time following the
%neural utterance planner's reordering compared to the random permutations, but
With phrase-training,
all E2E models achieve less than 0.6\% semantic error rate following random 
utterance plans.
Starker differences emerge on the ViGGO dataset.  The biGRU\textsc{+NUP+p} model
achieves a 8.98\% semantic error rate and only correctly follows the given 
order 64.5\% of
the time, which is a large decrease in performance compared to the ViGGO test set.% (1.62\% SER and 94.3\% OA).

\input{nlg/tables/main_human.tex}

\paragraph{Human Evaluation} Results of the human evaluation are shown in
\autoref{tab:human}. We show the number of times each system was ranked 1
(most natural), 2, or 3 (least natural) and the average rank overall.
Overall, we see that BART  with the neural utterance planner 
and phrase-augmentation training is
preferred on both datasets, suggesting that the utterance planner is
producing natural orderings of the attribute-values, and the model can
generate reasonable output for it. On the E2E dataset, we also see small
differences in between the \lsshort{At+p} and \lsshort{At} models
suggesting that when following an arbitrary ordering, the phrase-augmented
model is about as natural as the non-phrase trained model. This is encouraging
as the phrase trained model has lower semantic error rates. 
On the ViGGO dataset we do find
that the phrase trained model is less natural, suggesting that in the small
data setting, phrase-training may hurt fluency when trying to follow a
difficult utterance plan.

For agreement we compute average Kendall's $\tau$ between each pair of
annotators for each dataset. On E2E, we have $\tau=.853$ and ViGGO we have
$\tau=.932$ suggesting very strong agreement.
%($\tau=0$ would indicate rankings
%are random, and $\tau=-1$ would indicate annotators preferred completely
%opposite models). 

\subsection{Discussion}

One consistently worrying sign throughout the first two experiments is that the
automatic metrics are not good indicators of semantic correctness.  For
example the \rougel~score of the E2E \lsshort{At Oracle} models is about 8
points higher than the \lsshort{At+NUP} models, but the \lsshort{At+NUP}
models make fewer semantic errors. Other similar examples can be found where
the automatic metric would suggest picking the more error prone model over
another. As generating fluent text becomes less of a difficult a problem,
these shallow ngram overlap methods will cease to suffice as distinguishing
criteria.

The second experiments also reveal limitations in the controllable model's
ability to follow arbitrary orderings. The biGRU and Transformer models in the
small-data ViGGO setting are not able to generalize effectively on
non-training distribution utterance plans. BART performance is much
better here, but is still hovering around 2\% semantic error rate and only roughly 88\% of
outputs conform to the intended utterance plan.  Thankfully, if an exact
ordering is not required, using the neural utterance planner to propose an order leads to more
semantically correct outputs.


\section{Limitations}

While we are able to acheive very low test-set SER for both corpora, we 
should caution that this required extensive manual development of matching 
rules to produce MR/utterance alignments, which in turn resulted in 
significant cleaning of the training datasets. We chose to do this over 
pursuing a model based strategy of aligning utterance subspans to 
attribute-values %predicting the semantic correctness
because we wanted to better understand how systematically S2S models can
represent arbitray order permutations independent of alignment model error. 
%so that
%an upper bound on this representational capacity might be estimated. 

%There are, of course, many other works that either separately or jointly
%model the semantic correctness \cite{nie2019,kedzie2019}, and additionally the semantic segmentation \cite{wiseman2018,shen2020,li2020} which
% could be used in practice to avoid such manual efforts.

Also we should note that 
%while possibly less glamorous than proposing novel
%model architecture, 
data cleaning can yield more substantial decreases in
semantic errors \cite{dusek2019,hongminwang2019} and is an important 
consideration in any practical neural NLG.
