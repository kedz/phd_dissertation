
\subsection{Experiments}
\subsubsection{E2E Challenge}
 We train base generators $\basegen$~on the original training data $\corpus$, 
 with and without
 delexicalizing the \Atr{name} and \Atr{near} attributes. 
We train for 500 epochs with gradient descent. We use a batch size of 128,
with a learning rate of 0.25, weight decay penalty of 0.0001, and a dropout 
probability of 0.25.
We select the best model iteration using validation
set BLEU score.\footnote{We use the official shared task script to
compute automatic quality metrics on the E2E dataset.}

Using the self-training method outlined in \autoref{sec:daprotos},
we create augmented datasets using either $\ruledmodel$~or
$\learndmodel$, which we refer to as 
$\augdata_{\ruledmodel}$ and $\augdata_{\learndmodel}$ respectively 
We only use the model parser, $\learndmodel$, in the delexicalized setting.
We repeat the while loop in the data-augmentation algorithm 
25,000 times for each valid MR size $3,\ldots,8$,
%See \autoref{table:samplequal} for statistics on the total sample sizes 
%after filtering.
yielding 1,591,788 additional samples for the lexicalized $\gen_0/\ruledmodel$
pairing, and 
501,909 for $\gen_0/\ruledmodel$ and 384,436 for 
$\gen_0/\learndmodel$~delexicalized pairings.

%?
%?
%?To sample a novel MR with  $S$ attributes, we sample a combination of $S-1$ attributes
%?uniformly at random %from the $\binom{7}{S-1}$ possible combinations
%?(always appending the \textit{name} attribute since every MR contains it).
%? We 
%?then sample attribute values for each slot % independently
%?inversely proportional to their empirical frequency
%?in the training set so as to increase the likelihood of creating a novel
%?or under-represented MR.
%?
%?
%?After obtaining such a sample $\utttoks$~we then perform noise injection sampling,
%?generating 200 samples $\pdaCandUtt \sim 
%?\basegen(\cdot|\utttoks, \epsilon^{(i)})$ 
%?in parallel and discarding all but the top 20
%?samples by average log likelihood according to $\basegen$.
%?We also discard any utterances that have previously been generated.
%?%to avoid 
%?%adding repeat utterances to the augmented data.
%?
%?We then apply the parser to the sampled utterances, to obtain its
%?predicted MR, $\pdaPredMr^{(i)} = \dmodel(\pdaCandUtt)$. If using 
%?the rule based parser $\ruledmodel$~and $\pdaPredMr^{(i)} 
%?= \emptyset$, i.e. the utterance
%?does not have a valid parse, we discard it.
%?Similarly, when using the classifier based parser, \learndmodel, if 
%?any attribute value is predicted with less than 50\% probability we discard 
%?it. All surviving $(\pdaPredMr^{(i)}, \predutttoks^{(i)})$ pairs are added to \augdata.
%?We repeat this process 25,000 times for each valid MR size $S$.
%?See \autoref{table:samplequal} for statistics on the total sample sizes 
%?after filtering.
%?%yielding 1,591,788 additional samples for the lexicalized \basegen/\ruleclf,
%?%501,909 for the delexicalized \basegen/\ruleclf, and 384,436 for the 
%?%delexicalized \basegen/\learnedclf~pairings.
%?
%?\

For both $\corpus \cup \augdata_{\ruledmodel}$ and 
$\corpus \cup \augdata_{\learndmodel}$ we train new generators \auggen~using 
the same training setting as above (although we terminate training after 50 
epochs
because the models converge much faster with the additional data).
We report \textsc{Bleu}, \textsc{Rouge-L}, and \textsc{Meteor} on the E2E Challenge test set, using the official shared-task evaluation script.
We show results for both greedy decoding and beam decoding with beam size 8
under $\basegen$~and
\auggen~models. We compare our models to the best sequence-to-sequence DNN
model, Slug \citep{juraska2018}, the best grammar-rule based model, 
DANGNT \citep{nguyen2018},
and the best template based model, TUDA \citep{puzikov2018}, as determined during 
the shared task evaluation \citep{dusek2019}.


%We then create augmented datasets using the methods
%described in section~\ref{}, using either the rule-based classifier 
%\ruleclf~or learned classifier \learnedclf~to reconstruct the MRs.
%For each filtering method we train a new generator \auggen~on the union of 
%the original training data the augmented data collection, i.e. 
%$\trdata \cup \augdata^{\ruleclf}$ and $\trdata \cup \augdata^{\learnedclf}$.
%The architecture of \auggen ~ is identical to that of \basegen. We 
%train \auggen ~ for 50 epochs (with the added data, the \auggen ~ 
%models converge much faster), again selecting the best model via validation 
%set BLEU score.

\subsubsection{Laptops and TVs}
We perform similar experiments on the Laptops and TVs datasets. 
We train a separate $\basegen$~model for each dataset 
for 300 epochs with a learning rate of 0.1
for Laptops and 0.25 for TVs. The weight decay penalty is 0.0001 
and dropout probability is 0.25. Best model iteration is determined
by validation set \textsc{Bleu} score. As in the E2E experiments, we create an
augmented dataset for both the Laptops and TVs dataset using the method
outlined in \autoref{sec:daprotos}. We then train new generators 
$\auggen$~on the union of original training data and the augmented dataset.

We repeat the while loop in the noise-injection sampling algorithm 
 25,000 times for each dialogue act and legal dialogue act size.\footnote{A number of attributes $S$ is ``legal'' if we observe a training instance with that dialogue act instance with $S$  attributes in the original training data.}
We obtain 373,468 and 33,478 additional samples for the Laptops 
and TVs datasets respectively.


%On the Laptops and TVs dataset,
%%each DA can have a different minimum and maximum number of attributes.
%for each dialogue acts and a legal number of attributes $S$ we draw $S$ random attributes
%(modulo any required attributes like \textit{name}; not all dialogue acts require it).\footnote{A number of attributes $S$ is ``legal'' if we observe a training instance with that dialogue act instance with $S$  attributes in the original training data.}
%%For binary attributes, or attributes that can have the \textit{don't care}
%%value, we randomly sample these values, to obtain a MR \mrx.
%
%We then perform noise injection sampling,
%generating 200 samples $\pdaCandUtt \sim 
%\basegen(\cdot|\mrtoks, \epsilon^{(i)})$ under the same settings as the E2E
%dataset. We repeat this process 25,000 times for each DA and DA size.
%We obtain 373,468 and 33,478 additional samples for the Laptops 
%and TVs datasets respectively.
%

We automatically evaluate our models using the evaluation script of
\citep{wen2016}, which computes \textsc{Bleu} scores, as well as slot
alignment error rate (since this dataset is almost fully delexicalized,
it simply checks for the presence of the correct attribute placeholders
according to the MR). We compare again to the Slug model as well
as the Semantically Conditioned LSTM (SCLSTM) \citep{wen2015}
which report state-of-the-art results on these datasets.




