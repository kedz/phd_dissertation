\subsection{Alignment Training Linearization}
%As was mentioned in \autoref{mrtproblemdef}, there are many choices of
%\linearizationstrategy~$\ls$ when modeling \surfacerealization~with
%a \sequencetosequence~archictecture. One particular \linearizationstrategy,
%which we call the \alignmenttraining~linearization, yields a controllable \sequencetosequence~model which is capable of following an utterance plan indicating
% the \surfacerealization~order of a \meaningrepresentation's \attributevalues.
%
%
Unlike the {\color{red}arbitrary linearization} used in \autoref{somewhere}, \alignmenttraining~linearization~is not soley a function of $\mr$, but is determined by both $\mr$ and a reference utterance $\utttoks$. Given a $\left(\mr, \utttoks\right)$ pair, the \alignmenttraining~linearization finds a linearization
$\ls$ such that the order of the \attributevalues~in $\ls(\mr)$ corresponds
to the order in which they are realized in $\utttoks$. %Formally, this means that 
%for any $\left(i^{(\ls_k)},j^{(\ls_k)}\right), \left(i^{(\ls_{k+1})},j^{(\ls_{k+1})}\right) \in \denotationset_{\mr,\utttoks}$, we have $j^{(\ls_k)} < i^{(\ls_{k+1})}$.


\autoref{fig:atexamples} shows some examples of the 
\alignmenttraining~linearization, including some special cases. When
linearizing list-valued attributes, for instance, we treat them as distinct
\attributevalue~pairs. Occasionally, we encounter repeated \attributevalues~in the training set, and in that case we include extra \attributevalue~pairs
in the correpsonding location in the linearization. We ignore any instances
of  ungrounded information, as in example \autoref{} where (such and such)
has no explicit representation.  

%\subsubsection{Alignment Training Implementation}

%\input{nlg/figures/atalg.tex}

%\hyperref[alg:at]{Algorithm \ref{alg:at}} shows the alignment training algorithm for a training example $(\mr,\utttoks)$. 

In \autoref{fig:at} we show the steps of our procedure for obtaining
the alignment training linearization, given a reference utterance $\utttoks$.
%the intermediate outputs of each line in \hyperref[alg:at]{Algorithm \ref{alg:at}}. %In practice, there are two distinct and important issues when implementing the 
%\alignmenttraining~linearization. 
The first step is 
to tag the utterance tokens 
$\utttoks = \left[\utttok_1,\ldots,\utttok_\uttSize\right]$ 
with a corresponding tag sequence 
$\mrtags=\left[\mrtag_1,\ldots, \mrtag_\uttSize\right]$ where each 
tag $\mrtag_i$ is equal to an \attributevalue~$\mrtok_j \in \mr$ or 
the null tag $\nulltag$. We assume that we have access to such a tagger
$\tagger : \outSpace \rightarrow \inSpace$ (see \autoref{sec:align}
for implementation details). After producing the tag sequence $\mrtags^{(1)} = \tagger\left(\utttoks\right)$ 
%(\hyperref[alg:attag]{Lines \ref{alg:attag}} in \hyperref[alg:at]{Algorithm \ref{alg:at}} and \hyperref[fig:at]{\autoref{fig:at}.b}),
(\hyperref[fig:at]{\autoref{fig:at}b}),
we then group contiguous sequences of tags sharing the same tag value, discarding any null tag sequences to obtain the sequence of subsequences 
$\mrtags^{(2)} = \left[\mrtags^{(1)}_{i_1:j_1},\ldots,\mrtags^{(1)}_{i_\mrSize:j_\mrSize}, \right]$ (\hyperref[fig:at]{\autoref{fig:at}c}).
Finally, $\mrtoks$ is constructed by 
by prepending the \dialogueact~$\mrtok_0$ of $\mr$ to the ordered sequence 
of \attributevalue~pairs $\mrtok_1,\ldots,\mrtok_\mrSize$ implied by $\mrtags^{(1)}_{i_1},\ldots,\mrtags^{(1)}_{i_\mrSize}$ (\hyperref[fig:at]{\autoref{fig:at}d}). 



%~\\~\\
%we then segment $\mrtags^{(1)}$ into contiguous sequences of tags containing no null tags, $\nulltag$, to obtain the sequence of subsequences
%$\mrtags^{(2)}$ (\hyperref[fig:at]{\autoref{fig:at}c}). Each subsequence of
%$\mrtags^{(2)}$ is further split into the longest contiguous tag sequences 
%such that all 
%tags of a segment are equal to the same \attributevalue~pair(\hyperref[fig:at]{\autoref{fig:at}d}). 
%Finally, $\mrtoks$ is constructed by 
%by prepending the \dialogueact~$\mrtok_0$ of $\mr$ to the ordered sequence 
%of \attributevalue~pairs $???$ implied by $\mrtoks^{(3)}$ (\hyperref[fig:at]{\autoref{fig:at}e}). 
%
\input{nlg/figures/atouts.tex}

At test time, the generation model is only
presented with a \meaningrepresentation~$\mr$ and we don't have a reference 
utterance $\utttoks$ with which to apply the alignment training 
linearization. In this case,
we can use an utterance planning model $\planner : \mrspace \rightarrow \inSpace$ to map a \meaningrepresentation~$\mr$ to a
linear sequence $\mrtoks$. Alternatively, we can use the test set reference
to obtain the alignment training linearization; this represents an unrealistically
optimistic case where the model has clairvoyant known of the discourse
ordering preferred by a human.
%
%a human reference utterance. In the latter case, this represents an
%unrealistic case where the model has clairvoyant knowledge of the 
%discourse ordering of the desired utterance but is also representative
%of discourse orderings prefered by humans. 
In either case, we refer to 
a linearization $\mrtoks$ obtained either from $\planner$ or a human reference
as an \utteranceplan~since a generation model trained with
\alignmenttraining~linearizations will attempt to follow it during 
the generation of the utterance.



\subsubsection{Alternative Linearization Strategies}

In our experiments, we compare alignment training to three other 
linearization strategies, which we describe below. 
These linearizations, while sensible methods of mapping a 
\meaningrepresentation~to a linear sequence of tokens, have no 
correspondence between the
\meaningrepresentation~linearization and surface realization order. Because
of this, \sequencetosequence~models trained using these linearization strategies are not controllable. These linearization strategies may have some effect on the faithfulness when
compared to each other and alignemnt training, so evaluation of this modelling choice has
additional benefits beyond benchmarking alignment training.
See \autoref{fig:linstrats} for examples of the
different linearization strategies on the same \meaningrepresentation/utterance
pair.
%Crucially, \alignmenttraining~stands in contrast to these 
%alternative linea
%three strategies
%(\lsshort{Rnd}, \lsshort{If}, and \lsshort{Fp}) which do not have any
%correspondence between the the order of attribute-value pairs in $\lin(\mr)$
%and the order in which they are realized in the corresponding
%utterance $\utttoks$.



\paragraph{Random (\textsc{Rnd})} In the \textsc{Random} linearization (\textsc{Rnd}), we randomly
order the attribute-value pairs for a given \meaningrepresentation. This strategy 
serves as a baseline for determining
if linearization ordering matters at all for faithfulness. \textsc{Rnd} 
is similar to token level noise used in
sequential denoising autoencoders \citep{wang2019denoising} and might even improve faithfulness.
During training, we resample the ordering for each example at every epoch
so as not to over fit to a particular random ordering.
We do not resample the validation set in order to obtain stable results 
from which to pick the best model.

\input{nlg/figures/inputordering.tex}

\paragraph{Increasing Frequency (\textsc{If})} 
In the \textsc{Increasing Frequency} linearization (\textsc{If}), 
we order the attribute-value pairs by increasing frequency of 
occurrence in the training data
i.e. $\acount(\attr_i=\aval_i) \le \acount(\attr_{i+1}=\aval_{i+1})$.
We hypothesize that placing frequently occurring items in a consistent location
may make it easier for the generation model to realize those items correctly, possibly
at the expense of rarer items.

\paragraph{Fixed Position (\textsc{Fp})} We take  consistency one step further 
and create a fixed ordering of all attributes, \textit{n.b.} not attribute-values, ordering them in increasing
frequency of occurrence on the training set (i.e. every instance has the same
order of attributes in the encoder input). In this \textsc{Fixed Position}
linearization (\textsc{Fp}), attributes that are not present 
in an \meaningrepresentation~are explicitly represented with an \textit{N/A} value. 
For list-valued slots, we determine the maximum length list in the training
data and create that many repeated slots in the input sequence.
This linearization is feasible for datasets with a modest number of 
unique attributes (in our case ViGGO has 14 attributes and
the E2E Challenge corpus has eight) but would not easily scale to 10s, 100s, or larger
attribute vocabularies. 





