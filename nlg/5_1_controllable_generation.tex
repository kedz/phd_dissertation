\subsection{Controllable Generation}

In the previous section, we showed how to make an arbitrary \sequencetosequence~model more likely to generate semantically correct utterances using 
data-augmenation. While the resultant generation model is more faithful,
it still lacks even coarse-grained control over the organization of the 
generated utterance.  What's more, there's no gaurrantee that 
small changes to the input don't lead to dramatically different outputs.
For example, changing boolean attribute, e.g. changing
\AV{family\_friendly}{yes} to \AV{family\_friendly}{no}, may lead to dramatically
different syntactic structure in the output.
This is because the structure or plan of the utterance is only determined 
implicitly by the \sequencetosequence~decoder's language model.

%In this section, we show how to make an arbitrary \sequencetosequence~controllable. That is, given a \meaningrepresentation~input, we can specify ahead of time the order in which the model realizes each \attributevalue~in the utterance.

In this section, we show how to make a \sequencetosequence~controllable, which
we achieve through a particular linearization of the input 
\meaningrepresentation, a linearization strategy we call \textit{alignment training}. That is, we
can specify the order in which the \attributevalues~of an input
\meaningrepresentation~are to be realized in the utterance.  See
\autoref{fig:examplecontrol} for example realizations from a controllable
generation model that follow three different permutations of \Atr{name},
\Atr{eat\_type}, and \Atr{area} attributes.
Through evaluation on two dialogue generation benchmarks we
show that alignment training yields high levels of control in both GRU
and Transformer models. This holds when models follow either 
a separate planning model or a human provided plan.

We also propose using a phrase-based data augmentation
method to further improve the robustness of control. We further evaluate the 
control mechanism on randomly generated plans which are much harder to follow
than human or model provided plans. We find that phrase-based data augmentation
helps \sequencetosequence~models follow these more difficult plans.


%Before describing alignment training, we formally define controllable 
%generation.
%
%
%
%
%In o
%often semantically correct with respect to the input, they are not 
%controllable in the sense that the order that the various \attributevalues~are
%realized in the output utterance is implicitly controlled by the decoder
%language model.
%
%We say that a \meaningrepresentation-to-text model $\model : \mrspace \rightarrow \outSpace$ is controllable if 
%we can specify the \surfacerealizationorder~for it to follow. That is,
%given a \meaningrepresentation~$\mr$ with $\mrSize$ \attributevalues,
%we can specify an ordering $\ls$ of attribute-values, $\attr_{\ls_1}=\aval_{\ls_1}, \ldots, \attr_{\ls_{\mrSize}}=\aval_{\ls_{\mrSize}}$, such that for
%$\utttoks = \model_\ls(\mr)$, we have that $\denotes{\utttoks} = \mr$, 
%and there is an ordered sequence of spans $\utttoks_{i_{1}:j_{1}} \ldots,
%\utttoks_{i_\mrSize:j_\mrSize}$
%such that $i_k \le j_k < i_{k+1} \le j_{k+1}$ for all $k \in \{1,\ldots,\mrSize-1\}$ and $\aval_{\ls_k} = \denotes{\utttoks_{i_k:j_k}}$.
%
%
%
%~\\~\\
% \linearizationstrategy~$\ls$ such that $\ls(\mr) = \mrtoks = \left[\mrtok_0, \mrtok_{\pi_1}, \ldots, \mrtok_{\pi_\mrSize}\right]$, a controllable
%\surfacerealization~model $\model\left(\cdot|\mrtoks;\params\right)$
%will put more probability mass on utterances $\utttoks$ where the 
%$\denotationset_{\mr,\utttoks}$ has the property that $j^{(\pi_k)} < i^{(\pi_{k+1})}$. In other words, the \surfacerealizationorder~of the \attributevalues~follows the permutation implied by $\ls$.
%
%
%
%In this section, we show how to make an arbitrary \sequencetosequence~controllable. That is, given a \meaningrepresentation~input, we can specify ahead of time the order in which the model realizes each \attributevalue~in the utterance.
%
%In this section, we now show how to make an arbitray \sequencetosequence~controllable, which we achieve through a particular linearization the input
%\meaningrepresentation s, a linearization strategy called \textit{alignment 
%training}. 
%
%
%
%
%Through evaluation on two dialogue generation benchmarks we
%show that alignment training yields high levels of controll in both GRU
%and Transformer models.   We also propose using a phrase-based data augmentation
%method to further improve the reliability of control. 
%
%Before describing alignment training, we formally define controllable 
%generation.
%
