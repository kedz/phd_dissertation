\section{Modeling \MeaningRepresentation-to-Text Generation with \SequencetoSequence~Architectures}
\label{mrtproblemdef}

We approach the problem of mapping a \meaningrepresentation~to
a natural language utterance with a variety of popular \sequencetosequence~architectures. 
%In this chapter, we propose strategies for faithful and controllable
%generation when using a \sequencetosequence~model to perform 
%\surfacerealization~of a \meaningrepresentation. Before delving into those 
%aspects, we first describe generally how a typical \sequencetosequence~model
%is used to perform \surfacerealization~of a \meaningrepresentation.
A \sequencetosequence~model is a neural network with parameters $\params$
that implements a probabilistic mapping
$\gen(\cdot|\cdot;\params) : \inSpace \times \outSpace \rightarrow
(0,1)$ from input sequences $\mrtoks = \left[\mrtok_1,\ldots,\mrtok_\mrSize\right] \in \inSpace$
to output sequences $\utttoks = \left[\utttok_1, \ldots, \utttok_\uttSize\right] \in \outSpace.$
Tokens from the input sequence are drawn from a finite vocabulary $\mrvocab$ and input sequences its
Kleene closure $\mrvocab^* = \inSpace$. 
Analogously, tokens from the output sequence are drawn from a distinct, finite vocabulary $\uttvocab$ and output sequences its
Kleene closure $\uttvocab^* = \outSpace$. For clarity we occasionally 
omit $\params$ in subsequent equations.

Typically, \gen~is implemented as a bi-partite network consisting of distinct
encoder and decoder networks $\encMod$~and \decMod~respectively. 
The encoder $\encMod : \inSpace \rightarrow \reals^{*\times\encDim}$ is a mapping of an input sequence $\mrtoks$ of $\mrSize$ tokens to
$\mrSize$ corresponding  vectors $\encState_1,\ldots\encState_\mrSize \in \reals^\encDim$ and
\[ \gen(\utttoks|\mrtoks) = \gen(\utttoks|\encMod(\mrtoks)) = \gen(\utttoks|\encState_1,\ldots,\encState_\mrSize).\]
The decoder $\decMod : \uttvocab^+ \times \reals^{*\times \encDim} \rightarrow (0,1)$ then is 
a mapping of previously generated tokens $\utttoks_{1:i-1} = \left[\utttok_1,\ldots,\utttok_{i-1}\right]$ and encoder states $\encState_1,\ldots,\encState_\mrSize$ to a probability distribution over
the output vocabulary $\uttvocab$, where
\[ \gen(\utttok_i|\utttoks_{1:i-1},\mrtoks) = \decMod\left(\utttoks_{1:i}, \encMod(\mrtoks)\right)   \quad \textrm{and} \quad \sum_{\utttok\in \uttvocab} \gen\left(\utttok| \utttoks_{1:i-1}, \encMod(\mrtoks)\right) = 1.  \]
Hence, $\gen(\cdot|\mrtoks)$ is a conditional
language model over utterance tokens that factorizes in a left-to-right fashion, i.e.,
\[\gen\left( \utttoks| \mrtoks \right) = \prod_{i=1}^\uttSize \gen\left(\utttok_i|\utttoks_{1:i-1},\mrtoks\right). \]


Notice that the ``inputs'' and ``outputs'' to the \sequencetosequence~model are sequences of tokens, 
$\mrtoks$ and $\utttoks$ respectively.
In order to use a \sequencetosequence~model for \naturallanguagegeneration~from a 
\meaningrepresentation~we need only map our desired inputs and outputs to sequences of discrete tokens. In English, the desired output is relatively straightforward to represent as 
a sequence as an English language 
utterance can naturally be represented as a sequence of word tokens. In practice, we also indicate full sentence stops with a special token \senttok~and prepend and append distinguished tokens \starttok~and \stoptok, respectively, to indicate the start and 
end of the utterance, as well as lower-case all tokens. As an example, the utterance 
\[\textit{The Vaults pub is near Caf{\'e} Adriatic. It is not a good place for families.} \]
would be represented as 
\[\small \utttoks = \left[\starttok, \textit{the},\,\textit{vaults},\,\textit{pub},\,\textit{is},\,\textit{near},\,\textit{caf{\'e}},\,\textit{adriatic},\,\textit{.},\,\textit{\senttok},\,\textit{it},\,\textit{is},\,\textit{not},\,\textit{a},\,\textit{good},\,\textit{place},\,\textit{for},\,\textit{families},\,\textit{.},\,\textit{\stoptok}\right]. \]

The \meaningrepresentation~is not 
itself a sequence, however, so we cannot apply it to a 
\sequencetosequence~model
directly.
%Let $\mr \in \mrspace$ be a \meaningrepresentation. 
 Instead it must first be ``linearized,'' or mapped to a linear sequence of 
 tokens. We refer to a function $\ls : \mrspace \rightarrow \inSpace$,
 as a \linearizationstrategy. We experiment with several 
 \linearizationstrategies~in this chapter, however, all of them operate over
 the same domain, $\mrvocab^+$, where $\mrvocab$ consists of 
 distinct tokens for each \dialogueact~and \attributevalue~pairs. As an example
 consider the following \meaningrepresentation,
 \begin{singlespace}
  \[ \mr = \left[\!\!\left[\begin{array}{l} \textsc{Inform} \\ \AV{name}{Aromi}\\\AV{area}{city centre} \\ \AV{eat\_type}{coffee shop} \end{array}\right]\!\!\right] \]
 \end{singlespace}
\noindent and some possible linearizations,
 \begin{align*}
     \ls_1(\mr) & = \mrtoks= \left[\textit{inform},\,\textit{name=Aromi},\,\textit{eat\_type=coffee shop},\,\textit{area=city centre}\right] \\
     \ls_2(\mr) & = \mrtoks= \left[\textit{inform},\,\textit{eat\_type=coffee shop},\,\textit{name=Aromi},\,\textit{area=city centre}\right] \\
     \ls_3(\mr) & = \mrtoks= \left[\textit{inform},\,\textit{eat\_type=coffee shop},\,\textit{area=city centre},\,\textit{name=Aromi}\right].
 \end{align*}
In practice, regardless of the choice of \linearizationstrategy, we prepend a start token, $\starttok$,
and append and a stop token, $\stoptok$, to all input token sequences, e.g.
\[ \mrtoks = \left[\starttok,\,\textit{inform},\,\textit{name=Aromi},\,\textit{eat\_type=coffee shop},\,\textit{area=city centre},\,\stoptok\right]. \]



The encoder and decoder networks of the \sequencetosequence~model can be implemented with variety
of architectures. We show two popular architectures in particular, the gated recurrent unit (GRU)
\citep{cho2014gru} and the transformer \citep{vaswani2017}.




