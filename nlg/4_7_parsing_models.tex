\subsection{\MeaningRepresentation~Parsing Models}

Given a novel utterance $\predutttoks$~sampled from $\gen$, we need to 
reliably parse the implied \meaningrepresentation, i.e. 
$\pdaPredMr = \dmodel(\predutttoks)$, 
where $\dmodel$~is our parsing model. We have two things going for us in 
our experimental setting. First, even with noise-injection sampling,
model outputs are fairly patterned, reducing the variability of the utterances
we need to parse in practice. 

Second, the \meaningrepresentation~in this study are
flat lists of attributes that are somewhat independent of each other.
We only need to detect the presence of each attribute and its value.
For the Laptops and TVs datasets we also need to recover the dialog
act but these also are signaled by a fairly limited repertoire 
of cues, e.g. ``we recommend.'' % or ``compared to'' for the 
%\textit{Recommend} and \textit{Compare} DAs respectively. 
Given this, we experiment with both hand crafted regular expression 
rules and learned classifiers to predict the value of
an attribute if present or that it is missing. 


\paragraph{Rule-based parser (\ruledmodel)} We design hand-crafted 
regular expression based rules to match for the presence of key phrases 
for each of the attributes and dialouge acts in the datasets while also checking to
make sure that there is only one match per attribute.

To construct the rules, we look through both the training data references as 
well as the generation model outputs as this is what the rules will
be operating on in practice. For each lexicalized attribute (and dialogue act) we 
develop a list of regular expressions % or conjunctions of regular expressions
such as,\[
\texttt{/is (family|kid|child) friendly/} \Rightarrow \AV{family\_friendly}{yes}.\]
For the delexicalized attributes, we simply check for the presence 
of the placeholder token. 

%One failure mode of the generation models on
%Laptops and TVs is to use the wrong attribute value token in an expression,
%e.g. ``...has a DRIVESIZERANGE price range,'' so we add additional rules
%to mark an utterance/MR pair as invalid in these cases.
%e.g.
%\begin{align*}
%\texttt{/price/}~\land~\lnot\texttt{/PRICERANGE/} 
%    \Rightarrow \emptyset.
%\end{align*}

We design these rules to be high precision, as it is safer to miss out on 
more obscure varieties of utterance to avoid adding incorrectly parsed data 
points.
However,  in many cases the rules are also high recall as well. 
The average F-score on the E2E validation set is 0.93.
%In some cases, this is probably
%close to the performance ceiling; the human authored references in the validation
%set are noisy and are often incorrectly labeled or omit realizing attributes.


%This is a relativity conservative approach as it is possible we will
%miss good phrases, however, it is more important that we add as little
%noise to augmented dataset as possible.
%rules to detect the presence of 
%key phrases related to each attribute's realization. We design these rules
%to be high-precision, so even while we may not cover all possible 
%only need to augment our training data with pairs $(\samplex, \sampley)$ 
%If the rules indicate that the MR is valid (e.g. only mentions an attribute
%once). While we may miss out on more diverse constructions, we more reliably
%ensure the augmented data is correct, i.e. \sampley~correct conveys the
%discourse act and attibute values specified in \samplex. Adding too many
%incorrect pairs will degrade the reliability of our final generation model.



\paragraph{Classifier-based parser (\learndmodel)} 

It is perhaps too optimistic to believe we can construct reasonable rules in
all cases. Rule creation quickly becomes tedious and for more complex
\meaningrepresentations, this would become a bottleneck. To address these
concerns, we also study the feasibility of using learned classifiers to
predict the presence and value of the attributes. For each attribute in the
E2E dataset, we trained a separate convolutional neural network 
classifier to predict the correct attribute value (or \textit{n/a} if the
attribute is not present) from an utterance.

The architecture largely follows that of \citet{kim2014convolutional}.  Let
$\parEmbs \in \reals^{\setsize{\uttvocab} \times \parEmbsDims}$ be an
embedding matrix for the utterance token vocabulary, $\parEmbs$, with each
token $\utttok \in \uttvocab$ associated with a row in $\parEmbs$, which we
indicate with $\parEmbs_\utttok \in \reals^{\parEmbsDims}$.  For each
attribute $\attr$, the set of possible values (including \textit{n/a}) is
denoted $\attrvocab$.

Given an utterance $\utttoks = \left[\utttok_1,\ldots,\utttok_\uttSize\right],$
we first embed the utterance tokens to obtain a sequence of 
word embeddings, \[  \parEmb_1, \ldots, \parEmb_\uttSize = \parEmbs_{\utttok_1}, \ldots, \parEmbs_{\utttok_\uttSize}. \]

We then apply a series of unigram, bigram, and trigram (i.e., convolutional feature widths $\parkwidth$ of 1, 2, and 3 respectively)  convolutional
filters each with $\parFtrDims$ output features, which are computed as,
\begin{align*} \parFeat_{k,i} & = 
    \max_{j \in \convRange{\parkwidth}{\uttSize} } 
    \relu\left(\convbias{k,i} + \convweight{k,i} \cdot
\left[\begin{array}{c} \parEmb_j \\ \parEmb_{j+1}\\\vdots \\\parEmb_{j+k-1} \end{array}  \right]\right)  \quad \forall k,j: \begin{array}{l} k \in \{1,2,3\}, \\[-1em] j \in \{1,\ldots, \parFtrDims\} \end{array}
\end{align*}
where $\convbias{k,i} \in \reals$ and $\convweight{k,i} \in \reals^{\parkwidth\parEmbsDims}$ are learned parameters and we use the same zero-padded
convolution described in \autoref{sec:sentconvenc} with $\parEmb_i = \zeroEmb$
for $i < 1$ and $i > \uttSize$.
The individual convolutional features are collected in a hidden
state encoding of the utterance, $\parHid \in \reals^{\parkwidth \parFtrDims}$,
with
\[
\parHid = \left[\parFeat_{1,1},\ldots,f_{1,\parFtrDims},f_{2,1},\ldots,f_{2,\parFtrDims}, f_{3,1}, \ldots, f_{3,\parFtrDims}\right].
\] 
The hidden state is then fed through a two layer \feedforward~network 
to compute the probability of a particular attribute value,
\[    
\dmodel_\attr\left(\aval|\utttoks\right)   = \softmax\left(\cweight{\attr,2} \left( \cweight{\attr,1} \parHid + \cbias{\attr,1}\right)   + \cbias{\attr,2} \right)_\aval
\]
where $\cweight{\attr,1} \in \reals^{\parEmbsDims \times \parkwidth\parEmbsDims}$, $\cbias{\attr,1} \in \reals^{\parEmbsDims}$, $\cweight{\attr,2} \in \reals^{\setsize{\attrvocab} \times \parEmbsDims}$, and $\cbias{\attr,2} \in \reals^{\setsize{\attrvocab}}$ are learned parameters. If $\mr$ contains a $\mrSize$
\attributevalue~pairs, $\attr_1=\aval_1,\ldots,\attr_\mrSize=\aval_\mrSize$,
the probability of $\mr$ under the parsing model is $\dmodel(\mr|\utttoks) = \prod_{i=1}^\mrSize \dmodel_{\attr_i}\left(\aval_i|\utttoks \right)$.

Each attribute classifier has distinct parameters and is trained on the 
training set but minimizing the negative log-likelihood, 
\[\mathcal{L}(\dparams) = -\sum_{(\attr=\aval, \utttoks) \in \corpus} \log \dmodel_\attr(\aval| \utttoks;\dparams),  \]
 using minibatch stochastic gradient descent on the training set, $\corpus$.

During training we apply dropout (with drop rate of 0.25) to 
the embedding layer, convolutional filter outputs, and hidden
layers. We train for 30 epochs with gradient descent
using  a learning rate of 0.25 and 
weight decay penalty of 0.0001, using validation set F1
as our model selection criterion.
The average E2E validation F-score is 0.94.


%~\\~\\
%
%original training data. 
%
%
%
%We use a separate CNN classifier for each attribute to predict
%the corresponding value (or \textit{n/a}) from an utterance $\utttoks$.
%We first look up the tokens in $\utttoks = \left[ \utttok_1,\ldots,\utttok_\uttSize\right]$ in an embedding matrix $\decEmbs \in \reals^{\setsize{\uttvocab} \times 50}$,
%to obtain a matrix $\mathbf{V} \in\mathbb{R}^{\uttSize \times 50}$,
%\[ \mathbf{V}= \left[\begin{array}{c} \mathbf{v}_1,\\ \vdots \\ \mathbf{v}_\uttSize  \end{array} \right]  = \left[\begin{array}{c} \decEmbs_{\utttok_1},\\ \vdots \\ \decEmbs_{\utttok_\uttSize}  \end{array} \right].\]
%
%We then apply a series of unigram, bigram, and trigram convolutional
%filters each with $50$ output features.
%\begin{align*} f_{k,j} & = \max_{i \in \left\{ 
%    1 - \left\lfloor \frac{\ckernelWidth}{2} \right\rfloor, 
%    \ldots, \uttSize +  
%    \left\lfloor \frac{\ckernelWidth}{2} \right\rfloor - \ckernelWidth + 1 \right\}}\relu\left(\beta + \boldsymbol{\nu}  
%\left[\begin{array}{c} \mathbf{v}_i \\ \mathbf{v}_{i+1}\\\vdots \\\mathbf{v}_{i+k-1} \end{array}  \right]\right)  \quad \forall k,j: k \in \{1,2,3\},j \in \{1,\ldots, 50\} \\
%\mathbf{h} & = \left[f_{1,1},\ldots,f_{1,50},f_{2,1},\ldots,f_{2,50}, f_{3,1}, \ldots, f_{3,50}\right]
%\end{align*}
%
%\begin{align*}
%\mathbf{h} & = \left[f_{1,1},\ldots,f_{1,50},f_{2,1},\ldots,f_{2,50}, f_{3,1}, \ldots, f_{3,50}\right]\\
%\dmodel\left(a=v|\utttoks;\dparams\right) & = \softmax\left(\weight{a,2} \left( \weight{a,1} \mathbf{h} + \bias{a,1}\right)   + \bias{a,2} \right)_v
%\end{align*}
%
%After concatenating and max-pooling over the sequence dimension,
%and applying a ReLU activation,
%we obtain a hidden layer in $\mathbb{R}^{150}$.
%We then apply another fully-connected layer with ReLU activation
%which down projects the hidden layer to $\mathbb{R}^{50}$.
%Finaly we apply the final softmax layer to predict the class label.
%
%During training we apply dropout (with drop rate 0.25) to 
%the embedding layer, convolutional filter outputs, and hidden
%layers. We train for 30 epochs with gradient descent
%using  a learning rate of 0.25 and 
%weight decay penalty of 0.0001, using validation set F1
%as our model selection criterion.
%The average E2E validation F-score is 0.94.
%%iThe classifiers
%%required significantly less manual effort
%%(and almost zero domain knowledge) to construct.
%
%
