\subsection{Datasets}

\input{nlg/tables/fgdatasets.tex}

We experimentally validate the noise-injection sampling and self-training
data-augmentation scheme on three recent dialgue generation datasets,
the E2E Challenge dataset~\citep{novikova2017} 
and the Laptops and TVs datasets \citep{wen2016}.
%three collections of 
%\meaningrepresentation/utterance pairs for training response generation models
%
%
%We use three recent dialogue generation datasets in our experiments,
%the E2E Challenge Dataset \cite{novikova2017}, and the Laptops and 
%TVs datasets \cite{wen2016}.
%We only briefly review them here.
Each dataset consists of \meaningrepresentations~paired with one or more 
reference utterances.
%(see \autoref{figure:introexample} for an example from the E2E dataset).
%The structure of each \meaningrepresentation~is relatively simple, 
%consisting of the 
%dialog act itself, (e.g. \textit{inform}, \textit{recommend}, 
%\textit{compare}, etc.) and a variable number of attribute slots
%which need to be realised in the utterance. 
All attribute values 
come from a closed vocabulary.
%If an attribute is not present in the MR it should not be realized in the 
%corresponding utterance. 

The three datasets also represent different training size conditions; 
with the E2E Challenge dataset representing the ``large data'' training
condition and the Laptops and TVs dataset representing ``small data'' conditinos. See \autoref{tab:fgds} for dataset size statistics.
The E2E Challenge dataset has only one dialogue act, \textsc{Inform}, 
and its training \meaningrepresentations~contain three to eight
\attributes~unique attributes. 
The Laptops and TVs datasets contain a more diverse set of \meaningrepresentation/utterance pairs. There are 14 unique dialogue acts. 
The number of minimum and maximum \attributes~varies according to the dialogue
act. See \autoref{tab:fgdas} for a list of the unique dialogue acts
and attributes for the three training sets. 

\input{nlg/tables/fgdas.tex}


\paragraph{Delexicalization}
Prior work using neural \naturallanguagegeneration~models often relies on 
delexicalization, that is, replacing realizations of named-entity or 
numeric values in an utterance with a placeholder token, in order to alleviate
data sparsity issues and yield better generalization when a generating utterances about named-entities not seen in the training dataset. For example
on the E2E Challenge dataset, the \Atr{name}~and \Atr{near}~attributes
are often delexicalized because they are proper names of estabilishments
that are simple to find and replace in the utterance.
When delexicalizing the \Atr{name}~and \Atr{near}~attributes,
the fully lexicalized utterance 

\begin{center}\noindent~~~~\textit{Near The Six Bells is a venue that is children
friendly named The Golden Curry.}\end{center}

\noindent can be delexicalized as

\begin{center}\noindent ~~~~\textit{Near <<near>> is a venue that is children
friendly named <<name>>.}\end{center}

\noindent Delexicalized utterances can be re-lexicalized as a post-processing
step, where the placeholder token is replaced with the correct value text.

On the E2E Challenge dataset, we experiment with delexicalization of the 
\textit{Name} and \textit{Near} attributes since they have a relatively large vocabulary of valid slot fillers, some of which are only seen  infrequently in the training data;
it can be difficult for fully lexicalized models to produce some of the 
rarer location names for these attributes. 

However, since delexicalization might be difficult or 
impossible in other domains, we implement both delexicalized and lexicalized
versions of the generation models on the E2E dataset to 
more fully evaluate the
self-training method.
   

The evaluation script for the Laptops and TVs datasets uses delexicalization
to evaluate attribute realization error, and so we use it here to be 
consistent with prior work, 
delexicalizing all possible attributes.
%



