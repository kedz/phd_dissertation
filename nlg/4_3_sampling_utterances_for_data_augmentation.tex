
\subsection{Conditional Utterance Sampling for Data-Augmentation}

We cannot use $\gen_0$ with beam search as we saw previously,
there are some \meaningrepresentations~that $\gen_0$ won't be able to create utterances for (as we saw with \AV{near}{Burger King}). We could try a variant
of ancestral sampling, however 
it is difficult for ancestral sampling schemes to produce 
extremely different outputs that break from spurious associations learned
from the training distribution without hurting fluency. 



The fundamental issue with ancestral sampling is that the randomness of the
model at the word selection stage. This means that in the middle of generating
a phrase it is possible for a disfluent word to be selected, which can
disrupt the current phrase but also destabilize subsequent generation steps
as the model tries to recover from the unsual selection.
Ideally, randomness in a model would occur earlier in determining the 
``topicality'' or ``aboutness'' (you might even say content selection) 
of the generated utterance. 

Beyond the conditioning input $\ls(\mr)$,
the content that is to be generated is implicitly represented by
inner hidden states of the model. 
In \citet{cho2016}, they argue that the hidden states, $\decHidState_i$, 
of the \sequencetosequence~decoder lie on a manifold, as a requirement of
learning the next word prediction, i.e.
$\hat{\utttok} = \argmax_\utttok \gen(\utttok|\utttoks_{1:i},\ls(\mr)) = 
\argmax_\utttok \left(\weight{o}  \decHidState_i + \bias{o}\right)_\utttok$ 
implies that $\hat{\utttok}$ must be linearly separable from other 
words $\utttok^\prime \in \uttvocab$ along the hidden state manifold. The implication is that moving about the manifold will change 
the ``topicality'' of the distribution $\gen(\utttok|\utttoks_{1:i},\ls(\mr))$.
They further suggest adding Gaussian noise to $\decHidState_i$ as a way to
obtain random samples from $\gen$, which we refer to as noise-injection
sampling.

\input{nlg/figures/noiseinjectionalg.tex}


We show the noise-injection sampling algorithm in \autoref{fig:noiseinj}
along with greedy decoding and ancestral sampling to emphasize the 
how the location of the stochasticity moves from the next word selection (line 8) to a peturbation of the hidden state (line 7). 
Note that in line 7 of the noise injection sampling algorithm, the standard
deviation of the normal distribution, $\frac{\sigma}{i}$, is scaled by the 
decoder step $i$ and in the limit turns to zero, i.e. $\lim_{i \rightarrow +\infty} \decHidState + \boldsymbol{\epsilon}_i = \decHidState$. The inuition 
behind this scaling is that we add the most noise at the first steps 
of decoding, which encourages the decoder to start from a topically novel
region of the hidden state manifold. As the decoding proceeds, the noise
reduces along with the chances of sending the decoder off the manifold
and destabilizing the decoding, and gradually we converge on the behavior of
greedy decoding. 


We can understand noise-injection sampling as a compromise between
greedy decoding and ancestral sampling; rather than draw a sequence of utterance
tokens stochasticity, we instead draw a sequence of hidden state spaces.
Given the sequence of hidden state spaces, the corresponding sequence of
utterance tokens is deterministically decided by the most likely next token
given the last hidden state. This next word selection strategy helps to
avoid disfluent continuations.


\input{nlg/figures/exsamples.tex}

In \autoref{fig:examplesamples} we show examples of samples obtained with
noise-injection sampling as well as some ancestral sampling schemes.
We can see that the ancestral sampling examples are not very diverse.
The noise-injection sampling
example, however, semantically diverges from the input while maintaining
fluency. It was even able to generate an utterance containing the phrase
``near Burger King'' which is was practically impossible to generate with
beam search.


\input{nlg/tables/nextwordprobs.tex}

In \autoref{tab:sampprobs}, we show the probability of generating the 
example

\begin{center}
\textit{\starttok~the waterman is not family friendly and is located near burger king . \stoptok}\end{center}

\noindent under the various sampling schemes. In our present case, top-$k$ and nucleus
sampling have very similar distributions to the ancestral sampling
distribution ($\gen(\utttok_{i+1}|\decHidState_i)$). All three of these 
techniques assign a very low probability to generating the example utterance,
and in the case of nucleus sampling, it only gives non-zero probability when
using a nucleus of 0.99 cumulative probability (i.e. $\nucleusVocab{.99}{i}$)!
In particular, noise-injection sampling puts much more probability 
mass on generating relatively rare \attributevalue~realizations ($i=2$, ``waterman'' and $i=11$, ``burger''). This aspect of noise-injection sampling
makes it very attractive for data-augmentation as we can use it to create
semantically novel utterances that are not represented in the training dataset,
while still producing fluent outputs.



